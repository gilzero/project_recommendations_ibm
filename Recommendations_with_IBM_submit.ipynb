{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recommendations with IBM (Submit Version)\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/2322/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "2      1429.0         use deep learning for image classification   \n",
       "3      1338.0          ml optimization using cognitive assistant   \n",
       "4      1276.0          deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from pprint import pp\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_val = df.groupby('email').size().sort_values(ascending=True).median()\n",
    "max_views_by_user = df.groupby('email').size().sort_values(ascending=False).head(1).values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>Community Detection at Scale</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>During the seven-week Insight Data Engineering...</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>* United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...</td>\n",
       "      <td>When used to make sense of huge amounts of con...</td>\n",
       "      <td>How smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>One of the earliest documented catalogs was co...</td>\n",
       "      <td>How smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>Today’s world of data science leverages data f...</td>\n",
       "      <td>Using Apache Spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>Today’s world of data science leverages data f...</td>\n",
       "      <td>Using Apache Spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_body  \\\n",
       "50   Follow Sign in / Sign up Home About Insight Da...   \n",
       "365  Follow Sign in / Sign up Home About Insight Da...   \n",
       "221  * United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...   \n",
       "692  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "232  Homepage Follow Sign in Get started Homepage *...   \n",
       "971  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "399  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "761  Homepage Follow Sign in Get started Homepage *...   \n",
       "578  This video shows you how to construct queries ...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "\n",
       "                                       doc_description  \\\n",
       "50                        Community Detection at Scale   \n",
       "365  During the seven-week Insight Data Engineering...   \n",
       "221  When used to make sense of huge amounts of con...   \n",
       "692  One of the earliest documented catalogs was co...   \n",
       "232  If you are like most data scientists, you are ...   \n",
       "971  If you are like most data scientists, you are ...   \n",
       "399  Today’s world of data science leverages data f...   \n",
       "761  Today’s world of data science leverages data f...   \n",
       "578  This video shows you how to construct queries ...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "\n",
       "                                         doc_full_name doc_status  article_id  \n",
       "50                        Graph-based machine learning       Live          50  \n",
       "365                       Graph-based machine learning       Live          50  \n",
       "221  How smart catalogs can turn the big data flood...       Live         221  \n",
       "692  How smart catalogs can turn the big data flood...       Live         221  \n",
       "232  Self-service data preparation with IBM Data Re...       Live         232  \n",
       "971  Self-service data preparation with IBM Data Re...       Live         232  \n",
       "399  Using Apache Spark as a parallel processing fr...       Live         398  \n",
       "761  Using Apache Spark as a parallel processing fr...       Live         398  \n",
       "578                              Use the Primary Index       Live         577  \n",
       "970                              Use the Primary Index       Live         577  "
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "\n",
    "dups = df_content.groupby('article_id').size().sort_values(ascending=False) # duplicates of article id\n",
    "dups_idx = dups[dups > 1].index # duplicates index\n",
    "\n",
    "df_content[df_content['article_id'].isin(dups_idx)].sort_values(by='article_id') # retrieve duplicated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "\n",
    "df_content = df_content.drop_duplicates('article_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles = df.article_id.nunique() # The number of unique articles that have at least one interaction\n",
    "total_articles = df_content.article_id.nunique() # The number of unique articles on the IBM platform\n",
    "unique_users = df.email.nunique() # The number of unique users\n",
    "user_article_interactions = df.shape[0] # The number of user-article interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_ds = df.groupby('article_id').size().sort_values(ascending=False).head(1)\n",
    "\n",
    "# The most viewed article in the dataset as a string with one value following the decimal \n",
    "most_viewed_article_id = str(most_viewed_ds.index[0])\n",
    "\n",
    "# The most viewed article in the dataset was viewed how many times?\n",
    "max_views = most_viewed_ds.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1      1314.0       healthcare python streaming application demo        2\n",
       "2      1429.0         use deep learning for image classification        3\n",
       "3      1338.0          ml optimization using cognitive assistant        4\n",
       "4      1276.0          deploy your python model as a restful api        5"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you have everything right here! Nice job!\n"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val,\n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions,\n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views,\n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id,\n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles,\n",
    "    '`The number of unique users in the dataset is ______`': unique_users,\n",
    "    '`The number of unique articles on the IBM platform`': total_articles\n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids_ds = df.groupby('article_id').size().sort_values(ascending=False).head(n)\n",
    "    ids = article_ids_ds.index\n",
    "    \n",
    "    top_articles = []\n",
    "    for i in ids:\n",
    "        top_articles.append(df[df['article_id'] == i].head(1).title.values[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return top_articles # Return the top article titles from df (not df_content)\n",
    "\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles_ids - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids_ds = df.groupby('article_id').size().sort_values(ascending=False).head(n)\n",
    "    ids = article_ids_ds.index\n",
    "    top_articles_ids = list(ids)\n",
    " \n",
    "    return top_articles_ids # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use deep learning for image classification', 'insights from new york car accident reports', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'healthcare python streaming application demo', 'finding optimal locations of new store using decision optimization', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model']\n",
      "[1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0, 1162.0, 1304.0]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top_5 looks like the solution list! Nice job.\n",
      "Your top_10 looks like the solution list! Nice job.\n",
      "Your top_20 looks like the solution list! Nice job.\n"
     ]
    }
   ],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    \n",
    "    df_user_item = df.copy()\n",
    "    \n",
    "    # Fill in the function here\n",
    "    df_user_item['title'] = 1\n",
    "    df_user_item = df_user_item[['user_id', 'article_id', 'title']]\n",
    "    \n",
    "    user_item = pd.pivot_table(df_user_item, index='user_id', columns='article_id', values='title', fill_value=0)\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>12.0</th>\n",
       "      <th>14.0</th>\n",
       "      <th>15.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>18.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1434.0</th>\n",
       "      <th>1435.0</th>\n",
       "      <th>1436.0</th>\n",
       "      <th>1437.0</th>\n",
       "      <th>1439.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1441.0</th>\n",
       "      <th>1442.0</th>\n",
       "      <th>1443.0</th>\n",
       "      <th>1444.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5149 rows × 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0.0     2.0     4.0     8.0     9.0     12.0    14.0    15.0    \\\n",
       "user_id                                                                      \n",
       "1                0       0       0       0       0       0       0       0   \n",
       "2                0       0       0       0       0       0       0       0   \n",
       "3                0       0       0       0       0       1       0       0   \n",
       "4                0       0       0       0       0       0       0       0   \n",
       "5                0       0       0       0       0       0       0       0   \n",
       "...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "5145             0       0       0       0       0       0       0       0   \n",
       "5146             0       0       0       0       0       0       0       0   \n",
       "5147             0       0       0       0       0       0       0       0   \n",
       "5148             0       0       0       0       0       0       0       0   \n",
       "5149             0       0       0       0       0       0       0       0   \n",
       "\n",
       "article_id  16.0    18.0    ...  1434.0  1435.0  1436.0  1437.0  1439.0  \\\n",
       "user_id                     ...                                           \n",
       "1                0       0  ...       0       0       1       0       1   \n",
       "2                0       0  ...       0       0       0       0       0   \n",
       "3                0       0  ...       0       0       1       0       0   \n",
       "4                0       0  ...       0       0       0       0       0   \n",
       "5                0       0  ...       0       0       0       0       0   \n",
       "...            ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "5145             0       0  ...       0       0       0       0       0   \n",
       "5146             0       0  ...       0       0       0       0       0   \n",
       "5147             0       0  ...       0       0       0       0       0   \n",
       "5148             0       0  ...       0       0       0       0       0   \n",
       "5149             1       0  ...       0       0       0       0       0   \n",
       "\n",
       "article_id  1440.0  1441.0  1442.0  1443.0  1444.0  \n",
       "user_id                                             \n",
       "1                0       0       0       0       0  \n",
       "2                0       0       0       0       0  \n",
       "3                0       0       0       0       0  \n",
       "4                0       0       0       0       0  \n",
       "5                0       0       0       0       0  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "5145             0       0       0       0       0  \n",
       "5146             0       0       0       0       0  \n",
       "5147             0       0       0       0       0  \n",
       "5148             0       0       0       0       0  \n",
       "5149             0       0       0       0       0  \n",
       "\n",
       "[5149 rows x 714 columns]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    user_ids_all = user_item.index\n",
    "    \n",
    "    # a dictionary holds [user id : similarity] value pairs\n",
    "    dot_product_dict = {}\n",
    "\n",
    "    # compute similarity of each user to the provided user\n",
    "    for i in user_ids_all:\n",
    "        dot_product = np.dot(user_item.loc[user_id:user_id,:], user_item.loc[i:i,:].transpose())\n",
    "        dot_product_dict[i] = dot_product[0][0]\n",
    "        \n",
    "    # sort by highest (most similar), also drop against self    \n",
    "    ds = pd.Series(dot_product_dict.values(), index=dot_product_dict.keys()).drop(index=user_id).sort_values(ascending=False)\n",
    "    \n",
    "    # extract only index as list of ids. \n",
    "    most_similar_users = list(ds.index)\n",
    "       \n",
    "    return most_similar_users # return a list of the users in order from most to least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: [3933, 23, 3782, 203, 4459, 3870, 131, 46, 4201, 395]\n",
      "The 5 most similar users to user 3933 are: [1, 23, 3782, 4459, 203]\n",
      "The 3 most similar users to user 46 are: [4201, 23, 3782]\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    article_names = []\n",
    "    \n",
    "    for i in article_ids:\n",
    "        \n",
    "        # print(i)\n",
    "\n",
    "        name = df[df.article_id == i].title.head(1).values[0]\n",
    "\n",
    "        article_names.append(name)\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    article_ids = list(user_item.loc[user_id][user_item.loc[user_id] > 0].index)\n",
    "    article_names = get_article_names(article_ids)\n",
    "    \n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # get all similar user ids for the targeted user\n",
    "    similar_uids = find_similar_users(user_id)\n",
    "\n",
    "\n",
    "    # get all article ids of the targeted user\n",
    "    article_ids_target_user = get_user_articles(user_id)[0]\n",
    "    # print(f\"[article_ids_target_user]:\\n {article_ids_target_user} \\n\")\n",
    "\n",
    "    \n",
    "    # a list contain unseen articles to recommend\n",
    "    recs = []\n",
    "\n",
    "    for uid in similar_uids:\n",
    "\n",
    "        # print(f\"\\n\\n\\n[number of recs]: {len(recs)}\\n\")\n",
    "        if len(recs) == m:\n",
    "            # print(f\"Number of recs reaches threadhold. Enough. Stop\")\n",
    "            break\n",
    "\n",
    "\n",
    "        #print(f\"[similar user id]: {uid}\")\n",
    "\n",
    "        # get this uid's article ids, and arbitrarily shuffle\n",
    "        article_ids_similar_user = get_user_articles(uid)[0]\n",
    "\n",
    "        # compute the differences of articles seen between the this user and targeted user\n",
    "        # subtraction's order matters\n",
    "        set_diff = list(set(article_ids_similar_user) - set(article_ids_target_user))\n",
    "        # print(f\"[set_diff]:\\n {set_diff} \\n\")\n",
    "        \n",
    "        # make a shuffle for arbitraily chocies from the diff set\n",
    "        random.shuffle(set_diff)\n",
    "\n",
    "        # add the differences of article ids to recs [], append only unique (no duplicate)\n",
    "        for i in set_diff:\n",
    "            if i not in recs and len(recs) < m:\n",
    "                recs.append(i)\n",
    "                # print(f\"[id] {i} appended\")\n",
    "\n",
    "    return recs # return your recommendations for this user_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twelve\\xa0ways to color a map of africa using brunel',\n",
       " 'optimizing a marketing campaign: moving from predictions to actions',\n",
       " 'airbnb data for analytics: mallorca reviews',\n",
       " 'intents & examples for ibm watson conversation',\n",
       " 'web picks (week of 4 september 2017)',\n",
       " 'awesome deep learning papers',\n",
       " 'deep learning from scratch i: computational graphs',\n",
       " 'fertility rate by country in total births per woman',\n",
       " 'using machine learning to predict parking difficulty',\n",
       " 'visualize data with the matplotlib library']"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# According to dataset, df and df_content, article id are all actually numeric. \n",
    "\n",
    "print(df.article_id.dtype)\n",
    "print(df_content.article_id.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here. \n",
    "# I use numeric id instead of string id for article id, since they are actually all numeric.\n",
    "# It makes sense to make it consistent numeric over this notebook\n",
    "\n",
    "assert set(get_article_names([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names([1320.0, 232.0, 844.0])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set([1320.0, 232.0, 844.0])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    # get all neighbors ids\n",
    "    nbh_ids = find_similar_users(user_id)\n",
    "    \n",
    "    \n",
    "    # assemble a data matrix\n",
    "    data_matrix = np.array([\n",
    "    [\n",
    "        x, # neighbor id\n",
    "        np.dot(user_item.loc[user_id:user_id,:], user_item.loc[x:x,:].transpose())[0][0], # similarity score\n",
    "        df[df.user_id == x].shape[0] # number of content interaction\n",
    "    ] for x in nbh_ids])\n",
    "    \n",
    "    # make a dataframe\n",
    "    neighbors_df = pd.DataFrame(data=data_matrix, \n",
    "                                columns=['neighbor_id', 'similarity', 'num_interactions'], \n",
    "                                index=data_matrix[:,0]).sort_values(by=['similarity', 'num_interactions'],\n",
    "                                                                    ascending=[False, False])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # get all similar user ids for the targeted user\n",
    "    # fetch with the 'neighbors_df'\n",
    "    similar_uids = list(get_top_sorted_users(user_id).index)\n",
    "    #print(f\"[similar_uids]: \\n{similar_uids}\")\n",
    "\n",
    "\n",
    "    # get all article ids of the targeted user\n",
    "    article_ids_target_user = get_user_articles(user_id)[0]\n",
    "    # print(f\"[article_ids_target_user]:\\n {article_ids_target_user} \\n\")\n",
    "\n",
    "    \n",
    "    # a list contain unseen articles to recommend\n",
    "    recs = []\n",
    "\n",
    "    for uid in similar_uids:\n",
    "\n",
    "        # print(f\"\\n\\n\\n[number of recs]: {len(recs)}\\n\")\n",
    "        if len(recs) == m:\n",
    "            #print(f\"Number of recs reaches threadhold {m}. Enough. Stop\")\n",
    "            break\n",
    "\n",
    "\n",
    "        #print(f\"[similar user id]: {uid}\")\n",
    "\n",
    "        # get this uid's article ids, and arbitrarily shuffle\n",
    "        article_ids_similar_user = get_user_articles(uid)[0]\n",
    "\n",
    "        # compute the differences of articles seen between the this user and targeted user\n",
    "        # subtraction's order matters\n",
    "        set_diff = list(set(article_ids_similar_user) - set(article_ids_target_user))\n",
    "        #print(f\"[set_diff before sort]:\\n {set_diff} \\n\")\n",
    "        \n",
    "        # Sort the set. Determine with highest total interactions metric \n",
    "        set_diff = list(df[df.article_id.isin(set_diff)]['article_id'].value_counts().index)\n",
    "        #print(f\"[set_diff after sort]:\\n {set_diff} \\n\")\n",
    "\n",
    "        # add the differences of article ids to recs [], append only unique (no duplicate)\n",
    "        for i in set_diff:\n",
    "            if i not in recs and len(recs) < m:\n",
    "                recs.append(i)\n",
    "                #print(f\"[id] {i} appended\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    rec_names = get_article_names(recs)\n",
    "    \n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "[1330.0, 1427.0, 1364.0, 1170.0, 1162.0, 1304.0, 1351.0, 1160.0, 1354.0, 1368.0]\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['insights from new york car accident reports', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model', 'model bike sharing data with spss', 'analyze accident reports on amazon emr spark', 'movie recommender system with spark machine learning', 'putting a human face on machine learning']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = 3933 # Find the user that is most similar to user 1 \n",
    "user131_10th_sim = 242 # Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This all looks good!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "\n",
    "For new users, code start problem, we can use knowledge base approach, pulling most-interacted (viewed) content and trending content. \n",
    "Since the dataset has no timestamp attribute, we might draw the most-interacted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = 0.0\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "\n",
    "\n",
    "new_user_recs = get_top_article_ids(10, df) # Your recommendations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "assert set(new_user_recs) == set([1314.0, 1429.0, 1293.0, 1427.0, 1162.0, 1364.0, 1304.0, 1170.0, 1431.0,\n",
    "                                  1330.0]), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations (EXTRA - NOT REQUIRED)</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term.  You might consider content to be the **doc_body**, **doc_description**, or **doc_full_name**.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` Use the function body below to create a content based recommender.  Since there isn't one right answer for this recommendation tactic, no test functions are provided.  Feel free to change the function inputs if you decide you want to try a method that requires more input values.  The input values are currently set with one idea in mind that you may use to make content based recommendations.  One additional idea is that you might want to choose the most popular recommendations that meet your 'content criteria', but again, there is a lot of flexibility in how you might make these recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Constants and Reusable objects for tokenization\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Soft copy a article content dataframe for NLP processing.\n",
    "df_nlp = df_content.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization helper\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    private tokenizer to transform each text.\n",
    "    As a NLP helper function including following tasks:\n",
    "    - Replace URLs\n",
    "    - Normalize text\n",
    "    - Remove punctuation\n",
    "    - Tokenize words\n",
    "    - Remove stop words\n",
    "    - Legmmatize words\n",
    "    :param text: A message text.\n",
    "    :return: cleaned tokens extracted from original message text.\n",
    "    '''\n",
    "\n",
    "    # print(f\"original text: \\n {text}\")\n",
    "\n",
    "    # replace urls\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word).strip() for word in tokens if word not in stop_words]\n",
    "\n",
    "    # in case after normalize/lemmatize, if there is no words, make a dummy element. otherwise follwing transformation\n",
    "    # may breaks\n",
    "    if len(tokens) < 1:\n",
    "        tokens = ['none']\n",
    "\n",
    "    # print(f\"tokens: \\n {tokens} \\n\\n\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some cleaning to the df_nlp dataset.\n",
    "\n",
    "\n",
    "# Found index and article id mismatched. \n",
    "# Update index with article to make it consistent and eaiser to process with.\n",
    "df_nlp.index = df_nlp.article_id\n",
    "\n",
    "\n",
    "# Clean empty / missing page body and desc content. Update the empty (NaN) with 'empty' placeholder\n",
    "df_nlp.loc[df_nlp[df_nlp.doc_body.isnull()].index, 'doc_body'] = 'empty'\n",
    "df_nlp.loc[df_nlp[df_nlp.doc_description.isnull()].index, 'doc_description'] = 'empty'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal private similarity function of body, title, desc respectly\n",
    "# These internal functions will be called by similarity overall function (a weighed sum of all similarity).\n",
    "\n",
    "\n",
    "\n",
    "def _compute_article_body_similarity(article_id_1, article_id_2, data=df_nlp):\n",
    "    # Compute the consine similarity based on tfidf of body content\n",
    "    # This is a private helper function that is used by overall similarity function.\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(data.loc[article_id_1].doc_body))\n",
    "    doc_b = ' '.join(tokenize(data.loc[article_id_2].doc_body))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examinate details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]\n",
    "\n",
    "\n",
    "\n",
    "def _compute_article_title_similarity(article_id_1, article_id_2, data=df_nlp):\n",
    "    # Compute the consine similarity based on tfidf of title (doc_full_name)\n",
    "    # think of title tag for seo pagerank\n",
    "    \n",
    "    # This is a private helper function that is used by overall similarity function.\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(data.loc[article_id_1].doc_full_name))\n",
    "    doc_b = ' '.join(tokenize(data.loc[article_id_2].doc_full_name))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examinate details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]\n",
    "\n",
    "\n",
    "\n",
    "def _compute_article_desc_similarity(article_id_1, article_id_2, data=df_nlp):\n",
    "    # Compute the consine similarity based on tfidf of desc content (doc_description)\n",
    "    # think of desc tag for seo pagerank\n",
    "    \n",
    "    # This is a private helper function that is used by overall similarity function.\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(data.loc[article_id_1].doc_description))\n",
    "    doc_b = ' '.join(tokenize(data.loc[article_id_2].doc_description))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_article_similarity(article_id_1, article_id_2):\n",
    "    \"\"\"\n",
    "    Cosine similarty of overall content details, \n",
    "    in consideration of: similarity_title, similarity_body, similarity_desc\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate similary for body, title, desc, then combine a single value.\n",
    "    # think about how google weight title,desc,body. Title tag is very heavy. SEO-wised\n",
    "    # so having 3 consine similarity values, then do a normailized one. \n",
    "    # what's the formular? think of a course, assignments weight x, final exam weight y,\n",
    "    # then what's total grade.\n",
    "    # https://www.indeed.com/career-advice/career-development/how-to-calculate-weighted-average\n",
    "    \n",
    "    similarity_title = _compute_article_title_similarity(article_id_1,article_id_2)\n",
    "    similarity_body = _compute_article_body_similarity(article_id_1,article_id_2)\n",
    "    similarity_desc = _compute_article_desc_similarity(article_id_1,article_id_2)\n",
    "    \n",
    "    # a weighted sum caluculation for final score\n",
    "    # I give title 0.5 weight, body 0.4 weight, desc 0.1 weight. (adjust if necessary)\n",
    "    \n",
    "    overall = similarity_title * 0.5 + similarity_body * 0.4 + similarity_desc * 0.1\n",
    "    \n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07122043497708203"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot Check:\n",
    "\n",
    "compute_article_similarity(55, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find similar articles for a given article.\n",
    "\n",
    "This only works on the df_content/df_nlp dataset. \n",
    "\n",
    "Use full details of an article. (title, desc, body), this info only available in the df_content/df_nlp dataset\n",
    "Calculate in real-time. It might take some seconds. \n",
    "(Because it loops to all articles against the targeted article, use cosine similarity, not dot product.)\n",
    "For articles not in the df_content/df_nlp dataset, another \n",
    "\n",
    "method is suitable loopup_similar_title_articles(), which checks only on the title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar articles for a given article based on content similarity.(overall of title+desc+body)\n",
    "# This only works on the df_content/df_nlp dataset. \n",
    "# Use full details of an article. (title, desc, body), this info only available in the df_content/df_nlp dataset\n",
    "# Calculate in real-time. It might take some seconds. \n",
    "# (since it loops to all articles against the targeted article, use cosine similarity, not dot product.)\n",
    "# For articles not in the df_content/df_nlp dataset, another \n",
    "# method is suitable loopup_similar_title_articles(), which checks only on the title. \n",
    "\n",
    "\n",
    "def find_similar_content_articles(article_id, data=df_nlp):\n",
    "    \n",
    "    article_ids_all = data.index\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for i in article_ids_all:\n",
    "        # print(f\"\\n[i]: {i}\")\n",
    "        \n",
    "        if i == article_id:\n",
    "            continue\n",
    "\n",
    "        similarity_score = compute_article_similarity(article_id,i)\n",
    "        similarity_dict[i] = similarity_score\n",
    "        # print(f\"[similarity_score]: {similarity_score}\")\n",
    "    \n",
    "    \n",
    "    similarity_ds = pd.Series(data=similarity_dict.values(), \n",
    "                           index=similarity_dict.keys()).sort_values(ascending=False).index\n",
    "    \n",
    "    return list(similarity_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32945080200831095 seconds\n",
      "[389, 993, 949, 592, 714, 117, 678, 942, 231, 925, 15, 463, 353, 835, 907, 284, 977, 600, 595, 997]\n"
     ]
    }
   ],
   "source": [
    "# Spot Check: \n",
    "# Articles that relevant to article id 420, based on content details similarity (title, desc, body)\n",
    "# Calculate in real time, it might take some seconds\n",
    "# (Because it loops to all articles against the targeted article, use language consine similarity not dot product.)\n",
    "start = time.time()\n",
    "relevant_content_articles_for_420th = find_similar_content_articles(420)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{(end - start) / 60} seconds\")\n",
    "print(relevant_content_articles_for_420th[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Apache Spark™ 2.0: Impressive Improvements to ...</td>\n",
       "      <td>What a difference a version number makes! With...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Configuring the Apache Spark SQL Context</td>\n",
       "      <td>The Apache Spark website documents the propert...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Apache Spark SQL Analyzer Resolves Order-by Co...</td>\n",
       "      <td>The Apache Spark SQL component has several sub...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Apache Spark Analytics</td>\n",
       "      <td>Combine Apache® Spark™ with other cloud servic...</td>\n",
       "      <td>APACHE SPARK ANALYTICSCombine Apache® Spark™ w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>A Survey of Books about Apache Spark™</td>\n",
       "      <td>From the big crop of books about Apache Spark™...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Apache Spark™ 2.0: Migrating Applications</td>\n",
       "      <td>This post provides a brief summary of sample c...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>Spark SQL - Rapid Performance Evolution</td>\n",
       "      <td>Spark SQL Version 1.6 runs queries faster! Tha...</td>\n",
       "      <td>{ spark .tc } * Community\\r\\n * Projects\\r\\n *...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>Interview with Sean Li, New Apache Spark™ Comm...</td>\n",
       "      <td>Sean looks back on his first encounter with Sp...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Speed your SQL Queries with Spark SQL</td>\n",
       "      <td>Get faster queries and write less code too. Le...</td>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Build SQL queries with Apache Spark in DSX</td>\n",
       "      <td>This video shows you how to use the Spark SQL ...</td>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Apache Spark™ 2.0: Extend Structured Streaming...</td>\n",
       "      <td>Early methods to integrate machine learning us...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>What is Spark?</td>\n",
       "      <td>Learn Why and How To Use Spark for large amoun...</td>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>sparklyr — R interface for Apache Spark</td>\n",
       "      <td>We’re excited today to announce sparklyr, a ne...</td>\n",
       "      <td>RStudio Blog * Home\\r\\n\\r\\n * Subscribe to fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Build SQL Queries in a Scala notebook using Ap...</td>\n",
       "      <td>How to build SQL Queries in a Scala notebook u...</td>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Build Spark SQL Queries</td>\n",
       "      <td>How to build SQL Queries in a Scala notebook u...</td>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Apache Spark 2.0: Machine Learning. Under the ...</td>\n",
       "      <td>Now that the dust has settled on Apache Spark ...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Apache Spark as the New Engine of Genomics</td>\n",
       "      <td>A handful of talks at the recent Spark Summit ...</td>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Access IBM Analytics for Apache Spark from RSt...</td>\n",
       "      <td>In this post I will show you how to use the IB...</td>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Load dashDB Data with Apache Spark</td>\n",
       "      <td>Learn how to create a connection to dashDB dat...</td>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Apache Spark: Upgrade and speed-up your analytics</td>\n",
       "      <td>One of the best things about Apache Spark is t...</td>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                doc_full_name  \\\n",
       "article_id                                                      \n",
       "389         Apache Spark™ 2.0: Impressive Improvements to ...   \n",
       "993                  Configuring the Apache Spark SQL Context   \n",
       "949         Apache Spark SQL Analyzer Resolves Order-by Co...   \n",
       "592                                    Apache Spark Analytics   \n",
       "714                     A Survey of Books about Apache Spark™   \n",
       "117                 Apache Spark™ 2.0: Migrating Applications   \n",
       "678                   Spark SQL - Rapid Performance Evolution   \n",
       "942         Interview with Sean Li, New Apache Spark™ Comm...   \n",
       "231                     Speed your SQL Queries with Spark SQL   \n",
       "925                Build SQL queries with Apache Spark in DSX   \n",
       "15          Apache Spark™ 2.0: Extend Structured Streaming...   \n",
       "463                                            What is Spark?   \n",
       "353                   sparklyr — R interface for Apache Spark   \n",
       "835         Build SQL Queries in a Scala notebook using Ap...   \n",
       "907                                   Build Spark SQL Queries   \n",
       "284         Apache Spark 2.0: Machine Learning. Under the ...   \n",
       "977                Apache Spark as the New Engine of Genomics   \n",
       "600         Access IBM Analytics for Apache Spark from RSt...   \n",
       "595                        Load dashDB Data with Apache Spark   \n",
       "997         Apache Spark: Upgrade and speed-up your analytics   \n",
       "\n",
       "                                              doc_description  \\\n",
       "article_id                                                      \n",
       "389         What a difference a version number makes! With...   \n",
       "993         The Apache Spark website documents the propert...   \n",
       "949         The Apache Spark SQL component has several sub...   \n",
       "592         Combine Apache® Spark™ with other cloud servic...   \n",
       "714         From the big crop of books about Apache Spark™...   \n",
       "117         This post provides a brief summary of sample c...   \n",
       "678         Spark SQL Version 1.6 runs queries faster! Tha...   \n",
       "942         Sean looks back on his first encounter with Sp...   \n",
       "231         Get faster queries and write less code too. Le...   \n",
       "925         This video shows you how to use the Spark SQL ...   \n",
       "15          Early methods to integrate machine learning us...   \n",
       "463         Learn Why and How To Use Spark for large amoun...   \n",
       "353         We’re excited today to announce sparklyr, a ne...   \n",
       "835         How to build SQL Queries in a Scala notebook u...   \n",
       "907         How to build SQL Queries in a Scala notebook u...   \n",
       "284         Now that the dust has settled on Apache Spark ...   \n",
       "977         A handful of talks at the recent Spark Summit ...   \n",
       "600         In this post I will show you how to use the IB...   \n",
       "595         Learn how to create a connection to dashDB dat...   \n",
       "997         One of the best things about Apache Spark is t...   \n",
       "\n",
       "                                                     doc_body  \n",
       "article_id                                                     \n",
       "389         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "993         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "949         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "592         APACHE SPARK ANALYTICSCombine Apache® Spark™ w...  \n",
       "714         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "117         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "678         { spark .tc } * Community\\r\\n * Projects\\r\\n *...  \n",
       "942         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "231         Skip to main content IBM developerWorks / Deve...  \n",
       "925         Skip navigation Sign in SearchLoading...\\r\\n\\r...  \n",
       "15          * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "463         Skip navigation Upload Sign in SearchLoading.....  \n",
       "353         RStudio Blog * Home\\r\\n\\r\\n * Subscribe to fee...  \n",
       "835         Skip to main content IBM developerWorks / Deve...  \n",
       "907         Skip to main content IBM developerWorks / Deve...  \n",
       "284         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "977         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...  \n",
       "600         Homepage Follow Sign in / Sign up Homepage * H...  \n",
       "595         Skip to main content IBM developerWorks / Deve...  \n",
       "997         Skip to main content IBM developerWorks / Deve...  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot Check:\n",
    "# Checking relavancy. They are ordered by relavancy. \n",
    "\n",
    "df_nlp.iloc[relevant_content_articles_for_420th].head(20)[['doc_full_name','doc_description','doc_body']]\n",
    "\n",
    "# Looks like related to 'Apache Spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughs\n",
    "\n",
    "The above find_similar_content_articles() work well. However, it is calculating on the fly, could take time to load. \n",
    "\n",
    "Such real-time calculating is too expensive in terms of user experience. (Waited xx seconds for similar articles)\n",
    "\n",
    "To improve the experience, we can pre-calculate or cache the cosine similarity scores.\n",
    "\n",
    "So make an article_article_similary data frame for lookup. (the content-content recs). \n",
    "\n",
    "(people who view X article also might be interested in Y article based on the relevancy of article_article NLP cosine similarity.)\n",
    "\n",
    "#### Approach\n",
    "\n",
    "Make a dataframe, store article-article-similarity. (Based on the df_nlp dataframe, the cleaned df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1041</th>\n",
       "      <th>1042</th>\n",
       "      <th>1043</th>\n",
       "      <th>1044</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "article_id                                                              ...   \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "article_id  1041  1042  1043  1044  1045  1046  1047  1048  1049  1050  \n",
       "article_id                                                              \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1051 columns]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a article_article dummy dataframe based on shape of df_nlp\n",
    "\n",
    "article_article = pd.DataFrame(\n",
    "    data=np.zeros((len(df_nlp.index), len(df_nlp.index))),\n",
    "    index=df_nlp.index,\n",
    "    columns=df_nlp.index\n",
    ")\n",
    "\n",
    "article_article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute article_article cosine similarity for each article, and update to the article_article dataframe\n",
    "\n",
    "# ATTENTION: THE ITERATION TAKES HOURS to finish. 1050 x 1050 instances. \n",
    "\n",
    "def compute_and_update_similarity():\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    # Number of articles\n",
    "    len_articles = article_article.shape[0]\n",
    "\n",
    "    # List of index of articles\n",
    "    idx_articles = list(article_article.index)\n",
    "\n",
    "\n",
    "    # Loop thru each article: for each row, loop each columns. \n",
    "    for i in idx_articles:\n",
    "        print(f\"Processing row {i} of {len_articles-1}.\")\n",
    "\n",
    "        for j in idx_articles:\n",
    "            article_article.loc[i, j] = compute_article_similarity(i, j)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"{(end - start) / 60} seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment the below cell ONLY if want to run the compute and update again. (Warning: took hours)\n",
    "\n",
    "Basically you don't need to do so, as I have already done it and exported to pkl.\n",
    "(**It took 8 hours**)\n",
    "\n",
    "Ready to use. You can download from this link:\n",
    "\n",
    "Just load it and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check info above. Uncomment only if necessary\n",
    "\n",
    "# compute_and_update_similarity()\n",
    "# article_article.to_pickle('article_article_similarity_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1041</th>\n",
       "      <th>1042</th>\n",
       "      <th>1043</th>\n",
       "      <th>1044</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032471</td>\n",
       "      <td>0.072460</td>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.259333</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>0.046419</td>\n",
       "      <td>0.047824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063201</td>\n",
       "      <td>0.051671</td>\n",
       "      <td>0.143709</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.032471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186967</td>\n",
       "      <td>0.028671</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>0.098205</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>0.155317</td>\n",
       "      <td>0.206362</td>\n",
       "      <td>0.103742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104562</td>\n",
       "      <td>0.152361</td>\n",
       "      <td>0.093032</td>\n",
       "      <td>0.030101</td>\n",
       "      <td>0.082490</td>\n",
       "      <td>0.014128</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>0.090637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072460</td>\n",
       "      <td>0.186967</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053018</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>0.041646</td>\n",
       "      <td>0.204603</td>\n",
       "      <td>0.339824</td>\n",
       "      <td>0.153497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149979</td>\n",
       "      <td>0.208527</td>\n",
       "      <td>0.139453</td>\n",
       "      <td>0.036274</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.104403</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.160396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.028671</td>\n",
       "      <td>0.053018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>0.037732</td>\n",
       "      <td>0.084109</td>\n",
       "      <td>0.048761</td>\n",
       "      <td>0.031276</td>\n",
       "      <td>0.049991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.033499</td>\n",
       "      <td>0.010938</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.036531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.259333</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095990</td>\n",
       "      <td>0.026990</td>\n",
       "      <td>0.154261</td>\n",
       "      <td>0.135846</td>\n",
       "      <td>0.152328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122135</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.137278</td>\n",
       "      <td>0.024670</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.072108</td>\n",
       "      <td>0.138237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id      0         1         2         3         4         5     \\\n",
       "article_id                                                               \n",
       "0           1.000000  0.032471  0.072460  0.020439  0.259333  0.033410   \n",
       "1           0.032471  1.000000  0.186967  0.028671  0.093232  0.098205   \n",
       "2           0.072460  0.186967  1.000000  0.053018  0.172458  0.106455   \n",
       "3           0.020439  0.028671  0.053018  1.000000  0.027609  0.037732   \n",
       "4           0.259333  0.093232  0.172458  0.027609  1.000000  0.095990   \n",
       "\n",
       "article_id      6         7         8         9     ...      1041      1042  \\\n",
       "article_id                                          ...                       \n",
       "0           0.016156  0.051355  0.046419  0.047824  ...  0.063201  0.051671   \n",
       "1           0.035012  0.155317  0.206362  0.103742  ...  0.104562  0.152361   \n",
       "2           0.041646  0.204603  0.339824  0.153497  ...  0.149979  0.208527   \n",
       "3           0.084109  0.048761  0.031276  0.049991  ...  0.027299  0.026927   \n",
       "4           0.026990  0.154261  0.135846  0.152328  ...  0.122135  0.141956   \n",
       "\n",
       "article_id      1043      1044      1045      1046      1047      1048  \\\n",
       "article_id                                                               \n",
       "0           0.143709  0.020163  0.010646  0.003492  0.004095  0.028436   \n",
       "1           0.093032  0.030101  0.082490  0.014128  0.019350  0.056505   \n",
       "2           0.139453  0.036274  0.009098  0.009416  0.013930  0.104403   \n",
       "3           0.033499  0.010938  0.036107  0.012851  0.011922  0.010652   \n",
       "4           0.137278  0.024670  0.007071  0.006955  0.003037  0.038724   \n",
       "\n",
       "article_id      1049      1050  \n",
       "article_id                      \n",
       "0           0.000000  0.078722  \n",
       "1           0.010927  0.090637  \n",
       "2           0.018914  0.160396  \n",
       "3           0.009425  0.036531  \n",
       "4           0.072108  0.138237  \n",
       "\n",
       "[5 rows x 1051 columns]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from pre-calculated pkl. \n",
    "\n",
    "article_article = pd.read_pickle('article_article_similarity_df.pkl')\n",
    "\n",
    "\n",
    "# All calculated and cached cosine similarity score for every article. Ready to use.\n",
    "article_article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of finding in real time calculation with find_similar_content_articles(),\n",
    "# this is another approach that uses a lookup from pre-calculated\n",
    "# article_article df. It saves time for user.\n",
    "\n",
    "def loopup_similar_content_articles(article_id, data=article_article, n=20):\n",
    "    \"\"\"\n",
    "    NOTE THAT: the cosine similarity score is based on content: title + desc + body, \n",
    "    therefore only ids are in df_content are available to work with this method. \n",
    "    Otherwise, use loopup_similar_title_articles()\n",
    "    \n",
    "    input: n - number of top similar to return\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    if article_id in article_article.index.values:\n",
    "\n",
    "        ids = list(\n",
    "            data.loc[article_id][data.loc[article_id].index != article_id].sort_values(ascending=False).head(n).index)\n",
    "        names = list(df_nlp.loc[ids].doc_full_name.values)\n",
    "\n",
    "    else:\n",
    "        print(f\"Article {article_id} is not in df_content.\")\n",
    "        print(f\"We are unable to compute overall content similarity for it.\")\n",
    "        print(f\"Alternatively, you might try loopup_similar_title_articles() which is based on title relevancy.\")\n",
    "        ids = []\n",
    "        names = []\n",
    "\n",
    "    return ids, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800, 1035, 313, 805, 444, 721, 260, 967, 122, 96, 384, 809, 124, 723, 234, 892, 54, 253, 812, 861, 412, 732, 871, 74, 221, 89, 479, 616, 500, 567]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Machine Learning for the Enterprise',\n",
       " 'Machine Learning for the Enterprise.',\n",
       " 'What is machine learning?',\n",
       " 'Machine Learning for everyone',\n",
       " 'Declarative Machine Learning',\n",
       " 'The power of machine learning in Spark',\n",
       " 'The Machine Learning Database',\n",
       " 'ML Algorithm != Learning Machine',\n",
       " 'Watson Machine Learning for Developers',\n",
       " 'Improving quality of life with Spark-empowered machine learning',\n",
       " 'Continuous Learning on Watson',\n",
       " 'Use the Machine Learning Library',\n",
       " 'Python Machine Learning: Scikit-Learn Tutorial',\n",
       " '10 Essential Algorithms For Machine Learning Engineers',\n",
       " '3 Scenarios for Machine Learning on Multicloud',\n",
       " 'Breaking the 80/20 rule: How data catalogs transform data scientists’ productivity',\n",
       " '8 ways to turn data into value with Apache Spark machine learning',\n",
       " 'Lifelong (machine) learning: how automation can help your models get smarter over time',\n",
       " 'Machine Learning Exercises In Python, Part 1',\n",
       " 'Cleaning the swamp: Turn your data lake into a source of crystal-clear insight',\n",
       " 'Adoption of machine learning to software failure prediction',\n",
       " 'Rapidly build Machine Learning flows with DSX',\n",
       " 'Overfitting in Machine Learning: What It Is and How to Prevent It',\n",
       " 'The 3 Kinds of Context: Machine Learning and the Art of the Frame',\n",
       " 'How smart catalogs can turn the big data flood into an ocean of opportunity',\n",
       " 'Top 20 R Machine Learning and Data Science packages',\n",
       " 'Drowning in data sources: How data cataloging could fix your findability problems',\n",
       " 'Three reasons machine learning models go out of sync',\n",
       " 'The Difference Between AI, Machine Learning, and Deep Learning?',\n",
       " 'You could be looking at it all wrong']"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot Check:\n",
    "# Lookup most relevant articles for 455th article\n",
    "\n",
    "relevant_for_455th_article = loopup_similar_content_articles(455, n=30)\n",
    "\n",
    "print(relevant_for_455th_article[0])\n",
    "relevant_for_455th_article[1]\n",
    "\n",
    "# Revealing that they are about on 'machine learning'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note on Datasets Inconsistency: \n",
    "\n",
    "- df_content (page content of articles)\n",
    "- df interaction between users and articles)\n",
    "\n",
    "Since that original df_content only contains articles from [0 - 1050].\n",
    "\n",
    "However, df (the interaction) dataset shows more unique article ids than those beyond 1050. e,g 11xx, 12xx, 13xx, 14xx.\n",
    "\n",
    "So these two datasets' articles are not up to date.\n",
    "\n",
    "The given df_content is 'late' (not catch up), which means that:\n",
    "\n",
    "**articles in the df (interaction) dataset might not be found in the df_content (content info) dataset.**\n",
    "\n",
    "Therefore, due to this inconsistency, we will not see content details for some articles mentioned in df (interaction set).\n",
    "\n",
    "The inconsistency is a limitation of given datasets. \n",
    "\n",
    "---\n",
    "\n",
    "Due to the limitation of inconsistency mentioned above,\n",
    "\n",
    "Let's create a unified article data frame that **stores every unique article's \\[ID\\] and \\[TITLE\\]**.\n",
    "\n",
    "This dataset serves the purpose of lookup the article's title. (Think of an index of all articles).\n",
    "\n",
    "Because of the inconsistency, the bottomline: **not every article has body/desc info, but all articles DO have titles**.\n",
    "\n",
    "The idea is that: **for those articles that cannot do NLP processing on \\[TITLE + DESC + BODY\\] content, we at least can process NLP on \\[TITLE\\] for them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1440</td>\n",
       "      <td>world marriage data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1441</td>\n",
       "      <td>world tourism data by the world tourism organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1442</td>\n",
       "      <td>worldwide county and region - national account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1443</td>\n",
       "      <td>worldwide electricity demand and production 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1444</td>\n",
       "      <td>worldwide fuel oil consumption by household (i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1328 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                              title\n",
       "article_id                                                              \n",
       "0                   0  Detect Malfunctioning IoT Sensors with Streami...\n",
       "1                   1  Communicating data science: A guide to present...\n",
       "2                   2         This Week in Data Science (April 18, 2017)\n",
       "3                   3  DataLayer Conference: Boost the performance of...\n",
       "4                   4      Analyze NY Restaurant data using Spark in DSX\n",
       "...               ...                                                ...\n",
       "1440             1440                                world marriage data\n",
       "1441             1441  world tourism data by the world tourism organi...\n",
       "1442             1442  worldwide county and region - national account...\n",
       "1443             1443  worldwide electricity demand and production 19...\n",
       "1444             1444  worldwide fuel oil consumption by household (i...\n",
       "\n",
       "[1328 rows x 2 columns]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a all titles dataframe. Store every unique article.\n",
    "# Schema: article id, article title\n",
    "\n",
    "def make_titles_df(df=df, df_content=df_content):\n",
    "    \"\"\"\n",
    "    Generate an article index dataframe, from orginal df and df_content\n",
    "    Only store article id and title for every unique article.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    unique_ids_in_df = sorted(list(df.article_id.astype('int64').value_counts().index))\n",
    "    unique_ids_in_df_content = sorted(list(df_content.article_id.astype('int64').value_counts().index))\n",
    "    \n",
    "    # ids that are not in df_content, but appear in df\n",
    "    ids_not_in_df_content = list(set(unique_ids_in_df) - set(unique_ids_in_df_content))\n",
    "    # print(f\"How many?: {len(ids_not_in_df_content)}\")\n",
    "    \n",
    "    # Subset the diff articles dataframe\n",
    "    ids_not_in_df_content_float = [float(x) for x in ids_not_in_df_content]\n",
    "    df_diff = df[df.article_id.isin(ids_not_in_df_content_float)][['article_id', 'title']].drop_duplicates()\n",
    "    df_diff.article_id = df_diff.article_id.astype('int64')\n",
    "    df_diff = df_diff.sort_values(by='article_id', ascending=True)\n",
    "    \n",
    "    # Subset from df_content\n",
    "    df_content_titles_subset = df_content[['article_id', 'doc_full_name']]\n",
    "    df_content_titles_subset.columns = [['article_id', 'title']]\n",
    "    df_content_titles_subset.index = df_content_titles_subset['article_id'].values.flatten()\n",
    "    \n",
    "    # convert to numpy array shape and concatenate\n",
    "    a = df_diff.to_numpy()\n",
    "    b = df_content_titles_subset.to_numpy()\n",
    "    all_titles_np = np.concatenate((a, b))\n",
    "    \n",
    "    # make a dataframe\n",
    "    titles_df = pd.DataFrame(all_titles_np)\n",
    "    titles_df.columns = ['article_id', 'title']\n",
    "    titles_df.index = titles_df.article_id\n",
    "    titles_df = titles_df.sort_index()\n",
    "    \n",
    "    return titles_df\n",
    "    \n",
    "    \n",
    "    \n",
    "# Get titles df\n",
    "titles_df = make_titles_df()\n",
    "\n",
    "# Spot check titles df\n",
    "titles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also Do Similarty Finding Based on Title only \n",
    "\n",
    "As we can see, we do have articles that have no content details avaiable.\n",
    "\n",
    "For these items, we use title to find similarity. \n",
    "\n",
    "We can do real-time compuation across all, or like above, we precalculate and cache to title-title dataframe for lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar articles for a given article based on title only similarity.\n",
    "# This only works on all articles in df or df_content.\n",
    "# Use only the TITLE of an article. this info available in both df and df_content\n",
    "# Calculate in real-time. It might take some seconds. \n",
    "# (since it loops to all articles against the targeted article, use cosine similarity, not dot product.)\n",
    "# For articles also in the df_content/df_nlp dataset, another \n",
    "# method is suitable loopup_similar_content_articles(), which checks based on more content information. \n",
    "\n",
    "\n",
    "def find_similar_title_articles(article_id, data=titles_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Note that it is based on page title only.\n",
    "    According orginal datasets, every article has title, not not all article have body/desc info.\n",
    "    \n",
    "    If the article is in df_content, we consider it as 'lucky', we can use NLP based on content.\n",
    "    So we also check at the end, see if it is 'lucky', if yes, we give it a reminder / hint that\n",
    "    notice we can run find_similar_content_articles() with the article also.\n",
    "    \"\"\"\n",
    "    \n",
    "    if article_id not in titles_df.index:\n",
    "        print(f\"Article {article_id} is not found.\")\n",
    "        print(f\"Please double check the id or try another.\")\n",
    "        result = []\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # similarities holder\n",
    "        similarity_dict = {}\n",
    "\n",
    "        # loop thru article_id aginst every article\n",
    "\n",
    "        for i in data.article_id.to_list():\n",
    "            # print(f\"\\n[i]: {i}\")\n",
    "\n",
    "            if i == article_id:\n",
    "                # if against self, skip to next loop\n",
    "                continue\n",
    "\n",
    "            # Tokenize self and another. \n",
    "            # Rejoin as documents.\n",
    "            doc_a = ' '.join(tokenize(data.loc[article_id].title))\n",
    "            doc_b = ' '.join(tokenize(data.loc[i].title))\n",
    "\n",
    "\n",
    "            # combine to a list of documents\n",
    "            documents = [doc_a, doc_b]\n",
    "\n",
    "\n",
    "            # instanciate a scikitlearn tfidf vecorizer\n",
    "            vectorizer = TfidfVectorizer()\n",
    "\n",
    "            # fit transform to get a sparse matrix\n",
    "            matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "            # # Uncomment belows to explore details (term features, tfidf values)\n",
    "            # ====================================\n",
    "            # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "            # # convert to readable array \n",
    "            # matrix_array = matrix.toarray()\n",
    "\n",
    "            # # assemble to a dataframe for explor \n",
    "            # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "            # ====================================\n",
    "\n",
    "            similarity_score = cosine_similarity(matrix[:1], matrix[1:])[0][0]\n",
    "            similarity_dict[i] = similarity_score\n",
    "            # print(f\"[similarity_score]: {similarity_score}\")\n",
    "\n",
    "\n",
    "        similarity_ds = pd.Series(data=similarity_dict.values(), \n",
    "                               index=similarity_dict.keys()).sort_values(ascending=False).index\n",
    "\n",
    "        result = list(similarity_ds)\n",
    "        \n",
    "        # Reminder check. \n",
    "        if article_id in df_content.article_id.index:\n",
    "            print(f\"We also have content details (desc, body) information for this article: {article_id}.\")\n",
    "            print(f\"Alternatively, you might want to try find_similar_content_articles() for it.\")\n",
    "            print(f\"It might give you better result, because it checks on title, desc, body\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>1420</td>\n",
       "      <td>use apache systemml and spark for machine lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>Use the Machine Learning Library in Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>893</td>\n",
       "      <td>Use the Machine Learning Library in IBM Analyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>1172</td>\n",
       "      <td>apache spark lab, part 3: machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>284</td>\n",
       "      <td>Apache Spark 2.0: Machine Learning. Under the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>Building Custom Machine Learning Algorithms Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>809</td>\n",
       "      <td>Use the Machine Learning Library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>721</td>\n",
       "      <td>The power of machine learning in Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>313</td>\n",
       "      <td>What is machine learning?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>375</td>\n",
       "      <td>Apache SystemML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>8 ways to turn data into value with Apache Spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>762</td>\n",
       "      <td>From Machine Learning to Learning Machine (Din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>1354</td>\n",
       "      <td>movie recommender system with spark machine le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>260</td>\n",
       "      <td>The Machine Learning Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>805</td>\n",
       "      <td>Machine Learning for everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>1049</td>\n",
       "      <td>Use dashDB with Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>1035</td>\n",
       "      <td>Machine Learning for the Enterprise.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>444</td>\n",
       "      <td>Declarative Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>800</td>\n",
       "      <td>Machine Learning for the Enterprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>592</td>\n",
       "      <td>Apache Spark Analytics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                              title\n",
       "article_id                                                              \n",
       "1420             1420  use apache systemml and spark for machine lear...\n",
       "161               161          Use the Machine Learning Library in Spark\n",
       "893               893  Use the Machine Learning Library in IBM Analyt...\n",
       "1172             1172         apache spark lab, part 3: machine learning\n",
       "284               284  Apache Spark 2.0: Machine Learning. Under the ...\n",
       "112               112  Building Custom Machine Learning Algorithms Wi...\n",
       "809               809                   Use the Machine Learning Library\n",
       "721               721             The power of machine learning in Spark\n",
       "313               313                          What is machine learning?\n",
       "375               375                                    Apache SystemML\n",
       "54                 54  8 ways to turn data into value with Apache Spa...\n",
       "762               762  From Machine Learning to Learning Machine (Din...\n",
       "1354             1354  movie recommender system with spark machine le...\n",
       "260               260                      The Machine Learning Database\n",
       "805               805                      Machine Learning for everyone\n",
       "1049             1049                              Use dashDB with Spark\n",
       "1035             1035               Machine Learning for the Enterprise.\n",
       "444               444                       Declarative Machine Learning\n",
       "800               800                Machine Learning for the Enterprise\n",
       "592               592                             Apache Spark Analytics"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot Check:\n",
    "\n",
    "# Relevant articles for 1420, based on title NLP cosine calculation.\n",
    "\n",
    "relevant_title_articles_for_1420th = find_similar_title_articles(1420)\n",
    "\n",
    "titles_df.loc[[1420] + relevant_title_articles_for_1420th].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: get n-grams (two grams here)\n",
    "\n",
    "# Why 2-grams, not 3-grams or everygram? Because thru experiment, I found \n",
    "# 2 grams generalize the meaning well, \n",
    "# 3 grams often off the track, every-gram went too verbose and miss out meaning\n",
    "# It is case by case, however for these articles, 2 grams works well. \n",
    "\n",
    "\n",
    "def get_content_bigrams_by_article_id(article_id, data=df_nlp):\n",
    "    \"\"\"\n",
    "    Input an article id, return the bigrams results of its content\n",
    "    Then Reorder grams By most frequent.\n",
    "    \n",
    "    Note that the content is combination string of title, desc, body.\n",
    "    So assume that the article id is IN df_content, if the artcile is not in, we will\n",
    "    not be able to get its decs, body content.\n",
    "    \n",
    "    For those artcile not in df_content, only in df (interaction set), use alternative method \n",
    "    get_title_bigrams_by_article_id(), which get ngrams only from title.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if article_id not in data.index:\n",
    "        print(f\"{article_id} is not found in article content details dataset.\")\n",
    "        print(f\"You can try ngrams from title. \\nAlternative method: get_title_bigrams_by_article_id()\")\n",
    "        return []\n",
    "    \n",
    "    else:\n",
    "        title = data.loc[article_id].doc_full_name\n",
    "        desc = data.loc[article_id].doc_description\n",
    "        body = data.loc[article_id].doc_body\n",
    "\n",
    "        # a list of tokens\n",
    "        tokened_text = tokenize(title) + tokenize(desc) + tokenize(body)\n",
    "\n",
    "        # ngrams 2\n",
    "        grams = [f\"{x[0]} {x[1]}\" for x in list(bigrams(tokened_text))]\n",
    "\n",
    "        # sort with frequency descending\n",
    "        sorted_grams = [x[0] for x in Counter(grams).most_common()]\n",
    "\n",
    "        return sorted_grams\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def get_title_bigrams_by_article_id(article_id, data=titles_df):\n",
    "    \"\"\"\n",
    "    Input an article id, return the bigrams results of its title\n",
    "    Then Reorder grams By most frequent.\n",
    "    \n",
    "    Note that it is based on page title only.\n",
    "    According orginal datasets, every article has title, not not all article have body/desc info.\n",
    "    \n",
    "    If the article is in df_content, we consider it as 'lucky', we can use NLP on content.\n",
    "    So we do another check here, see if it is 'lucky', if yes, we give it a reminder / hint that\n",
    "    notice we do run get_content_bigrams_by_article_id() with the article also.\n",
    "    \n",
    "    \"\"\"\n",
    "    if article_id not in data.index:\n",
    "        print(f\"{article_id} is not found in article titles dataset.\")\n",
    "        print(f\"We don't have any information for this article. \\nPlease check article id again.\")\n",
    "        return []\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        title = data.loc[article_id].title\n",
    "\n",
    "        # a list of tokens\n",
    "        tokened_text = tokenize(title)\n",
    "\n",
    "        # ngrams 2\n",
    "        grams = [f\"{x[0]} {x[1]}\" for x in list(bigrams(tokened_text))]\n",
    "\n",
    "        # sort with frequency descending\n",
    "        sorted_grams = [x[0] for x in Counter(grams).most_common()]\n",
    "        \n",
    "        # Reminder check\n",
    "        if article_id in df_content.article_id.index:\n",
    "            print(f\"We also have content details, e,g desc, body information for this article: {article_id}.\")\n",
    "            print(f\"Alternatively, you might want to try get_content_bigrams_by_article_id() for it.\")\n",
    "        \n",
    "        return sorted_grams\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logical plan',\n",
       " 'apache spark',\n",
       " 'spark sql',\n",
       " 'c 3',\n",
       " 'sort operator',\n",
       " 'parsed logical',\n",
       " 'make sure',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'spark spark',\n",
       " 'aggregate operator',\n",
       " 'a1 14',\n",
       " 'a2 15',\n",
       " 'sql analyzer',\n",
       " 'analyzer resolve',\n",
       " 'resolve order',\n",
       " 'order column',\n",
       " 'column apache',\n",
       " 'sql component',\n",
       " 'component several',\n",
       " 'several sub',\n",
       " 'sub component',\n",
       " 'component including',\n",
       " 'including analyzer',\n",
       " 'analyzer play',\n",
       " 'play important',\n",
       " 'important role',\n",
       " 'role making',\n",
       " 'making sure',\n",
       " 'sure logical',\n",
       " 'plan fully',\n",
       " 'fully resolved',\n",
       " 'resolved end',\n",
       " 'end analysis',\n",
       " 'analysis phase',\n",
       " 'phase analyzer',\n",
       " 'analyzer take',\n",
       " 'take parsed',\n",
       " 'plan input',\n",
       " 'input make',\n",
       " 'sure table',\n",
       " 'table reference',\n",
       " 'reference attribute',\n",
       " 'attribute column',\n",
       " 'column reference',\n",
       " 'reference function',\n",
       " 'function reference',\n",
       " 'reference resolved',\n",
       " 'resolved looking',\n",
       " 'looking metadata',\n",
       " 'metadata catalog',\n",
       " 'catalog work',\n",
       " 'work applying',\n",
       " 'applying set',\n",
       " 'set rule',\n",
       " 'rule logical',\n",
       " 'plan transforming',\n",
       " 'transforming stage',\n",
       " 'stage order',\n",
       " 'order resolve',\n",
       " 'resolve specific',\n",
       " 'specific portion',\n",
       " 'portion plan',\n",
       " 'blog resource',\n",
       " 'resource code',\n",
       " 'code contribution',\n",
       " 'contribution university',\n",
       " 'university ibm',\n",
       " 'ibm design',\n",
       " 'design apache',\n",
       " 'apache systemml',\n",
       " 'systemml apache',\n",
       " 'sql apache',\n",
       " 'working analyzer',\n",
       " 'a1 c',\n",
       " 'c a2',\n",
       " 'attribute referenced',\n",
       " 'sort a1',\n",
       " 'asc true',\n",
       " 'true aggregate',\n",
       " 'aggregate c',\n",
       " 'a1 17',\n",
       " 'a2 18',\n",
       " 'mode complete',\n",
       " 'complete isdistinct',\n",
       " 'isdistinct false',\n",
       " 'false a3',\n",
       " 'a3 19',\n",
       " 'localrelation 1',\n",
       " '1 b',\n",
       " 'b 2',\n",
       " '2 c',\n",
       " '3 4',\n",
       " '4 e',\n",
       " 'e 5',\n",
       " 'operator resolved',\n",
       " 'referenced sort',\n",
       " 'child aggregate',\n",
       " 'order properly',\n",
       " 'properly resolve',\n",
       " 'resolve sort',\n",
       " 'sort resolved',\n",
       " 'a3 16l',\n",
       " 'dilip biswal',\n",
       " 'technology center',\n",
       " 'apache software',\n",
       " 'software foundation',\n",
       " 'plan home',\n",
       " 'home community',\n",
       " 'spark tc',\n",
       " 'tc community',\n",
       " 'plan examine',\n",
       " 'examine working',\n",
       " 'analyzer taking',\n",
       " 'taking example',\n",
       " 'example defect',\n",
       " 'defect describing',\n",
       " 'describing addressed',\n",
       " 'addressed problem',\n",
       " 'problem example',\n",
       " 'example query',\n",
       " 'query select',\n",
       " 'select a1',\n",
       " 'a2 count',\n",
       " 'count c',\n",
       " 'c a3',\n",
       " 'a3 tab',\n",
       " 'tab group',\n",
       " 'group b',\n",
       " 'b order',\n",
       " 'order a1',\n",
       " 'c problem',\n",
       " 'problem description',\n",
       " 'description case',\n",
       " 'case analyzer',\n",
       " 'analyzer unable',\n",
       " 'unable resolve',\n",
       " 'resolve attribute',\n",
       " 'referenced order',\n",
       " 'order clause',\n",
       " 'clause see',\n",
       " 'see let',\n",
       " 'let look',\n",
       " 'look underlying',\n",
       " 'underlying parsed',\n",
       " 'plan parsed',\n",
       " 'plan sort',\n",
       " 'a1 asc',\n",
       " 'asc c',\n",
       " 'c asc',\n",
       " 'c a1',\n",
       " '17 c',\n",
       " '18 count',\n",
       " 'count mode',\n",
       " '19 localrelation',\n",
       " '5 case',\n",
       " 'case localrelation',\n",
       " 'localrelation resolved',\n",
       " 'resolved none',\n",
       " 'none plan',\n",
       " 'plan operator',\n",
       " 'resolved since',\n",
       " 'since underlying',\n",
       " 'underlying attribute',\n",
       " 'attribute refer',\n",
       " 'refer resolved',\n",
       " 'resolved however',\n",
       " 'however see',\n",
       " 'see sort',\n",
       " 'operator aggregate',\n",
       " 'operator attribute',\n",
       " 'resolved output',\n",
       " 'output child',\n",
       " 'operator output',\n",
       " 'output aggregate',\n",
       " 'operator a1',\n",
       " '17 a2',\n",
       " '18 a3',\n",
       " '19 plan',\n",
       " 'plan missing',\n",
       " 'missing attribute',\n",
       " 'attribute c',\n",
       " '3 referenced',\n",
       " 'operator cause',\n",
       " 'cause failure',\n",
       " 'failure analysis',\n",
       " 'analysis process',\n",
       " 'process turn',\n",
       " 'turn result',\n",
       " 'result query',\n",
       " 'query failure',\n",
       " 'failure order',\n",
       " 'operator need',\n",
       " 'need make',\n",
       " 'sure a1',\n",
       " 'a1 sort',\n",
       " 'resolved immediate',\n",
       " 'immediate child',\n",
       " 'c sort',\n",
       " 'resolved grandchild',\n",
       " 'grandchild local',\n",
       " 'local relation',\n",
       " 'relation spark',\n",
       " 'spark analyzer',\n",
       " 'analyzer resolveaggregatefunctions',\n",
       " 'resolveaggregatefunctions rule',\n",
       " 'rule modified',\n",
       " 'modified order',\n",
       " 'operator query',\n",
       " 'query result',\n",
       " 'result following',\n",
       " 'following analyzed',\n",
       " 'analyzed logical',\n",
       " 'plan fix',\n",
       " 'fix project',\n",
       " 'project a1',\n",
       " '14 a2',\n",
       " '15 a3',\n",
       " '16l sort',\n",
       " '14 asc',\n",
       " 'asc a2',\n",
       " '15 asc',\n",
       " 'aggregate 1',\n",
       " '1 c',\n",
       " '3 1',\n",
       " '1 a1',\n",
       " '14 c',\n",
       " '3 a2',\n",
       " '15 count',\n",
       " 'count 1',\n",
       " '1 mode',\n",
       " '16l localrelation',\n",
       " '5 conclusion',\n",
       " 'conclusion hopefully',\n",
       " 'hopefully blog',\n",
       " 'blog give',\n",
       " 'give brief',\n",
       " 'brief insight',\n",
       " 'insight working',\n",
       " 'analyzer post',\n",
       " 'post extended',\n",
       " 'extended description',\n",
       " 'description analyzer',\n",
       " 'analyzer future',\n",
       " 'future general',\n",
       " 'general handling',\n",
       " 'handling analyzer',\n",
       " 'analyzer issue',\n",
       " 'issue requires',\n",
       " 'requires deep',\n",
       " 'deep understanding',\n",
       " 'understanding spark',\n",
       " 'spark logical',\n",
       " 'plan author',\n",
       " 'author dilip',\n",
       " 'biswal senior',\n",
       " 'senior software',\n",
       " 'software engineer',\n",
       " 'engineer spark',\n",
       " 'spark technology',\n",
       " 'center ibm',\n",
       " 'ibm active',\n",
       " 'active apache',\n",
       " 'spark contributor',\n",
       " 'contributor work',\n",
       " 'work open',\n",
       " 'open source',\n",
       " 'source community',\n",
       " 'community experienced',\n",
       " 'experienced relational',\n",
       " 'relational database',\n",
       " 'database distributed',\n",
       " 'distributed computing',\n",
       " 'computing big',\n",
       " 'big data',\n",
       " 'data analytics',\n",
       " 'analytics extensively',\n",
       " 'extensively worked',\n",
       " 'worked sql',\n",
       " 'sql engine',\n",
       " 'engine like',\n",
       " 'like informix',\n",
       " 'informix derby',\n",
       " 'derby big',\n",
       " 'big sql',\n",
       " 'sql share',\n",
       " 'share share',\n",
       " 'share dilip',\n",
       " 'biswal date',\n",
       " 'date 05',\n",
       " '05 june',\n",
       " 'june 2016tags',\n",
       " '2016tags spark',\n",
       " 'apache sparkspark',\n",
       " 'sparkspark technology',\n",
       " 'center community',\n",
       " 'blog apache',\n",
       " 'foundation affiliation',\n",
       " 'affiliation endorse',\n",
       " 'endorse review',\n",
       " 'review material',\n",
       " 'material provided',\n",
       " 'provided website',\n",
       " 'website managed',\n",
       " 'managed ibm',\n",
       " 'ibm apache',\n",
       " 'apache apache',\n",
       " 'spark trademark',\n",
       " 'trademark apache',\n",
       " 'foundation united',\n",
       " 'united state',\n",
       " 'state country']"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot Check\n",
    "\n",
    "get_content_bigrams_by_article_id(949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uci ml',\n",
       " 'ml repository',\n",
       " 'repository chronic',\n",
       " 'chronic kidney',\n",
       " 'kidney disease',\n",
       " 'disease data',\n",
       " 'data set']"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot Check\n",
    "\n",
    "get_title_bigrams_by_article_id(1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
