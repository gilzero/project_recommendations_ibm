{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/2322/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2465,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "2      1429.0         use deep learning for image classification   \n",
       "3      1338.0          ml optimization using cognitive assistant   \n",
       "4      1276.0          deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50% of individuals interact with ____ number of articles or fewer.\n",
    "\n",
    "median_val = df.groupby('email').size().sort_values(ascending=True).median()\n",
    "\n",
    "median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The maximum number of user-article interactions by any 1 user is ______. ?\n",
    "\n",
    "max_views_by_user = df.groupby('email').size().sort_values(ascending=False).head(1).values[0]\n",
    "\n",
    "max_views_by_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content['article_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>Community Detection at Scale</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>During the seven-week Insight Data Engineering...</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>* United States\\r\\n\\r\\nIBMÂ® * Site map\\r\\n\\r\\n...</td>\n",
       "      <td>When used to make sense of huge amounts of con...</td>\n",
       "      <td>How smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>One of the earliest documented catalogs was co...</td>\n",
       "      <td>How smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>Todayâs world of data science leverages data f...</td>\n",
       "      <td>Using Apache Spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>Todayâs world of data science leverages data f...</td>\n",
       "      <td>Using Apache Spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_body  \\\n",
       "50   Follow Sign in / Sign up Home About Insight Da...   \n",
       "365  Follow Sign in / Sign up Home About Insight Da...   \n",
       "221  * United States\\r\\n\\r\\nIBMÂ® * Site map\\r\\n\\r\\n...   \n",
       "692  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "232  Homepage Follow Sign in Get started Homepage *...   \n",
       "971  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "399  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "761  Homepage Follow Sign in Get started Homepage *...   \n",
       "578  This video shows you how to construct queries ...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "\n",
       "                                       doc_description  \\\n",
       "50                        Community Detection at Scale   \n",
       "365  During the seven-week Insight Data Engineering...   \n",
       "221  When used to make sense of huge amounts of con...   \n",
       "692  One of the earliest documented catalogs was co...   \n",
       "232  If you are like most data scientists, you are ...   \n",
       "971  If you are like most data scientists, you are ...   \n",
       "399  Todayâs world of data science leverages data f...   \n",
       "761  Todayâs world of data science leverages data f...   \n",
       "578  This video shows you how to construct queries ...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "\n",
       "                                         doc_full_name doc_status  article_id  \n",
       "50                        Graph-based machine learning       Live          50  \n",
       "365                       Graph-based machine learning       Live          50  \n",
       "221  How smart catalogs can turn the big data flood...       Live         221  \n",
       "692  How smart catalogs can turn the big data flood...       Live         221  \n",
       "232  Self-service data preparation with IBM Data Re...       Live         232  \n",
       "971  Self-service data preparation with IBM Data Re...       Live         232  \n",
       "399  Using Apache Spark as a parallel processing fr...       Live         398  \n",
       "761  Using Apache Spark as a parallel processing fr...       Live         398  \n",
       "578                              Use the Primary Index       Live         577  \n",
       "970                              Use the Primary Index       Live         577  "
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "\n",
    "dups = df_content.groupby('article_id').size().sort_values(ascending=False) # duplicates of article id\n",
    "dups_idx = dups[dups > 1].index # duplicates index\n",
    "\n",
    "df_content[df_content['article_id'].isin(dups_idx)].sort_values(by='article_id') # retrieve duplicated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "\n",
    "df_content = df_content.drop_duplicates('article_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_articles = df.article_id.nunique() # The number of unique articles that have at least one interaction\n",
    "unique_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_articles = df_content.article_id.nunique() # The number of unique articles on the IBM platform\n",
    "total_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5148"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users = df.email.nunique() # The number of unique users\n",
    "unique_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45993"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_article_interactions = df.shape[0] # The number of user-article interactions\n",
    "\n",
    "user_article_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_ds = df.groupby('article_id').size().sort_values(ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1429.0'"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most viewed article in the dataset as a string with one value following the decimal \n",
    "most_viewed_article_id = str(most_viewed_ds.index[0])\n",
    "most_viewed_article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most viewed article in the dataset was viewed how many times?\n",
    "\n",
    "max_views = most_viewed_ds.values[0]\n",
    "max_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1      1314.0       healthcare python streaming application demo        2\n",
       "2      1429.0         use deep learning for image classification        3\n",
       "3      1338.0          ml optimization using cognitive assistant        4\n",
       "4      1276.0          deploy your python model as a restful api        5"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you have everything right here! Nice job!\n"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val, \n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions, \n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views, \n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id, \n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles, \n",
    "    '`The number of unique users in the dataset is ______`': unique_users, \n",
    "    '`The number of unique articles on the IBM platform`': total_articles \n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids_ds = df.groupby('article_id').size().sort_values(ascending=False).head(n)\n",
    "    ids = article_ids_ds.index\n",
    "    \n",
    "    top_articles = []\n",
    "    for i in ids:\n",
    "        top_articles.append(df[df['article_id'] == i].head(1).title.values[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return top_articles # Return the top article titles from df (not df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles_ids - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids_ds = df.groupby('article_id').size().sort_values(ascending=False).head(n)\n",
    "    ids = article_ids_ds.index\n",
    "    top_articles_ids = list(ids)\n",
    " \n",
    "    return top_articles_ids # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use deep learning for image classification', 'insights from new york car accident reports', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'healthcare python streaming application demo', 'finding optimal locations of new store using decision optimization', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model']\n",
      "[1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0, 1162.0, 1304.0]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top_5 looks like the solution list! Nice job.\n",
      "Your top_10 looks like the solution list! Nice job.\n",
      "Your top_20 looks like the solution list! Nice job.\n"
     ]
    }
   ],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    df['title'] = 1\n",
    "    df = df[['user_id', 'article_id', 'title']]\n",
    "    \n",
    "    user_item = pd.pivot_table(df, index='user_id', columns='article_id', values='title', fill_value=0)\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>12.0</th>\n",
       "      <th>14.0</th>\n",
       "      <th>15.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>18.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1434.0</th>\n",
       "      <th>1435.0</th>\n",
       "      <th>1436.0</th>\n",
       "      <th>1437.0</th>\n",
       "      <th>1439.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1441.0</th>\n",
       "      <th>1442.0</th>\n",
       "      <th>1443.0</th>\n",
       "      <th>1444.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5149 rows Ã 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0.0     2.0     4.0     8.0     9.0     12.0    14.0    15.0    \\\n",
       "user_id                                                                      \n",
       "1                0       0       0       0       0       0       0       0   \n",
       "2                0       0       0       0       0       0       0       0   \n",
       "3                0       0       0       0       0       1       0       0   \n",
       "4                0       0       0       0       0       0       0       0   \n",
       "5                0       0       0       0       0       0       0       0   \n",
       "...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "5145             0       0       0       0       0       0       0       0   \n",
       "5146             0       0       0       0       0       0       0       0   \n",
       "5147             0       0       0       0       0       0       0       0   \n",
       "5148             0       0       0       0       0       0       0       0   \n",
       "5149             0       0       0       0       0       0       0       0   \n",
       "\n",
       "article_id  16.0    18.0    ...  1434.0  1435.0  1436.0  1437.0  1439.0  \\\n",
       "user_id                     ...                                           \n",
       "1                0       0  ...       0       0       1       0       1   \n",
       "2                0       0  ...       0       0       0       0       0   \n",
       "3                0       0  ...       0       0       1       0       0   \n",
       "4                0       0  ...       0       0       0       0       0   \n",
       "5                0       0  ...       0       0       0       0       0   \n",
       "...            ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "5145             0       0  ...       0       0       0       0       0   \n",
       "5146             0       0  ...       0       0       0       0       0   \n",
       "5147             0       0  ...       0       0       0       0       0   \n",
       "5148             0       0  ...       0       0       0       0       0   \n",
       "5149             1       0  ...       0       0       0       0       0   \n",
       "\n",
       "article_id  1440.0  1441.0  1442.0  1443.0  1444.0  \n",
       "user_id                                             \n",
       "1                0       0       0       0       0  \n",
       "2                0       0       0       0       0  \n",
       "3                0       0       0       0       0  \n",
       "4                0       0       0       0       0  \n",
       "5                0       0       0       0       0  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "5145             0       0       0       0       0  \n",
       "5146             0       0       0       0       0  \n",
       "5147             0       0       0       0       0  \n",
       "5148             0       0       0       0       0  \n",
       "5149             0       0       0       0       0  \n",
       "\n",
       "[5149 rows x 714 columns]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar). The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users.\n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.119027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119027</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id         1         2\n",
       "user_id                    \n",
       "1        1.000000  0.119027\n",
       "2        0.119027  1.000000"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment with pandas inbuilt corr\n",
    "\n",
    "user_item.loc[(1,2), :].transpose().corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar). The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. **Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users.**\n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    user_ids_all = user_item.index\n",
    "    \n",
    "    # a dictionary holds [user id : similarity] value pairs\n",
    "    dot_product_dict = {}\n",
    "\n",
    "    # compute similarity of each user to the provided user\n",
    "    for i in user_ids_all:\n",
    "        dot_product = np.dot(user_item.loc[user_id:user_id,:], user_item.loc[i:i,:].transpose())\n",
    "        dot_product_dict[i] = dot_product[0][0]\n",
    "        \n",
    "    # sort by highest (most similar), also drop against self    \n",
    "    ds = pd.Series(dot_product_dict.values(), index=dot_product_dict.keys()).drop(index=user_id).sort_values(ascending=False)\n",
    "    \n",
    "    # extract only index as list of ids. \n",
    "    most_similar_users = list(ds.index)\n",
    "       \n",
    "    return most_similar_users # return a list of the users in order from most to least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: [3933, 23, 3782, 203, 4459, 3870, 131, 46, 4201, 395]\n",
      "The 5 most similar users to user 3933 are: [1, 23, 3782, 4459, 203]\n",
      "The 3 most similar users to user 46 are: [4201, 23, 3782]\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['using pixiedust for fast, flexible, and easier data analysis and experimentation',\n",
       " 'healthcare python streaming application demo',\n",
       " 'deploy your python model as a restful api',\n",
       " 'bayesian nonparametric models â stats and bots']"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    article_names = []\n",
    "    \n",
    "    for id in article_ids:\n",
    "\n",
    "        name = df[df.article_id == id].title.head(1).values[0]\n",
    "\n",
    "        article_names.append(name)\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "a_ids = [1430.0, 1314.0, 1276.0, 233.0]\n",
    "get_article_names(a_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0],\n",
       " ['using deep learning to reconstruct high-resolution audio',\n",
       "  'build a python app on the streaming analytics service',\n",
       "  'gosales transactions for naive bayes model',\n",
       "  'healthcare python streaming application demo',\n",
       "  'use r dataframes & ibm watson natural language understanding',\n",
       "  'use xgboost, scikit-learn & ibm watson machine learning apis'])"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    article_ids = list(user_item.loc[user_id][user_item.loc[user_id] > 0].index)\n",
    "    article_names = get_article_names(article_ids)\n",
    "    \n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "get_user_articles(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # get all similar user ids for the targeted user\n",
    "    similar_uids = find_similar_users(user_id)\n",
    "\n",
    "\n",
    "    # get all article ids of the targeted user\n",
    "    article_ids_target_user = get_user_articles(user_id)[0]\n",
    "    # print(f\"[article_ids_target_user]:\\n {article_ids_target_user} \\n\")\n",
    "\n",
    "    \n",
    "    # a list contain unseen articles to recommend\n",
    "    recs = []\n",
    "\n",
    "    for uid in similar_uids:\n",
    "\n",
    "        # print(f\"\\n\\n\\n[number of recs]: {len(recs)}\\n\")\n",
    "        if len(recs) == m:\n",
    "            # print(f\"Number of recs reaches threadhold. Enough. Stop\")\n",
    "            break\n",
    "\n",
    "\n",
    "        #print(f\"[similar user id]: {uid}\")\n",
    "\n",
    "        # get this uid's article ids, and arbitrarily shuffle\n",
    "        article_ids_similar_user = get_user_articles(uid)[0]\n",
    "\n",
    "        # compute the differences of articles seen between the this user and targeted user\n",
    "        # subtraction's order matters\n",
    "        set_diff = list(set(article_ids_similar_user) - set(article_ids_target_user))\n",
    "        # print(f\"[set_diff]:\\n {set_diff} \\n\")\n",
    "        \n",
    "        # make a shuffle for arbitraily chocies from the diff set\n",
    "        random.shuffle(set_diff)\n",
    "\n",
    "        # add the differences of article ids to recs [], append only unique (no duplicate)\n",
    "        for i in set_diff:\n",
    "            if i not in recs and len(recs) < m:\n",
    "                recs.append(i)\n",
    "                # print(f\"[id] {i} appended\")\n",
    "\n",
    "    return recs # return your recommendations for this user_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data tidying in data science experience',\n",
       " 'this week in data science (april 18, 2017)',\n",
       " 'shaping data with ibm data refinery',\n",
       " 'data science platforms are on the rise and ibm is leading the way',\n",
       " 'timeseries data analysis of iot events by using jupyter notebook',\n",
       " 'got zip code data? prep it for analytics. â ibm watson data lab â medium',\n",
       " 'higher-order logistic regression for large datasets',\n",
       " 'using machine learning to predict parking difficulty',\n",
       " 'a tensorflow regression model to predict house values',\n",
       " 'deep forest: towards an alternative to deep neural networks']"
      ]
     },
     "execution_count": 1146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 1168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUT THE ANSWER WANT ARTICLE IDS TO BE STRING RATHER THAN NUMBER. However according to dataset, \n",
    "# they are all float. In the case, I just use its default astype for argument. \n",
    "\n",
    "df.article_id.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here\n",
    "assert set(get_article_names([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names([1320.0, 232.0, 844.0])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set([1320.0, 232.0, 844.0])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    # get all neighbors ids\n",
    "    nbh_ids = find_similar_users(user_id)\n",
    "    \n",
    "    \n",
    "    # assemble a data matrix\n",
    "    data_matrix = np.array([\n",
    "    [\n",
    "        x, # neighbor id\n",
    "        np.dot(user_item.loc[user_id:user_id,:], user_item.loc[x:x,:].transpose())[0][0], # similarity score\n",
    "        df[df.user_id == x].shape[0] # number of content interaction\n",
    "    ] for x in nbh_ids])\n",
    "    \n",
    "    # make a dataframe\n",
    "    neighbors_df = pd.DataFrame(data=data_matrix, \n",
    "                                columns=['neighbor_id', 'similarity', 'num_interactions'], \n",
    "                                index=data_matrix[:,0]).sort_values(by=['similarity', 'num_interactions'],\n",
    "                                                                    ascending=[False, False])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # get all similar user ids for the targeted user\n",
    "    # fetch with the 'neighbors_df'\n",
    "    similar_uids = list(get_top_sorted_users(user_id).index)\n",
    "    #print(f\"[similar_uids]: \\n{similar_uids}\")\n",
    "\n",
    "\n",
    "    # get all article ids of the targeted user\n",
    "    article_ids_target_user = get_user_articles(user_id)[0]\n",
    "    # print(f\"[article_ids_target_user]:\\n {article_ids_target_user} \\n\")\n",
    "\n",
    "    \n",
    "    # a list contain unseen articles to recommend\n",
    "    recs = []\n",
    "\n",
    "    for uid in similar_uids:\n",
    "\n",
    "        # print(f\"\\n\\n\\n[number of recs]: {len(recs)}\\n\")\n",
    "        if len(recs) == m:\n",
    "            #print(f\"Number of recs reaches threadhold {m}. Enough. Stop\")\n",
    "            break\n",
    "\n",
    "\n",
    "        #print(f\"[similar user id]: {uid}\")\n",
    "\n",
    "        # get this uid's article ids, and arbitrarily shuffle\n",
    "        article_ids_similar_user = get_user_articles(uid)[0]\n",
    "\n",
    "        # compute the differences of articles seen between the this user and targeted user\n",
    "        # subtraction's order matters\n",
    "        set_diff = list(set(article_ids_similar_user) - set(article_ids_target_user))\n",
    "        #print(f\"[set_diff before sort]:\\n {set_diff} \\n\")\n",
    "        \n",
    "        # Sort the set. Determine with highest total interactions metric \n",
    "        set_diff = list(df[df.article_id.isin(set_diff)]['article_id'].value_counts().index)\n",
    "        #print(f\"[set_diff after sort]:\\n {set_diff} \\n\")\n",
    "\n",
    "        # add the differences of article ids to recs [], append only unique (no duplicate)\n",
    "        for i in set_diff:\n",
    "            if i not in recs and len(recs) < m:\n",
    "                recs.append(i)\n",
    "                #print(f\"[id] {i} appended\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    rec_names = get_article_names(recs)\n",
    "    \n",
    "    return recs, rec_names\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "[1330.0, 1427.0, 1364.0, 1170.0, 1162.0, 1304.0, 1351.0, 1160.0, 1354.0, 1368.0]\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['insights from new york car accident reports', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model', 'model bike sharing data with spss', 'analyze accident reports on amazon emr spark', 'movie recommender system with spark machine learning', 'putting a human face on machine learning']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>num_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>3933</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      neighbor_id  similarity  num_interactions\n",
       "3933         3933          35                45"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_sorted_users(1).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>num_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>3870</td>\n",
       "      <td>74</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>3782</td>\n",
       "      <td>39</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>203</td>\n",
       "      <td>33</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>4459</td>\n",
       "      <td>33</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>29</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>3764</td>\n",
       "      <td>29</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>3697</td>\n",
       "      <td>29</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>242</td>\n",
       "      <td>25</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      neighbor_id  similarity  num_interactions\n",
       "3870         3870          74               144\n",
       "3782         3782          39               363\n",
       "23             23          38               364\n",
       "203           203          33               160\n",
       "4459         4459          33               158\n",
       "98             98          29               170\n",
       "3764         3764          29               169\n",
       "49             49          29               147\n",
       "3697         3697          29               145\n",
       "242           242          25               148"
      ]
     },
     "execution_count": 1316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_sorted_users(131).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = 3933 # Find the user that is most similar to user 1 \n",
    "user131_10th_sim = 242 # Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This all looks good!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. If we were given a new user, which of the above functions would you be able to use to make recommendations? Explain. Can you think of a better way we might make recommendations? Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: For new user, code start problem, we can use knowledge base approach, pulling most-interacted (viewed) content and/or trending content. Since the dataset has no timestamp attribute, we might just pull most-interacted content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below. You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1429.0,\n",
       " 1330.0,\n",
       " 1431.0,\n",
       " 1427.0,\n",
       " 1364.0,\n",
       " 1314.0,\n",
       " 1293.0,\n",
       " 1170.0,\n",
       " 1162.0,\n",
       " 1304.0]"
      ]
     },
     "execution_count": 1321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user = 0.0\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "\n",
    "\n",
    "new_user_recs = get_top_article_ids(10, df) # Your recommendations here\n",
    "\n",
    "new_user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "assert set(new_user_recs) == set([1314.0,1429.0,1293.0,1427.0,1162.0,1364.0,1304.0,1170.0,1431.0,1330.0]), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations (EXTRA - NOT REQUIRED)</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term.  You might consider content to be the **doc_body**, **doc_description**, or **doc_full_name**.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` Use the function body below to create a content based recommender.  Since there isn't one right answer for this recommendation tactic, no test functions are provided.  Feel free to change the function inputs if you decide you want to try a method that requires more input values.  The input values are currently set with one idea in mind that you may use to make content based recommendations.  One additional idea is that you might want to choose the most popular recommendations that meet your 'content criteria', but again, there is a lot of flexibility in how you might make these recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gilzero/project_disaster_responses/blob/main/train_classifier.py\n",
    "\n",
    "https://towardsdatascience.com/the-best-document-similarity-algorithm-in-2020-a-beginners-guide-a01b9ef8cf05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# constants and reusable objects\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2060,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â° * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Hereâs this weekâs news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Compose is all about immediacy. You want a new...</td>\n",
       "      <td>Using Compose's PostgreSQL data browser.</td>\n",
       "      <td>Browsing PostgreSQL Data with Compose</td>\n",
       "      <td>Live</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UPGRADING YOUR POSTGRESQL TO 9.5Share on Twitt...</td>\n",
       "      <td>Upgrading your PostgreSQL deployment to versio...</td>\n",
       "      <td>Upgrading your PostgreSQL to 9.5</td>\n",
       "      <td>Live</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Follow Sign in / Sign up 135 8 * Share\\r\\n * 1...</td>\n",
       "      <td>For a company like Slack that strives to be as...</td>\n",
       "      <td>Data Wrangling at Slack</td>\n",
       "      <td>Live</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>* Host\\r\\n * Competitions\\r\\n * Datasets\\r\\n *...</td>\n",
       "      <td>Kaggle is your home for data science. Learn ne...</td>\n",
       "      <td>Data Science Bowl 2017</td>\n",
       "      <td>Live</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>THE GRADIENT FLOW\\r\\nDATA / TECHNOLOGY / CULTU...</td>\n",
       "      <td>[A version of this post appears on the OâReill...</td>\n",
       "      <td>Using Apache Spark to predict attack vectors a...</td>\n",
       "      <td>Live</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OFFLINE-FIRST IOS APPS WITH SWIFT &amp; PART 1: TH...</td>\n",
       "      <td>Apple's sample app, Food Tracker, taught you i...</td>\n",
       "      <td>Offline-First iOS Apps with Swift &amp; Cloudant S...</td>\n",
       "      <td>Live</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Warehousing data from Cloudant to dashDB great...</td>\n",
       "      <td>Replicating data to a relational dashDB databa...</td>\n",
       "      <td>Warehousing GeoJSON documents</td>\n",
       "      <td>Live</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>This recipe showcases how one can analyze the ...</td>\n",
       "      <td>Timeseries Data Analysis of IoT events by usin...</td>\n",
       "      <td>Live</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Maureen McElaney Blocked Unblock Follow Follow...</td>\n",
       "      <td>Thereâs a reason youâve been hearing a lot abo...</td>\n",
       "      <td>Bridging the Gap Between Python and Scala Jupy...</td>\n",
       "      <td>Live</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Raj Singh Blocked Unblock Follow Following Dev...</td>\n",
       "      <td>Who are those people lurking behind the statis...</td>\n",
       "      <td>Got zip code data? Prep it for analytics. â IB...</td>\n",
       "      <td>Live</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Early methods to integrate machine learning us...</td>\n",
       "      <td>Apache Sparkâ¢ 2.0: Extend Structured Streaming...</td>\n",
       "      <td>Live</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>* Home\\r\\n * Research\\r\\n * Partnerships and C...</td>\n",
       "      <td>The performance of supervised predictive model...</td>\n",
       "      <td>Higher-order Logistic Regression for Large Dat...</td>\n",
       "      <td>Live</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Enterprise Pricing Articles Sign in Free 30-Da...</td>\n",
       "      <td>We've always considered MySQL as a potential C...</td>\n",
       "      <td>Compose for MySQL now for you</td>\n",
       "      <td>Live</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Homepage Follow Sign in / Sign up * Home\\r\\n *...</td>\n",
       "      <td>It has never been easier to build AI or machin...</td>\n",
       "      <td>The Greatest Public Datasets for AI â Startup ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>METRICS MAVEN: MODE D'EMPLOI - FINDING THE MOD...</td>\n",
       "      <td>In our Metrics Maven series, Compose's data sc...</td>\n",
       "      <td>Finding the Mode in PostgreSQL</td>\n",
       "      <td>Live</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>It is often useful to use RStudio for one piec...</td>\n",
       "      <td>Working interactively with RStudio and noteboo...</td>\n",
       "      <td>Live</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Raj Singh Blocked Unblock Follow Following Dev...</td>\n",
       "      <td>Youâre doing your data a disservice if you don...</td>\n",
       "      <td>Mapping for Data Science with PixieDust and Ma...</td>\n",
       "      <td>Live</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IMPORTING JSON DOCUMENTS WITH NOSQLIMPORT\\r\\nG...</td>\n",
       "      <td>Introducing nosqlimport, an npm module to help...</td>\n",
       "      <td>Move CSVs into different JSON doc stores</td>\n",
       "      <td>Live</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>This video shows you how to build and query a ...</td>\n",
       "      <td>This video shows you how to build and query a ...</td>\n",
       "      <td>Tutorial: How to build and query a Cloudant ge...</td>\n",
       "      <td>Live</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>THE CONVERSATIONAL INTERFACE IS THE NEW PARADI...</td>\n",
       "      <td>Botkit provides a simple framework to handle t...</td>\n",
       "      <td>The Conversational Interface is the New Paradigm</td>\n",
       "      <td>Live</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>Want to learn more about how we created the Da...</td>\n",
       "      <td>Creating the Data Science Experience</td>\n",
       "      <td>Live</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GOOGLE RESEARCH BLOG The latest news from Rese...</td>\n",
       "      <td>Much of driving is spent either stuck in traff...</td>\n",
       "      <td>Using Machine Learning to predict parking diff...</td>\n",
       "      <td>Live</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>This talk assumes you have a basic understandi...</td>\n",
       "      <td>Getting The Best Performance With PySpark</td>\n",
       "      <td>Live</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ACCESS DENIED\\r\\nSadly, your client does not s...</td>\n",
       "      <td>In this paper, we propose gcForest, a decision...</td>\n",
       "      <td>Deep Forest: Towards An Alternative to Deep Ne...</td>\n",
       "      <td>Live</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>Iâm very happy and proud to announce that IBM ...</td>\n",
       "      <td>Experience IoT with Coursera</td>\n",
       "      <td>Live</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KDNUGGETS\\r\\nData Mining, Analytics, Big Data,...</td>\n",
       "      <td>An open API is available on the internet for f...</td>\n",
       "      <td>How open API economy accelerates the growth of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video shows you how to sign up for a free...</td>\n",
       "      <td>Sign up for a free trial in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>Stacking is a model ensembling technique used ...</td>\n",
       "      <td>A Kaggler's Guide to Model Stacking in Practice</td>\n",
       "      <td>Live</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Working Vis * \\r\\n * \\r\\n\\r\\n * Home\\r\\n * Abo...</td>\n",
       "      <td>Analytics and visualization often go hand-in-h...</td>\n",
       "      <td>Using Brunel in IPython/Jupyter Notebooks</td>\n",
       "      <td>Live</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Steve Moore ...</td>\n",
       "      <td>Machine learning has already extended into so ...</td>\n",
       "      <td>Top 10 Machine Learning Use Cases: Part 1</td>\n",
       "      <td>Live</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nick Kasten Blocked Unblock Follow Following C...</td>\n",
       "      <td>In this article, Iâll describe an app I built ...</td>\n",
       "      <td>Gaze Into My Reddit Crystal Ball â IBM Watson ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...</td>\n",
       "      <td>Hereâs a quick and handy guide to creating dat...</td>\n",
       "      <td>Data visualization playbook: The right level o...</td>\n",
       "      <td>Live</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Homepage IBM Watson Data Lab Follow Sign in / ...</td>\n",
       "      <td>When you customise your Cloudant domain with C...</td>\n",
       "      <td>Create a Custom Domain for Cloudant Using Clou...</td>\n",
       "      <td>Live</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The primary index is the fastest way to retrie...</td>\n",
       "      <td>A guide to using Cloudant's _all_docs endpoint...</td>\n",
       "      <td>For Developers: Querying the Cloudant Primary ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>* R Views\\r\\n * About this Blog\\r\\n * Contribu...</td>\n",
       "      <td>Our app will be simple in that it displays pri...</td>\n",
       "      <td>Pulling and Displaying ETF Data</td>\n",
       "      <td>Live</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Stats and Bots Follow Sign in / Sign up * Home...</td>\n",
       "      <td>Ensemble learning helps improve machine learni...</td>\n",
       "      <td>Ensemble Learning to Improve Machine Learning ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TL;DR: It's easy to customise the Mongo shell'...</td>\n",
       "      <td>It's easy to customize the Mongo shell's promp...</td>\n",
       "      <td>Customizing MongoDBâs Shell with Compact Prompts</td>\n",
       "      <td>Live</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>GETTING STARTED WITH COMPOSE'S SCYLLADB\\r\\nSha...</td>\n",
       "      <td>Getting started with ScyllaDB is easy since it...</td>\n",
       "      <td>Getting Started with Compose's ScyllaDB</td>\n",
       "      <td>Live</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>â° * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>This free Deep Learning with TensorFlow course...</td>\n",
       "      <td>Deep Learning With Tensorflow Course by Big Da...</td>\n",
       "      <td>Live</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Learn how to use IBM Bluemix and the Simple Da...</td>\n",
       "      <td>Uncover Product Insights Hidden in Stack Overflow</td>\n",
       "      <td>Live</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Build a custom library for ApacheÂ® Sparkâ¢ and ...</td>\n",
       "      <td>Build a custom library for ApacheÂ® Sparkâ¢ and ...</td>\n",
       "      <td>Start Developing with Spark and Notebooks</td>\n",
       "      <td>Live</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>REAL-TIME Q&amp;A APP WITH RETHINKDB\\r\\nMatt Colli...</td>\n",
       "      <td>RethinkDB's push updates makes it great for re...</td>\n",
       "      <td>Q&amp;A Voting App with RethinkDB</td>\n",
       "      <td>Live</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Watch how to download and install Database Con...</td>\n",
       "      <td>Install IBM Database Conversion Workbench</td>\n",
       "      <td>Live</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Data Science Experience Datasci X * Data Scien...</td>\n",
       "      <td>Learn to use IBM Data Science Experience.</td>\n",
       "      <td>Data Science Experience Documentation</td>\n",
       "      <td>Live</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Compose The Compose logo Articles Sign in Free...</td>\n",
       "      <td>We'll also look at using PostGIS to filter our...</td>\n",
       "      <td>GeoFile: Using OpenStreetMap Data in Compose P...</td>\n",
       "      <td>Live</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>Community Detection at Scale</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>* Free 7-Day Crash Course\\r\\n * Blog\\r\\n * Mas...</td>\n",
       "      <td>Get to know the ML landscape through this prac...</td>\n",
       "      <td>Modern Machine Learning Algorithms</td>\n",
       "      <td>Live</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>* United States\\r\\n\\r\\nIBMÂ® * Site map\\r\\n\\r\\n...</td>\n",
       "      <td>Watch how to build a storefront web app with I...</td>\n",
       "      <td>Build an app using IBM Graph</td>\n",
       "      <td>Live</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>Starting today, users will be able to access S...</td>\n",
       "      <td>Introducing Streams Designer</td>\n",
       "      <td>Live</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...</td>\n",
       "      <td>Discover eight ways that Apache Sparkâs machin...</td>\n",
       "      <td>8 ways to turn data into value with Apache Spa...</td>\n",
       "      <td>Live</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>PREDICT FLIGHT DELAYS WITH APACHE SPARK MLLIB,...</td>\n",
       "      <td>Build a Machine Learning model with Apache Spa...</td>\n",
       "      <td>Predict Flight Delays with Apache Spark MLLib,...</td>\n",
       "      <td>Live</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>INTRODUCING THE SIMPLE AUTOCOMPLETE SERVICE\\r\\...</td>\n",
       "      <td>Easily add autocomplete to your web form field...</td>\n",
       "      <td>Introducing the Simple Autocomplete Service</td>\n",
       "      <td>Live</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>WILL WOLF\\r\\nDATA SCIENCE THINGS AND THOUGHTS ...</td>\n",
       "      <td>In this work, we explore improving a vanilla r...</td>\n",
       "      <td>Transfer Learning for Flight Delay Prediction ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>01. Holden Karau, IBM, Visits #theCUBE!. (00:2...</td>\n",
       "      <td>Advancements in the Spark Community</td>\n",
       "      <td>Live</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Homepage PUBLISHED IN AUTONOMOUS AGENTS â #AI ...</td>\n",
       "      <td>Letâs say you have the gift of flight (or you ...</td>\n",
       "      <td>How to tame the valley â Hessian-free hacks fo...</td>\n",
       "      <td>Live</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             doc_body  \\\n",
       "0   Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1   No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2   â° * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3   DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4   Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "5   Compose is all about immediacy. You want a new...   \n",
       "6   UPGRADING YOUR POSTGRESQL TO 9.5Share on Twitt...   \n",
       "7   Follow Sign in / Sign up 135 8 * Share\\r\\n * 1...   \n",
       "8   * Host\\r\\n * Competitions\\r\\n * Datasets\\r\\n *...   \n",
       "9   THE GRADIENT FLOW\\r\\nDATA / TECHNOLOGY / CULTU...   \n",
       "10  OFFLINE-FIRST IOS APPS WITH SWIFT & PART 1: TH...   \n",
       "11  Warehousing data from Cloudant to dashDB great...   \n",
       "12  Skip to main content IBM developerWorks / Deve...   \n",
       "13  Maureen McElaney Blocked Unblock Follow Follow...   \n",
       "14  Raj Singh Blocked Unblock Follow Following Dev...   \n",
       "15  * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "16  * Home\\r\\n * Research\\r\\n * Partnerships and C...   \n",
       "17  Enterprise Pricing Articles Sign in Free 30-Da...   \n",
       "18  Homepage Follow Sign in / Sign up * Home\\r\\n *...   \n",
       "19  METRICS MAVEN: MODE D'EMPLOI - FINDING THE MOD...   \n",
       "20  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "21  Raj Singh Blocked Unblock Follow Following Dev...   \n",
       "22  IMPORTING JSON DOCUMENTS WITH NOSQLIMPORT\\r\\nG...   \n",
       "23  This video shows you how to build and query a ...   \n",
       "24  THE CONVERSATIONAL INTERFACE IS THE NEW PARADI...   \n",
       "25  Skip navigation Upload Sign in SearchLoading.....   \n",
       "26  GOOGLE RESEARCH BLOG The latest news from Rese...   \n",
       "27  Skip navigation Upload Sign in SearchLoading.....   \n",
       "28  ACCESS DENIED\\r\\nSadly, your client does not s...   \n",
       "29  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "30  KDNUGGETS\\r\\nData Mining, Analytics, Big Data,...   \n",
       "31  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "32  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "33  Working Vis * \\r\\n * \\r\\n\\r\\n * Home\\r\\n * Abo...   \n",
       "34  Homepage Follow Sign in / Sign up Steve Moore ...   \n",
       "35  Nick Kasten Blocked Unblock Follow Following C...   \n",
       "36  Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...   \n",
       "37  Homepage IBM Watson Data Lab Follow Sign in / ...   \n",
       "38  The primary index is the fastest way to retrie...   \n",
       "39  * R Views\\r\\n * About this Blog\\r\\n * Contribu...   \n",
       "40  Stats and Bots Follow Sign in / Sign up * Home...   \n",
       "41  TL;DR: It's easy to customise the Mongo shell'...   \n",
       "42  GETTING STARTED WITH COMPOSE'S SCYLLADB\\r\\nSha...   \n",
       "43  â° * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "44  Skip to main content IBM developerWorks / Deve...   \n",
       "45  Build a custom library for ApacheÂ® Sparkâ¢ and ...   \n",
       "46  REAL-TIME Q&A APP WITH RETHINKDB\\r\\nMatt Colli...   \n",
       "47  Skip to main content IBM developerWorks / Deve...   \n",
       "48  Data Science Experience Datasci X * Data Scien...   \n",
       "49  Compose The Compose logo Articles Sign in Free...   \n",
       "50  Follow Sign in / Sign up Home About Insight Da...   \n",
       "51  * Free 7-Day Crash Course\\r\\n * Blog\\r\\n * Mas...   \n",
       "52  * United States\\r\\n\\r\\nIBMÂ® * Site map\\r\\n\\r\\n...   \n",
       "53  Homepage Follow Sign in Get started Homepage *...   \n",
       "54  Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...   \n",
       "55  PREDICT FLIGHT DELAYS WITH APACHE SPARK MLLIB,...   \n",
       "56  INTRODUCING THE SIMPLE AUTOCOMPLETE SERVICE\\r\\...   \n",
       "57  WILL WOLF\\r\\nDATA SCIENCE THINGS AND THOUGHTS ...   \n",
       "58  Skip navigation Upload Sign in SearchLoading.....   \n",
       "59  Homepage PUBLISHED IN AUTONOMOUS AGENTS â #AI ...   \n",
       "\n",
       "                                      doc_description  \\\n",
       "0   Detect bad readings in real time using Python ...   \n",
       "1   See the forest, see the trees. Here lies the c...   \n",
       "2   Hereâs this weekâs news in Data Science and Bi...   \n",
       "3   Learn how distributed DBs solve the problem of...   \n",
       "4   This video demonstrates the power of IBM DataS...   \n",
       "5            Using Compose's PostgreSQL data browser.   \n",
       "6   Upgrading your PostgreSQL deployment to versio...   \n",
       "7   For a company like Slack that strives to be as...   \n",
       "8   Kaggle is your home for data science. Learn ne...   \n",
       "9   [A version of this post appears on the OâReill...   \n",
       "10  Apple's sample app, Food Tracker, taught you i...   \n",
       "11  Replicating data to a relational dashDB databa...   \n",
       "12  This recipe showcases how one can analyze the ...   \n",
       "13  Thereâs a reason youâve been hearing a lot abo...   \n",
       "14  Who are those people lurking behind the statis...   \n",
       "15  Early methods to integrate machine learning us...   \n",
       "16  The performance of supervised predictive model...   \n",
       "17  We've always considered MySQL as a potential C...   \n",
       "18  It has never been easier to build AI or machin...   \n",
       "19  In our Metrics Maven series, Compose's data sc...   \n",
       "20  It is often useful to use RStudio for one piec...   \n",
       "21  Youâre doing your data a disservice if you don...   \n",
       "22  Introducing nosqlimport, an npm module to help...   \n",
       "23  This video shows you how to build and query a ...   \n",
       "24  Botkit provides a simple framework to handle t...   \n",
       "25  Want to learn more about how we created the Da...   \n",
       "26  Much of driving is spent either stuck in traff...   \n",
       "27  This talk assumes you have a basic understandi...   \n",
       "28  In this paper, we propose gcForest, a decision...   \n",
       "29  Iâm very happy and proud to announce that IBM ...   \n",
       "30  An open API is available on the internet for f...   \n",
       "31  This video shows you how to sign up for a free...   \n",
       "32  Stacking is a model ensembling technique used ...   \n",
       "33  Analytics and visualization often go hand-in-h...   \n",
       "34  Machine learning has already extended into so ...   \n",
       "35  In this article, Iâll describe an app I built ...   \n",
       "36  Hereâs a quick and handy guide to creating dat...   \n",
       "37  When you customise your Cloudant domain with C...   \n",
       "38  A guide to using Cloudant's _all_docs endpoint...   \n",
       "39  Our app will be simple in that it displays pri...   \n",
       "40  Ensemble learning helps improve machine learni...   \n",
       "41  It's easy to customize the Mongo shell's promp...   \n",
       "42  Getting started with ScyllaDB is easy since it...   \n",
       "43  This free Deep Learning with TensorFlow course...   \n",
       "44  Learn how to use IBM Bluemix and the Simple Da...   \n",
       "45  Build a custom library for ApacheÂ® Sparkâ¢ and ...   \n",
       "46  RethinkDB's push updates makes it great for re...   \n",
       "47  Watch how to download and install Database Con...   \n",
       "48          Learn to use IBM Data Science Experience.   \n",
       "49  We'll also look at using PostGIS to filter our...   \n",
       "50                       Community Detection at Scale   \n",
       "51  Get to know the ML landscape through this prac...   \n",
       "52  Watch how to build a storefront web app with I...   \n",
       "53  Starting today, users will be able to access S...   \n",
       "54  Discover eight ways that Apache Sparkâs machin...   \n",
       "55  Build a Machine Learning model with Apache Spa...   \n",
       "56  Easily add autocomplete to your web form field...   \n",
       "57  In this work, we explore improving a vanilla r...   \n",
       "58  01. Holden Karau, IBM, Visits #theCUBE!. (00:2...   \n",
       "59  Letâs say you have the gift of flight (or you ...   \n",
       "\n",
       "                                        doc_full_name doc_status  article_id  \n",
       "0   Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1   Communicating data science: A guide to present...       Live           1  \n",
       "2          This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3   DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4       Analyze NY Restaurant data using Spark in DSX       Live           4  \n",
       "5               Browsing PostgreSQL Data with Compose       Live           5  \n",
       "6                    Upgrading your PostgreSQL to 9.5       Live           6  \n",
       "7                             Data Wrangling at Slack       Live           7  \n",
       "8                              Data Science Bowl 2017       Live           8  \n",
       "9   Using Apache Spark to predict attack vectors a...       Live           9  \n",
       "10  Offline-First iOS Apps with Swift & Cloudant S...       Live          10  \n",
       "11                      Warehousing GeoJSON documents       Live          11  \n",
       "12  Timeseries Data Analysis of IoT events by usin...       Live          12  \n",
       "13  Bridging the Gap Between Python and Scala Jupy...       Live          13  \n",
       "14  Got zip code data? Prep it for analytics. â IB...       Live          14  \n",
       "15  Apache Sparkâ¢ 2.0: Extend Structured Streaming...       Live          15  \n",
       "16  Higher-order Logistic Regression for Large Dat...       Live          16  \n",
       "17                      Compose for MySQL now for you       Live          17  \n",
       "18  The Greatest Public Datasets for AI â Startup ...       Live          18  \n",
       "19                     Finding the Mode in PostgreSQL       Live          19  \n",
       "20  Working interactively with RStudio and noteboo...       Live          20  \n",
       "21  Mapping for Data Science with PixieDust and Ma...       Live          21  \n",
       "22           Move CSVs into different JSON doc stores       Live          22  \n",
       "23  Tutorial: How to build and query a Cloudant ge...       Live          23  \n",
       "24   The Conversational Interface is the New Paradigm       Live          24  \n",
       "25               Creating the Data Science Experience       Live          25  \n",
       "26  Using Machine Learning to predict parking diff...       Live          26  \n",
       "27          Getting The Best Performance With PySpark       Live          27  \n",
       "28  Deep Forest: Towards An Alternative to Deep Ne...       Live          28  \n",
       "29                       Experience IoT with Coursera       Live          29  \n",
       "30  How open API economy accelerates the growth of...       Live          30  \n",
       "31                    Sign up for a free trial in DSX       Live          31  \n",
       "32    A Kaggler's Guide to Model Stacking in Practice       Live          32  \n",
       "33          Using Brunel in IPython/Jupyter Notebooks       Live          33  \n",
       "34          Top 10 Machine Learning Use Cases: Part 1       Live          34  \n",
       "35  Gaze Into My Reddit Crystal Ball â IBM Watson ...       Live          35  \n",
       "36  Data visualization playbook: The right level o...       Live          36  \n",
       "37  Create a Custom Domain for Cloudant Using Clou...       Live          37  \n",
       "38  For Developers: Querying the Cloudant Primary ...       Live          38  \n",
       "39                    Pulling and Displaying ETF Data       Live          39  \n",
       "40  Ensemble Learning to Improve Machine Learning ...       Live          40  \n",
       "41   Customizing MongoDBâs Shell with Compact Prompts       Live          41  \n",
       "42            Getting Started with Compose's ScyllaDB       Live          42  \n",
       "43  Deep Learning With Tensorflow Course by Big Da...       Live          43  \n",
       "44  Uncover Product Insights Hidden in Stack Overflow       Live          44  \n",
       "45          Start Developing with Spark and Notebooks       Live          45  \n",
       "46                      Q&A Voting App with RethinkDB       Live          46  \n",
       "47          Install IBM Database Conversion Workbench       Live          47  \n",
       "48              Data Science Experience Documentation       Live          48  \n",
       "49  GeoFile: Using OpenStreetMap Data in Compose P...       Live          49  \n",
       "50                       Graph-based machine learning       Live          50  \n",
       "51                 Modern Machine Learning Algorithms       Live          51  \n",
       "52                       Build an app using IBM Graph       Live          52  \n",
       "53                       Introducing Streams Designer       Live          53  \n",
       "54  8 ways to turn data into value with Apache Spa...       Live          54  \n",
       "55  Predict Flight Delays with Apache Spark MLLib,...       Live          55  \n",
       "56        Introducing the Simple Autocomplete Service       Live          56  \n",
       "57  Transfer Learning for Flight Delay Prediction ...       Live          57  \n",
       "58                Advancements in the Spark Community       Live          58  \n",
       "59  How to tame the valley â Hessian-free hacks fo...       Live          59  "
      ]
     },
     "execution_count": 2060,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp = df_content.copy()\n",
    "df_nlp.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    private tokenizer to transform each text.\n",
    "    As a NLP helper function including following tasks:\n",
    "    - Replace URLs\n",
    "    - Normalize text\n",
    "    - Remove punctuation\n",
    "    - Tokenize words\n",
    "    - Remove stop words\n",
    "    - Legmmatize words\n",
    "    :param text: A message text.\n",
    "    :return: cleaned tokens extracted from original message text.\n",
    "    '''\n",
    "\n",
    "    # print(f\"original text: \\n {text}\")\n",
    "\n",
    "    # replace urls\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word).strip() for word in tokens if word not in stop_words]\n",
    "\n",
    "    # in case after normalize/lemmatize, if there is no words, make a dummy element. otherwise StartingVerb breaks\n",
    "    if len(tokens) < 1:\n",
    "        tokens = ['none']\n",
    "\n",
    "    # print(f\"tokens: \\n {tokens} \\n\\n\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skip navigation sign searchloading close yeah keep undo closethis video unavailable watch queue queue watch queue queue remove disconnect next video starting stop 1 loading watch queue queue count total find closedemo detect malfunctioning iot sensor streaming analytics ibm analyticsloading unsubscribe ibm analytics cancel unsubscribeworking subscribe subscribed unsubscribe 26kloading loading working add towant watch later sign add video playlist sign share reportneed report video sign report inappropriate content sign transcript statistic add translation 175 view 6like video sign make opinion count sign 7 0don like video sign make opinion count sign 1loading loading transcript interactive transcript could loaded loading loading rating available video rented feature available right please try later published nov 6 2017this video demonstrates streaming analytics application written python running ibm data science experience result analysis displayed map using plotly notebook demonstrated video available try urlplaceholder visit streamsdev article tip stream urlplaceholder python api developer guide urlplaceholder streaming analytics python course urlplaceholder category science technology license standard youtube license show show lessloading autoplay autoplay enabled suggested video automatically play next next python ecosystem data science guided tour christian staudt duration 25 41 pydata 1 411 view 25 41 ibm streaming analytics python duration 1 00 51 john neill 105 view 1 00 51 customer using ibm data science experience expected case expected one duration 18 29 databricks 327 view 18 29 giovanni lanzani applied data science duration 35 14 pydata 2 728 view 35 14 detecting fraud real time azure stream analytics duration 32 16 philip howard 71 view 32 16 step step guide build real time anomaly detection system using apache spark streaming duration 16 11 mariusz jacyno 4 591 view 16 11 real time analytics azure stream analytics duration 54 47 pas business analytics virtual group 940 view 54 47 real time machine learning analytics using structured streaming kinesis firehose duration 31 25 databricks 660 view 31 25 data science duration 25 05 manish telang 3 view 25 05 real time log analytics using amazon kinesis amazon elasticsearch service duration 28 32 amazon web service webinar channel 1 072 view 28 32 ibm data science experience machine learning use case healthcare duration 26 53 idea 157 view 26 53 streaming analytics comparison open source framework product cloud service duration 47 06 kai w hner 1 761 view 47 06 overview ibm streaming analytics bluemix duration 44 12 ibm analytics 1 311 view 44 12 predicting stock price learn python data science 4 duration 7 39 siraj raval 274 452 view 7 39 rest api concept example duration 8 53 webconcepts 1 687 034 view 8 53 streaming data analytics apache spark streaming duration 1 01 19 data guru 300 view 1 01 19 orchestrate ibm data science experience analytics workflow using node red duration 10 16 balaji kadambi 109 view 10 16 delight client data science ibm integrated analytics system duration 15 05 ibm analytics 1 581 view 15 05 devops simple english duration 7 07 rackspace 657 396 view 7 07 introduction learn python data science 1 duration 6 55 siraj raval 206 552 view 6 55 loading suggestion show language english location united state restricted mode history helploading loading loading press copyright creator advertise developer youtube term privacy policy safety send feedback test new feature loading working sign add watch lateradd loading playlist'"
      ]
     },
     "execution_count": 1396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tokenized result. \n",
    "\n",
    "df_nlp.iloc[0].doc_body\n",
    "\n",
    "token_doc = tokenize(df_nlp.iloc[0].doc_body)\n",
    "\n",
    "token_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2061,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found index and article id mismatched. Update index with article to make it consistant\n",
    "\n",
    "df_nlp.index = df_nlp.article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2062,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1046    False\n",
       "1047    False\n",
       "1048    False\n",
       "1049     True\n",
       "1050    False\n",
       "Name: doc_body, Length: 1051, dtype: bool"
      ]
     },
     "execution_count": 2062,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Data. For empty content in body\n",
    "# There are some rows' doc_body value is NaN\n",
    "df_nlp.doc_body.isnull()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2063,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the empty body with 'empty' placeholder\n",
    "df_nlp.loc[df_nlp[df_nlp.doc_body.isnull()].index, 'doc_body'] = 'empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2064,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the empty desc with 'empty' placeholder\n",
    "# There are some rows' doc_description value is NaN\n",
    "\n",
    "df_nlp.loc[df_nlp[df_nlp.doc_description.isnull()].index, 'doc_description'] = 'empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_article_body_similarity(article_id_1, article_id_2):\n",
    "    # Compute the consine similarity based on tfidf of body content\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(df_nlp.loc[article_id_1].doc_body))\n",
    "    doc_b = ' '.join(tokenize(df_nlp.loc[article_id_2].doc_body))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_article_title_similarity(article_id_1, article_id_2):\n",
    "    # Compute the consine similarity based on tfidf of title (doc_full_name)\n",
    "    # think of title tag for seo pagerank\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(df_nlp.loc[article_id_1].doc_full_name))\n",
    "    doc_b = ' '.join(tokenize(df_nlp.loc[article_id_2].doc_full_name))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_article_desc_similarity(article_id_1, article_id_2):\n",
    "    # Compute the consine similarity based on tfidf of desc content (doc_description)\n",
    "    # think of desc tag for seo pagerank\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(df_nlp.loc[article_id_1].doc_description))\n",
    "    doc_b = ' '.join(tokenize(df_nlp.loc[article_id_2].doc_description))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_article_similarity(article_id_1, article_id_2):\n",
    "    # calculate similary for body, title, desc, then combine a single value.\n",
    "    # think about how google weight. Title tag is very heavy. SEO-wised\n",
    "    # so having 3 consine similarity values, then do a normailized one. \n",
    "    # what's the formular? think of a course, assignments weight x, final exam weight y,\n",
    "    # then what's total grade.\n",
    "    # https://www.indeed.com/career-advice/career-development/how-to-calculate-weighted-average\n",
    "    \n",
    "    similarity_title = _compute_article_title_similarity(article_id_1,article_id_2)\n",
    "    similarity_body = _compute_article_body_similarity(article_id_1,article_id_2)\n",
    "    similarity_desc = _compute_article_desc_similarity(article_id_1,article_id_2)\n",
    "    \n",
    "    # a weighted sum caluculation for final score\n",
    "    \n",
    "    overall = similarity_title * 0.5 + similarity_body * 0.4 + similarity_desc * 0.1\n",
    "    \n",
    "    return overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07122043497708203"
      ]
     },
     "execution_count": 1576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_article_similarity(55,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1794,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar articles for a given article\n",
    "def find_similar_articles(article_id, data=df_nlp):\n",
    "    \n",
    "    article_ids_all = data.index\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for i in article_ids_all:\n",
    "        # print(f\"\\n[i]: {i}\")\n",
    "        \n",
    "        if i == article_id:\n",
    "            continue\n",
    "\n",
    "        similarity_score = compute_article_similarity(article_id,i)\n",
    "        similarity_dict[i] = similarity_score\n",
    "        # print(f\"[similarity_score]: {similarity_score}\")\n",
    "    \n",
    "    \n",
    "    similarity_ds = pd.Series(data=similarity_dict.values(), \n",
    "                           index=similarity_dict.keys()).sort_values(ascending=False).index\n",
    "    \n",
    "    return list(similarity_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1795,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[389,\n",
       " 993,\n",
       " 949,\n",
       " 592,\n",
       " 714,\n",
       " 117,\n",
       " 678,\n",
       " 942,\n",
       " 231,\n",
       " 925,\n",
       " 15,\n",
       " 463,\n",
       " 353,\n",
       " 835,\n",
       " 907,\n",
       " 284,\n",
       " 977,\n",
       " 600,\n",
       " 595,\n",
       " 997,\n",
       " 239,\n",
       " 240,\n",
       " 411,\n",
       " 45,\n",
       " 119,\n",
       " 473,\n",
       " 375,\n",
       " 934,\n",
       " 560,\n",
       " 264,\n",
       " 893,\n",
       " 515,\n",
       " 54,\n",
       " 404,\n",
       " 849,\n",
       " 815,\n",
       " 380,\n",
       " 486,\n",
       " 632,\n",
       " 674,\n",
       " 424,\n",
       " 443,\n",
       " 55,\n",
       " 9,\n",
       " 647,\n",
       " 398,\n",
       " 161,\n",
       " 707,\n",
       " 193,\n",
       " 948,\n",
       " 844,\n",
       " 721,\n",
       " 681,\n",
       " 223,\n",
       " 58,\n",
       " 961,\n",
       " 403,\n",
       " 77,\n",
       " 874,\n",
       " 1005,\n",
       " 762,\n",
       " 96,\n",
       " 143,\n",
       " 112,\n",
       " 1049,\n",
       " 938,\n",
       " 121,\n",
       " 249,\n",
       " 235,\n",
       " 4,\n",
       " 20,\n",
       " 341,\n",
       " 689,\n",
       " 842,\n",
       " 809,\n",
       " 489,\n",
       " 744,\n",
       " 141,\n",
       " 1042,\n",
       " 872,\n",
       " 941,\n",
       " 400,\n",
       " 594,\n",
       " 624,\n",
       " 302,\n",
       " 318,\n",
       " 342,\n",
       " 468,\n",
       " 272,\n",
       " 760,\n",
       " 706,\n",
       " 280,\n",
       " 643,\n",
       " 562,\n",
       " 47,\n",
       " 27,\n",
       " 956,\n",
       " 499,\n",
       " 469,\n",
       " 385,\n",
       " 7,\n",
       " 84,\n",
       " 110,\n",
       " 326,\n",
       " 241,\n",
       " 801,\n",
       " 728,\n",
       " 228,\n",
       " 378,\n",
       " 683,\n",
       " 198,\n",
       " 362,\n",
       " 138,\n",
       " 147,\n",
       " 369,\n",
       " 593,\n",
       " 154,\n",
       " 569,\n",
       " 847,\n",
       " 104,\n",
       " 772,\n",
       " 447,\n",
       " 787,\n",
       " 1028,\n",
       " 226,\n",
       " 176,\n",
       " 578,\n",
       " 936,\n",
       " 958,\n",
       " 13,\n",
       " 65,\n",
       " 48,\n",
       " 334,\n",
       " 267,\n",
       " 432,\n",
       " 634,\n",
       " 146,\n",
       " 776,\n",
       " 293,\n",
       " 828,\n",
       " 313,\n",
       " 44,\n",
       " 720,\n",
       " 717,\n",
       " 846,\n",
       " 97,\n",
       " 523,\n",
       " 120,\n",
       " 869,\n",
       " 575,\n",
       " 691,\n",
       " 800,\n",
       " 269,\n",
       " 1021,\n",
       " 430,\n",
       " 262,\n",
       " 351,\n",
       " 11,\n",
       " 12,\n",
       " 651,\n",
       " 864,\n",
       " 62,\n",
       " 644,\n",
       " 703,\n",
       " 729,\n",
       " 136,\n",
       " 230,\n",
       " 773,\n",
       " 856,\n",
       " 935,\n",
       " 73,\n",
       " 444,\n",
       " 1001,\n",
       " 87,\n",
       " 974,\n",
       " 865,\n",
       " 736,\n",
       " 416,\n",
       " 510,\n",
       " 245,\n",
       " 101,\n",
       " 325,\n",
       " 476,\n",
       " 5,\n",
       " 491,\n",
       " 1017,\n",
       " 829,\n",
       " 672,\n",
       " 232,\n",
       " 304,\n",
       " 1013,\n",
       " 698,\n",
       " 807,\n",
       " 374,\n",
       " 126,\n",
       " 306,\n",
       " 474,\n",
       " 319,\n",
       " 631,\n",
       " 1034,\n",
       " 183,\n",
       " 440,\n",
       " 780,\n",
       " 682,\n",
       " 461,\n",
       " 395,\n",
       " 710,\n",
       " 210,\n",
       " 962,\n",
       " 410,\n",
       " 557,\n",
       " 861,\n",
       " 78,\n",
       " 965,\n",
       " 836,\n",
       " 1035,\n",
       " 793,\n",
       " 712,\n",
       " 687,\n",
       " 95,\n",
       " 103,\n",
       " 108,\n",
       " 36,\n",
       " 251,\n",
       " 529,\n",
       " 75,\n",
       " 363,\n",
       " 502,\n",
       " 990,\n",
       " 966,\n",
       " 409,\n",
       " 732,\n",
       " 617,\n",
       " 584,\n",
       " 820,\n",
       " 448,\n",
       " 739,\n",
       " 427,\n",
       " 859,\n",
       " 564,\n",
       " 172,\n",
       " 191,\n",
       " 100,\n",
       " 621,\n",
       " 213,\n",
       " 653,\n",
       " 428,\n",
       " 663,\n",
       " 76,\n",
       " 711,\n",
       " 725,\n",
       " 194,\n",
       " 827,\n",
       " 604,\n",
       " 216,\n",
       " 758,\n",
       " 441,\n",
       " 540,\n",
       " 953,\n",
       " 192,\n",
       " 888,\n",
       " 417,\n",
       " 521,\n",
       " 524,\n",
       " 122,\n",
       " 1022,\n",
       " 144,\n",
       " 583,\n",
       " 1025,\n",
       " 29,\n",
       " 850,\n",
       " 753,\n",
       " 517,\n",
       " 202,\n",
       " 640,\n",
       " 766,\n",
       " 892,\n",
       " 364,\n",
       " 1039,\n",
       " 999,\n",
       " 217,\n",
       " 808,\n",
       " 559,\n",
       " 708,\n",
       " 52,\n",
       " 464,\n",
       " 344,\n",
       " 479,\n",
       " 764,\n",
       " 733,\n",
       " 665,\n",
       " 543,\n",
       " 134,\n",
       " 743,\n",
       " 685,\n",
       " 25,\n",
       " 1000,\n",
       " 556,\n",
       " 960,\n",
       " 348,\n",
       " 106,\n",
       " 980,\n",
       " 701,\n",
       " 260,\n",
       " 153,\n",
       " 881,\n",
       " 668,\n",
       " 763,\n",
       " 799,\n",
       " 113,\n",
       " 426,\n",
       " 382,\n",
       " 288,\n",
       " 244,\n",
       " 771,\n",
       " 21,\n",
       " 493,\n",
       " 53,\n",
       " 855,\n",
       " 3,\n",
       " 88,\n",
       " 534,\n",
       " 174,\n",
       " 1043,\n",
       " 102,\n",
       " 2,\n",
       " 988,\n",
       " 796,\n",
       " 549,\n",
       " 221,\n",
       " 419,\n",
       " 676,\n",
       " 459,\n",
       " 626,\n",
       " 450,\n",
       " 740,\n",
       " 929,\n",
       " 623,\n",
       " 694,\n",
       " 910,\n",
       " 114,\n",
       " 335,\n",
       " 518,\n",
       " 208,\n",
       " 139,\n",
       " 406,\n",
       " 769,\n",
       " 123,\n",
       " 716,\n",
       " 125,\n",
       " 964,\n",
       " 177,\n",
       " 49,\n",
       " 806,\n",
       " 973,\n",
       " 42,\n",
       " 298,\n",
       " 320,\n",
       " 933,\n",
       " 636,\n",
       " 512,\n",
       " 446,\n",
       " 646,\n",
       " 212,\n",
       " 972,\n",
       " 975,\n",
       " 184,\n",
       " 915,\n",
       " 688,\n",
       " 775,\n",
       " 456,\n",
       " 56,\n",
       " 278,\n",
       " 115,\n",
       " 211,\n",
       " 203,\n",
       " 111,\n",
       " 669,\n",
       " 567,\n",
       " 14,\n",
       " 453,\n",
       " 897,\n",
       " 22,\n",
       " 601,\n",
       " 848,\n",
       " 902,\n",
       " 1019,\n",
       " 274,\n",
       " 995,\n",
       " 950,\n",
       " 625,\n",
       " 791,\n",
       " 250,\n",
       " 969,\n",
       " 883,\n",
       " 655,\n",
       " 768,\n",
       " 66,\n",
       " 957,\n",
       " 778,\n",
       " 797,\n",
       " 613,\n",
       " 246,\n",
       " 323,\n",
       " 531,\n",
       " 186,\n",
       " 418,\n",
       " 330,\n",
       " 10,\n",
       " 587,\n",
       " 148,\n",
       " 1033,\n",
       " 1037,\n",
       " 1026,\n",
       " 159,\n",
       " 533,\n",
       " 501,\n",
       " 454,\n",
       " 661,\n",
       " 98,\n",
       " 487,\n",
       " 899,\n",
       " 642,\n",
       " 412,\n",
       " 852,\n",
       " 151,\n",
       " 229,\n",
       " 607,\n",
       " 281,\n",
       " 679,\n",
       " 649,\n",
       " 8,\n",
       " 35,\n",
       " 415,\n",
       " 72,\n",
       " 145,\n",
       " 349,\n",
       " 421,\n",
       " 582,\n",
       " 639,\n",
       " 436,\n",
       " 37,\n",
       " 275,\n",
       " 373,\n",
       " 658,\n",
       " 586,\n",
       " 985,\n",
       " 394,\n",
       " 129,\n",
       " 350,\n",
       " 573,\n",
       " 994,\n",
       " 285,\n",
       " 445,\n",
       " 522,\n",
       " 504,\n",
       " 519,\n",
       " 92,\n",
       " 305,\n",
       " 352,\n",
       " 885,\n",
       " 581,\n",
       " 438,\n",
       " 686,\n",
       " 383,\n",
       " 779,\n",
       " 978,\n",
       " 816,\n",
       " 31,\n",
       " 135,\n",
       " 439,\n",
       " 903,\n",
       " 822,\n",
       " 552,\n",
       " 188,\n",
       " 347,\n",
       " 452,\n",
       " 393,\n",
       " 268,\n",
       " 149,\n",
       " 568,\n",
       " 242,\n",
       " 550,\n",
       " 824,\n",
       " 170,\n",
       " 505,\n",
       " 219,\n",
       " 931,\n",
       " 690,\n",
       " 508,\n",
       " 1041,\n",
       " 105,\n",
       " 821,\n",
       " 329,\n",
       " 798,\n",
       " 63,\n",
       " 940,\n",
       " 628,\n",
       " 301,\n",
       " 741,\n",
       " 546,\n",
       " 737,\n",
       " 197,\n",
       " 1050,\n",
       " 390,\n",
       " 913,\n",
       " 619,\n",
       " 930,\n",
       " 548,\n",
       " 659,\n",
       " 675,\n",
       " 127,\n",
       " 513,\n",
       " 858,\n",
       " 86,\n",
       " 943,\n",
       " 719,\n",
       " 322,\n",
       " 503,\n",
       " 908,\n",
       " 831,\n",
       " 46,\n",
       " 805,\n",
       " 745,\n",
       " 91,\n",
       " 425,\n",
       " 182,\n",
       " 128,\n",
       " 782,\n",
       " 884,\n",
       " 530,\n",
       " 317,\n",
       " 734,\n",
       " 392,\n",
       " 490,\n",
       " 811,\n",
       " 673,\n",
       " 71,\n",
       " 455,\n",
       " 160,\n",
       " 277,\n",
       " 558,\n",
       " 0,\n",
       " 810,\n",
       " 696,\n",
       " 576,\n",
       " 413,\n",
       " 397,\n",
       " 986,\n",
       " 162,\n",
       " 606,\n",
       " 693,\n",
       " 866,\n",
       " 64,\n",
       " 616,\n",
       " 1038,\n",
       " 283,\n",
       " 38,\n",
       " 588,\n",
       " 379,\n",
       " 509,\n",
       " 402,\n",
       " 93,\n",
       " 654,\n",
       " 615,\n",
       " 50,\n",
       " 263,\n",
       " 74,\n",
       " 794,\n",
       " 596,\n",
       " 467,\n",
       " 218,\n",
       " 882,\n",
       " 368,\n",
       " 124,\n",
       " 1040,\n",
       " 266,\n",
       " 705,\n",
       " 921,\n",
       " 355,\n",
       " 166,\n",
       " 803,\n",
       " 585,\n",
       " 371,\n",
       " 442,\n",
       " 377,\n",
       " 920,\n",
       " 589,\n",
       " 321,\n",
       " 912,\n",
       " 1002,\n",
       " 1018,\n",
       " 259,\n",
       " 247,\n",
       " 834,\n",
       " 370,\n",
       " 252,\n",
       " 597,\n",
       " 33,\n",
       " 214,\n",
       " 751,\n",
       " 635,\n",
       " 492,\n",
       " 458,\n",
       " 754,\n",
       " 328,\n",
       " 832,\n",
       " 237,\n",
       " 257,\n",
       " 650,\n",
       " 603,\n",
       " 630,\n",
       " 457,\n",
       " 222,\n",
       " 327,\n",
       " 622,\n",
       " 795,\n",
       " 551,\n",
       " 99,\n",
       " 906,\n",
       " 360,\n",
       " 1023,\n",
       " 339,\n",
       " 179,\n",
       " 173,\n",
       " 670,\n",
       " 752,\n",
       " 367,\n",
       " 536,\n",
       " 923,\n",
       " 662,\n",
       " 372,\n",
       " 116,\n",
       " 1,\n",
       " 199,\n",
       " 387,\n",
       " 554,\n",
       " 475,\n",
       " 132,\n",
       " 547,\n",
       " 384,\n",
       " 19,\n",
       " 784,\n",
       " 1015,\n",
       " 167,\n",
       " 431,\n",
       " 854,\n",
       " 570,\n",
       " 695,\n",
       " 664,\n",
       " 843,\n",
       " 555,\n",
       " 171,\n",
       " 482,\n",
       " 612,\n",
       " 652,\n",
       " 983,\n",
       " 895,\n",
       " 39,\n",
       " 478,\n",
       " 528,\n",
       " 316,\n",
       " 83,\n",
       " 757,\n",
       " 300,\n",
       " 1045,\n",
       " 131,\n",
       " 1029,\n",
       " 748,\n",
       " 423,\n",
       " 175,\n",
       " 60,\n",
       " 566,\n",
       " 157,\n",
       " 702,\n",
       " 460,\n",
       " 954,\n",
       " 742,\n",
       " 618,\n",
       " 919,\n",
       " 496,\n",
       " 340,\n",
       " 853,\n",
       " 273,\n",
       " 1030,\n",
       " 253,\n",
       " 207,\n",
       " 168,\n",
       " 178,\n",
       " 574,\n",
       " 968,\n",
       " 945,\n",
       " 435,\n",
       " 833,\n",
       " 434,\n",
       " 258,\n",
       " 433,\n",
       " 786,\n",
       " 814,\n",
       " 361,\n",
       " 788,\n",
       " 150,\n",
       " 867,\n",
       " 718,\n",
       " 656,\n",
       " 946,\n",
       " 620,\n",
       " 727,\n",
       " 488,\n",
       " 818,\n",
       " 610,\n",
       " 516,\n",
       " 470,\n",
       " 477,\n",
       " 255,\n",
       " 224,\n",
       " 817,\n",
       " 887,\n",
       " 916,\n",
       " 1020,\n",
       " 984,\n",
       " 331,\n",
       " 1014,\n",
       " 466,\n",
       " 580,\n",
       " 605,\n",
       " 345,\n",
       " 1006,\n",
       " 645,\n",
       " 591,\n",
       " 785,\n",
       " 880,\n",
       " 254,\n",
       " 67,\n",
       " 40,\n",
       " 967,\n",
       " 599,\n",
       " 998,\n",
       " 781,\n",
       " 579,\n",
       " 709,\n",
       " 391,\n",
       " 449,\n",
       " 287,\n",
       " 43,\n",
       " 215,\n",
       " 756,\n",
       " 366,\n",
       " 133,\n",
       " 24,\n",
       " 878,\n",
       " 236,\n",
       " 495,\n",
       " 225,\n",
       " 697,\n",
       " 947,\n",
       " 271,\n",
       " 158,\n",
       " 1003,\n",
       " 614,\n",
       " 541,\n",
       " 437,\n",
       " 180,\n",
       " 876,\n",
       " 89,\n",
       " 959,\n",
       " 414,\n",
       " 918,\n",
       " 248,\n",
       " 804,\n",
       " 289,\n",
       " 527,\n",
       " 17,\n",
       " 109,\n",
       " 730,\n",
       " 765,\n",
       " 671,\n",
       " 94,\n",
       " 602,\n",
       " 917,\n",
       " 1011,\n",
       " 937,\n",
       " 724,\n",
       " 462,\n",
       " 971,\n",
       " 169,\n",
       " 494,\n",
       " 873,\n",
       " 715,\n",
       " 970,\n",
       " 34,\n",
       " 932,\n",
       " 471,\n",
       " 851,\n",
       " 1008,\n",
       " 726,\n",
       " 537,\n",
       " 26,\n",
       " 890,\n",
       " 359,\n",
       " 343,\n",
       " 525,\n",
       " 358,\n",
       " 611,\n",
       " 922,\n",
       " 979,\n",
       " 201,\n",
       " 877,\n",
       " 511,\n",
       " 914,\n",
       " 429,\n",
       " 722,\n",
       " 747,\n",
       " 51,\n",
       " 238,\n",
       " 862,\n",
       " 196,\n",
       " 265,\n",
       " 939,\n",
       " 870,\n",
       " 951,\n",
       " 497,\n",
       " 838,\n",
       " 189,\n",
       " 871,\n",
       " 571,\n",
       " 152,\n",
       " 332,\n",
       " 904,\n",
       " 989,\n",
       " 61,\n",
       " 381,\n",
       " 767,\n",
       " 981,\n",
       " 826,\n",
       " 308,\n",
       " 130,\n",
       " 205,\n",
       " 1044,\n",
       " 608,\n",
       " 1012,\n",
       " 812,\n",
       " 365,\n",
       " 770,\n",
       " 388,\n",
       " 30,\n",
       " 684,\n",
       " 307,\n",
       " 572,\n",
       " 713,\n",
       " 282,\n",
       " 627,\n",
       " 792,\n",
       " 987,\n",
       " 894,\n",
       " 542,\n",
       " 830,\n",
       " 609,\n",
       " 164,\n",
       " 845,\n",
       " 401,\n",
       " 1046,\n",
       " 909,\n",
       " 407,\n",
       " 1027,\n",
       " 553,\n",
       " 405,\n",
       " 819,\n",
       " 860,\n",
       " 590,\n",
       " 68,\n",
       " 837,\n",
       " 1016,\n",
       " 598,\n",
       " 16,\n",
       " 290,\n",
       " 28,\n",
       " 790,\n",
       " 310,\n",
       " 243,\n",
       " 735,\n",
       " 311,\n",
       " 761,\n",
       " 276,\n",
       " 641,\n",
       " 900,\n",
       " 336,\n",
       " 286,\n",
       " 484,\n",
       " 209,\n",
       " 545,\n",
       " 376,\n",
       " 996,\n",
       " 18,\n",
       " 142,\n",
       " 875,\n",
       " 465,\n",
       " 723,\n",
       " 982,\n",
       " 315,\n",
       " 185,\n",
       " 155,\n",
       " 514,\n",
       " 396,\n",
       " 955,\n",
       " 80,\n",
       " 783,\n",
       " 991,\n",
       " 6,\n",
       " 746,\n",
       " 911,\n",
       " 234,\n",
       " 220,\n",
       " 952,\n",
       " 165,\n",
       " 57,\n",
       " 738,\n",
       " 731,\n",
       " 386,\n",
       " 755,\n",
       " 422,\n",
       " 657,\n",
       " 506,\n",
       " 118,\n",
       " 140,\n",
       " 928,\n",
       " 565,\n",
       " 314,\n",
       " 200,\n",
       " 648,\n",
       " 324,\n",
       " 749,\n",
       " 233,\n",
       " 813,\n",
       " 680,\n",
       " 156,\n",
       " 357,\n",
       " 500,\n",
       " 1024,\n",
       " 137,\n",
       " 777,\n",
       " 802,\n",
       " 312,\n",
       " 85,\n",
       " 905,\n",
       " 69,\n",
       " 498,\n",
       " 337,\n",
       " 408,\n",
       " 81,\n",
       " 924,\n",
       " 399,\n",
       " 107,\n",
       " 82,\n",
       " 901,\n",
       " 520,\n",
       " 774,\n",
       " 863,\n",
       " 354,\n",
       " 309,\n",
       " 356,\n",
       " 338,\n",
       " 291,\n",
       " 1004,\n",
       " 1048,\n",
       " 296,\n",
       " 32,\n",
       " 699,\n",
       " 195,\n",
       " 70,\n",
       " 279,\n",
       " 181,\n",
       " 976,\n",
       " 898,\n",
       " 692,\n",
       " 297,\n",
       " 868,\n",
       " 667,\n",
       " 187,\n",
       " 1047,\n",
       " 227,\n",
       " 879,\n",
       " 190,\n",
       " 823,\n",
       " 577,\n",
       " 638,\n",
       " 41,\n",
       " 637,\n",
       " 789,\n",
       " 963,\n",
       " 700,\n",
       " 1007,\n",
       " 256,\n",
       " 666,\n",
       " 23,\n",
       " 90,\n",
       " 294,\n",
       " 507,\n",
       " 927,\n",
       " 535,\n",
       " ...]"
      ]
     },
     "execution_count": 1795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate in real time, it might take some seconds\n",
    "# (since it loop to all articles against the targeted article)\n",
    "\n",
    "relavant_articles = find_similar_articles(420)\n",
    "\n",
    "relavant_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>What a difference a version number makes! With...</td>\n",
       "      <td>Apache Sparkâ¢ 2.0: Impressive Improvements to ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>The Apache Spark website documents the propert...</td>\n",
       "      <td>Configuring the Apache Spark SQL Context</td>\n",
       "      <td>Live</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>The Apache Spark SQL component has several sub...</td>\n",
       "      <td>Apache Spark SQL Analyzer Resolves Order-by Co...</td>\n",
       "      <td>Live</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>APACHE SPARK ANALYTICSCombine ApacheÂ® Sparkâ¢ w...</td>\n",
       "      <td>Combine ApacheÂ® Sparkâ¢ with other cloud servic...</td>\n",
       "      <td>Apache Spark Analytics</td>\n",
       "      <td>Live</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>From the big crop of books about Apache Sparkâ¢...</td>\n",
       "      <td>A Survey of Books about Apache Sparkâ¢</td>\n",
       "      <td>Live</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>This post provides a brief summary of sample c...</td>\n",
       "      <td>Apache Sparkâ¢ 2.0: Migrating Applications</td>\n",
       "      <td>Live</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>{ spark .tc } * Community\\r\\n * Projects\\r\\n *...</td>\n",
       "      <td>Spark SQL Version 1.6 runs queries faster! Tha...</td>\n",
       "      <td>Spark SQL - Rapid Performance Evolution</td>\n",
       "      <td>Live</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Sean looks back on his first encounter with Sp...</td>\n",
       "      <td>Interview with Sean Li, New Apache Sparkâ¢ Comm...</td>\n",
       "      <td>Live</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Get faster queries and write less code too. Le...</td>\n",
       "      <td>Speed your SQL Queries with Spark SQL</td>\n",
       "      <td>Live</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video shows you how to use the Spark SQL ...</td>\n",
       "      <td>Build SQL queries with Apache Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Early methods to integrate machine learning us...</td>\n",
       "      <td>Apache Sparkâ¢ 2.0: Extend Structured Streaming...</td>\n",
       "      <td>Live</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>Learn Why and How To Use Spark for large amoun...</td>\n",
       "      <td>What is Spark?</td>\n",
       "      <td>Live</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>RStudio Blog * Home\\r\\n\\r\\n * Subscribe to fee...</td>\n",
       "      <td>Weâre excited today to announce sparklyr, a ne...</td>\n",
       "      <td>sparklyr â R interface for Apache Spark</td>\n",
       "      <td>Live</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>How to build SQL Queries in a Scala notebook u...</td>\n",
       "      <td>Build SQL Queries in a Scala notebook using Ap...</td>\n",
       "      <td>Live</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>How to build SQL Queries in a Scala notebook u...</td>\n",
       "      <td>Build Spark SQL Queries</td>\n",
       "      <td>Live</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Now that the dust has settled on Apache Spark ...</td>\n",
       "      <td>Apache Spark 2.0: Machine Learning. Under the ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>A handful of talks at the recent Spark Summit ...</td>\n",
       "      <td>Apache Spark as the New Engine of Genomics</td>\n",
       "      <td>Live</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>In this post I will show you how to use the IB...</td>\n",
       "      <td>Access IBM Analytics for Apache Spark from RSt...</td>\n",
       "      <td>Live</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Learn how to create a connection to dashDB dat...</td>\n",
       "      <td>Load dashDB Data with Apache Spark</td>\n",
       "      <td>Live</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>One of the best things about Apache Spark is t...</td>\n",
       "      <td>Apache Spark: Upgrade and speed-up your analytics</td>\n",
       "      <td>Live</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     doc_body  \\\n",
       "article_id                                                      \n",
       "389         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "993         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "949         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "592         APACHE SPARK ANALYTICSCombine ApacheÂ® Sparkâ¢ w...   \n",
       "714         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "117         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "678         { spark .tc } * Community\\r\\n * Projects\\r\\n *...   \n",
       "942         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "231         Skip to main content IBM developerWorks / Deve...   \n",
       "925         Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "15          * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "463         Skip navigation Upload Sign in SearchLoading.....   \n",
       "353         RStudio Blog * Home\\r\\n\\r\\n * Subscribe to fee...   \n",
       "835         Skip to main content IBM developerWorks / Deve...   \n",
       "907         Skip to main content IBM developerWorks / Deve...   \n",
       "284         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "977         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "600         Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "595         Skip to main content IBM developerWorks / Deve...   \n",
       "997         Skip to main content IBM developerWorks / Deve...   \n",
       "\n",
       "                                              doc_description  \\\n",
       "article_id                                                      \n",
       "389         What a difference a version number makes! With...   \n",
       "993         The Apache Spark website documents the propert...   \n",
       "949         The Apache Spark SQL component has several sub...   \n",
       "592         Combine ApacheÂ® Sparkâ¢ with other cloud servic...   \n",
       "714         From the big crop of books about Apache Sparkâ¢...   \n",
       "117         This post provides a brief summary of sample c...   \n",
       "678         Spark SQL Version 1.6 runs queries faster! Tha...   \n",
       "942         Sean looks back on his first encounter with Sp...   \n",
       "231         Get faster queries and write less code too. Le...   \n",
       "925         This video shows you how to use the Spark SQL ...   \n",
       "15          Early methods to integrate machine learning us...   \n",
       "463         Learn Why and How To Use Spark for large amoun...   \n",
       "353         Weâre excited today to announce sparklyr, a ne...   \n",
       "835         How to build SQL Queries in a Scala notebook u...   \n",
       "907         How to build SQL Queries in a Scala notebook u...   \n",
       "284         Now that the dust has settled on Apache Spark ...   \n",
       "977         A handful of talks at the recent Spark Summit ...   \n",
       "600         In this post I will show you how to use the IB...   \n",
       "595         Learn how to create a connection to dashDB dat...   \n",
       "997         One of the best things about Apache Spark is t...   \n",
       "\n",
       "                                                doc_full_name doc_status  \\\n",
       "article_id                                                                 \n",
       "389         Apache Sparkâ¢ 2.0: Impressive Improvements to ...       Live   \n",
       "993                  Configuring the Apache Spark SQL Context       Live   \n",
       "949         Apache Spark SQL Analyzer Resolves Order-by Co...       Live   \n",
       "592                                    Apache Spark Analytics       Live   \n",
       "714                     A Survey of Books about Apache Sparkâ¢       Live   \n",
       "117                 Apache Sparkâ¢ 2.0: Migrating Applications       Live   \n",
       "678                   Spark SQL - Rapid Performance Evolution       Live   \n",
       "942         Interview with Sean Li, New Apache Sparkâ¢ Comm...       Live   \n",
       "231                     Speed your SQL Queries with Spark SQL       Live   \n",
       "925                Build SQL queries with Apache Spark in DSX       Live   \n",
       "15          Apache Sparkâ¢ 2.0: Extend Structured Streaming...       Live   \n",
       "463                                            What is Spark?       Live   \n",
       "353                   sparklyr â R interface for Apache Spark       Live   \n",
       "835         Build SQL Queries in a Scala notebook using Ap...       Live   \n",
       "907                                   Build Spark SQL Queries       Live   \n",
       "284         Apache Spark 2.0: Machine Learning. Under the ...       Live   \n",
       "977                Apache Spark as the New Engine of Genomics       Live   \n",
       "600         Access IBM Analytics for Apache Spark from RSt...       Live   \n",
       "595                        Load dashDB Data with Apache Spark       Live   \n",
       "997         Apache Spark: Upgrade and speed-up your analytics       Live   \n",
       "\n",
       "            article_id  \n",
       "article_id              \n",
       "389                389  \n",
       "993                993  \n",
       "949                949  \n",
       "592                592  \n",
       "714                714  \n",
       "117                117  \n",
       "678                678  \n",
       "942                942  \n",
       "231                231  \n",
       "925                925  \n",
       "15                  15  \n",
       "463                463  \n",
       "353                353  \n",
       "835                835  \n",
       "907                907  \n",
       "284                284  \n",
       "977                977  \n",
       "600                600  \n",
       "595                595  \n",
       "997                997  "
      ]
     },
     "execution_count": 1663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking relavancy. They are order by relavancy. \n",
    "\n",
    "df_nlp.iloc[relavant_articles].head(20)\n",
    "\n",
    "# Looks like related to 'Apache Spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above find_similar_articles work well, however it is calculating on the fly, could take time to load, for user experience, real time calculating too expensive. So make a article_article_similary dataframe for look up. (the content-content rec). This can be act as, people view X article also might be interested in Y article based on relavancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "Make a dataframe, store article-article-similarity.\n",
    "(Based on the cleaned df_content version dataset: df_nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1041</th>\n",
       "      <th>1042</th>\n",
       "      <th>1043</th>\n",
       "      <th>1044</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows Ã 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "article_id                                                              ...   \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "1046         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1047         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1048         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1049         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1050         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "article_id  1041  1042  1043  1044  1045  1046  1047  1048  1049  1050  \n",
       "article_id                                                              \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "1046         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1047         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1048         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1049         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1050         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[1051 rows x 1051 columns]"
      ]
     },
     "execution_count": 1673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a user-user dummy dataframe based on shape of user_article_dict_trimmed\n",
    "article_article = pd.DataFrame(\n",
    "                data=np.zeros((len(df_nlp.index),len(df_nlp.index))), \n",
    "                index=df_nlp.index, \n",
    "                columns=df_nlp.index\n",
    "            )\n",
    "\n",
    "article_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0 of 1050.\n",
      "Processing row 1 of 1050.\n",
      "Processing row 2 of 1050.\n",
      "Processing row 3 of 1050.\n",
      "Processing row 4 of 1050.\n",
      "Processing row 5 of 1050.\n",
      "Processing row 6 of 1050.\n",
      "Processing row 7 of 1050.\n",
      "Processing row 8 of 1050.\n",
      "Processing row 9 of 1050.\n",
      "Processing row 10 of 1050.\n",
      "Processing row 11 of 1050.\n",
      "Processing row 12 of 1050.\n",
      "Processing row 13 of 1050.\n",
      "Processing row 14 of 1050.\n",
      "Processing row 15 of 1050.\n",
      "Processing row 16 of 1050.\n",
      "Processing row 17 of 1050.\n",
      "Processing row 18 of 1050.\n",
      "Processing row 19 of 1050.\n",
      "Processing row 20 of 1050.\n",
      "Processing row 21 of 1050.\n",
      "Processing row 22 of 1050.\n",
      "Processing row 23 of 1050.\n",
      "Processing row 24 of 1050.\n",
      "Processing row 25 of 1050.\n",
      "Processing row 26 of 1050.\n",
      "Processing row 27 of 1050.\n",
      "Processing row 28 of 1050.\n",
      "Processing row 29 of 1050.\n",
      "Processing row 30 of 1050.\n",
      "Processing row 31 of 1050.\n",
      "Processing row 32 of 1050.\n",
      "Processing row 33 of 1050.\n",
      "Processing row 34 of 1050.\n",
      "Processing row 35 of 1050.\n",
      "Processing row 36 of 1050.\n",
      "Processing row 37 of 1050.\n",
      "Processing row 38 of 1050.\n",
      "Processing row 39 of 1050.\n",
      "Processing row 40 of 1050.\n",
      "Processing row 41 of 1050.\n",
      "Processing row 42 of 1050.\n",
      "Processing row 43 of 1050.\n",
      "Processing row 44 of 1050.\n",
      "Processing row 45 of 1050.\n",
      "Processing row 46 of 1050.\n",
      "Processing row 47 of 1050.\n",
      "Processing row 48 of 1050.\n",
      "Processing row 49 of 1050.\n",
      "Processing row 50 of 1050.\n",
      "Processing row 51 of 1050.\n",
      "Processing row 52 of 1050.\n",
      "Processing row 53 of 1050.\n",
      "Processing row 54 of 1050.\n",
      "Processing row 55 of 1050.\n",
      "Processing row 56 of 1050.\n",
      "Processing row 57 of 1050.\n",
      "Processing row 58 of 1050.\n",
      "Processing row 59 of 1050.\n",
      "Processing row 60 of 1050.\n",
      "Processing row 61 of 1050.\n",
      "Processing row 62 of 1050.\n",
      "Processing row 63 of 1050.\n",
      "Processing row 64 of 1050.\n",
      "Processing row 65 of 1050.\n",
      "Processing row 66 of 1050.\n",
      "Processing row 67 of 1050.\n",
      "Processing row 68 of 1050.\n",
      "Processing row 69 of 1050.\n",
      "Processing row 70 of 1050.\n",
      "Processing row 71 of 1050.\n",
      "Processing row 72 of 1050.\n",
      "Processing row 73 of 1050.\n",
      "Processing row 74 of 1050.\n",
      "Processing row 75 of 1050.\n",
      "Processing row 76 of 1050.\n",
      "Processing row 77 of 1050.\n",
      "Processing row 78 of 1050.\n",
      "Processing row 79 of 1050.\n",
      "Processing row 80 of 1050.\n",
      "Processing row 81 of 1050.\n",
      "Processing row 82 of 1050.\n",
      "Processing row 83 of 1050.\n",
      "Processing row 84 of 1050.\n",
      "Processing row 85 of 1050.\n",
      "Processing row 86 of 1050.\n",
      "Processing row 87 of 1050.\n",
      "Processing row 88 of 1050.\n",
      "Processing row 89 of 1050.\n",
      "Processing row 90 of 1050.\n",
      "Processing row 91 of 1050.\n",
      "Processing row 92 of 1050.\n",
      "Processing row 93 of 1050.\n",
      "Processing row 94 of 1050.\n",
      "Processing row 95 of 1050.\n",
      "Processing row 96 of 1050.\n",
      "Processing row 97 of 1050.\n",
      "Processing row 98 of 1050.\n",
      "Processing row 99 of 1050.\n",
      "Processing row 100 of 1050.\n",
      "Processing row 101 of 1050.\n",
      "Processing row 102 of 1050.\n",
      "Processing row 103 of 1050.\n",
      "Processing row 104 of 1050.\n",
      "Processing row 105 of 1050.\n",
      "Processing row 106 of 1050.\n",
      "Processing row 107 of 1050.\n",
      "Processing row 108 of 1050.\n",
      "Processing row 109 of 1050.\n",
      "Processing row 110 of 1050.\n",
      "Processing row 111 of 1050.\n",
      "Processing row 112 of 1050.\n",
      "Processing row 113 of 1050.\n",
      "Processing row 114 of 1050.\n",
      "Processing row 115 of 1050.\n",
      "Processing row 116 of 1050.\n",
      "Processing row 117 of 1050.\n",
      "Processing row 118 of 1050.\n",
      "Processing row 119 of 1050.\n",
      "Processing row 120 of 1050.\n",
      "Processing row 121 of 1050.\n",
      "Processing row 122 of 1050.\n",
      "Processing row 123 of 1050.\n",
      "Processing row 124 of 1050.\n",
      "Processing row 125 of 1050.\n",
      "Processing row 126 of 1050.\n",
      "Processing row 127 of 1050.\n",
      "Processing row 128 of 1050.\n",
      "Processing row 129 of 1050.\n",
      "Processing row 130 of 1050.\n",
      "Processing row 131 of 1050.\n",
      "Processing row 132 of 1050.\n",
      "Processing row 133 of 1050.\n",
      "Processing row 134 of 1050.\n",
      "Processing row 135 of 1050.\n",
      "Processing row 136 of 1050.\n",
      "Processing row 137 of 1050.\n",
      "Processing row 138 of 1050.\n",
      "Processing row 139 of 1050.\n",
      "Processing row 140 of 1050.\n",
      "Processing row 141 of 1050.\n",
      "Processing row 142 of 1050.\n",
      "Processing row 143 of 1050.\n",
      "Processing row 144 of 1050.\n",
      "Processing row 145 of 1050.\n",
      "Processing row 146 of 1050.\n",
      "Processing row 147 of 1050.\n",
      "Processing row 148 of 1050.\n",
      "Processing row 149 of 1050.\n",
      "Processing row 150 of 1050.\n",
      "Processing row 151 of 1050.\n",
      "Processing row 152 of 1050.\n",
      "Processing row 153 of 1050.\n",
      "Processing row 154 of 1050.\n",
      "Processing row 155 of 1050.\n",
      "Processing row 156 of 1050.\n",
      "Processing row 157 of 1050.\n",
      "Processing row 158 of 1050.\n",
      "Processing row 159 of 1050.\n",
      "Processing row 160 of 1050.\n",
      "Processing row 161 of 1050.\n",
      "Processing row 162 of 1050.\n",
      "Processing row 163 of 1050.\n",
      "Processing row 164 of 1050.\n",
      "Processing row 165 of 1050.\n",
      "Processing row 166 of 1050.\n",
      "Processing row 167 of 1050.\n",
      "Processing row 168 of 1050.\n",
      "Processing row 169 of 1050.\n",
      "Processing row 170 of 1050.\n",
      "Processing row 171 of 1050.\n",
      "Processing row 172 of 1050.\n",
      "Processing row 173 of 1050.\n",
      "Processing row 174 of 1050.\n",
      "Processing row 175 of 1050.\n",
      "Processing row 176 of 1050.\n",
      "Processing row 177 of 1050.\n",
      "Processing row 178 of 1050.\n",
      "Processing row 179 of 1050.\n",
      "Processing row 180 of 1050.\n",
      "Processing row 181 of 1050.\n",
      "Processing row 182 of 1050.\n",
      "Processing row 183 of 1050.\n",
      "Processing row 184 of 1050.\n",
      "Processing row 185 of 1050.\n",
      "Processing row 186 of 1050.\n",
      "Processing row 187 of 1050.\n",
      "Processing row 188 of 1050.\n",
      "Processing row 189 of 1050.\n",
      "Processing row 190 of 1050.\n",
      "Processing row 191 of 1050.\n",
      "Processing row 192 of 1050.\n",
      "Processing row 193 of 1050.\n",
      "Processing row 194 of 1050.\n",
      "Processing row 195 of 1050.\n",
      "Processing row 196 of 1050.\n",
      "Processing row 197 of 1050.\n",
      "Processing row 198 of 1050.\n",
      "Processing row 199 of 1050.\n",
      "Processing row 200 of 1050.\n",
      "Processing row 201 of 1050.\n",
      "Processing row 202 of 1050.\n",
      "Processing row 203 of 1050.\n",
      "Processing row 204 of 1050.\n",
      "Processing row 205 of 1050.\n",
      "Processing row 206 of 1050.\n",
      "Processing row 207 of 1050.\n",
      "Processing row 208 of 1050.\n",
      "Processing row 209 of 1050.\n",
      "Processing row 210 of 1050.\n",
      "Processing row 211 of 1050.\n",
      "Processing row 212 of 1050.\n",
      "Processing row 213 of 1050.\n",
      "Processing row 214 of 1050.\n",
      "Processing row 215 of 1050.\n",
      "Processing row 216 of 1050.\n",
      "Processing row 217 of 1050.\n",
      "Processing row 218 of 1050.\n",
      "Processing row 219 of 1050.\n",
      "Processing row 220 of 1050.\n",
      "Processing row 221 of 1050.\n",
      "Processing row 222 of 1050.\n",
      "Processing row 223 of 1050.\n",
      "Processing row 224 of 1050.\n",
      "Processing row 225 of 1050.\n",
      "Processing row 226 of 1050.\n",
      "Processing row 227 of 1050.\n",
      "Processing row 228 of 1050.\n",
      "Processing row 229 of 1050.\n",
      "Processing row 230 of 1050.\n",
      "Processing row 231 of 1050.\n",
      "Processing row 232 of 1050.\n",
      "Processing row 233 of 1050.\n",
      "Processing row 234 of 1050.\n",
      "Processing row 235 of 1050.\n",
      "Processing row 236 of 1050.\n",
      "Processing row 237 of 1050.\n",
      "Processing row 238 of 1050.\n",
      "Processing row 239 of 1050.\n",
      "Processing row 240 of 1050.\n",
      "Processing row 241 of 1050.\n",
      "Processing row 242 of 1050.\n",
      "Processing row 243 of 1050.\n",
      "Processing row 244 of 1050.\n",
      "Processing row 245 of 1050.\n",
      "Processing row 246 of 1050.\n",
      "Processing row 247 of 1050.\n",
      "Processing row 248 of 1050.\n",
      "Processing row 249 of 1050.\n",
      "Processing row 250 of 1050.\n",
      "Processing row 251 of 1050.\n",
      "Processing row 252 of 1050.\n",
      "Processing row 253 of 1050.\n",
      "Processing row 254 of 1050.\n",
      "Processing row 255 of 1050.\n",
      "Processing row 256 of 1050.\n",
      "Processing row 257 of 1050.\n",
      "Processing row 258 of 1050.\n",
      "Processing row 259 of 1050.\n",
      "Processing row 260 of 1050.\n",
      "Processing row 261 of 1050.\n",
      "Processing row 262 of 1050.\n",
      "Processing row 263 of 1050.\n",
      "Processing row 264 of 1050.\n",
      "Processing row 265 of 1050.\n",
      "Processing row 266 of 1050.\n",
      "Processing row 267 of 1050.\n",
      "Processing row 268 of 1050.\n",
      "Processing row 269 of 1050.\n",
      "Processing row 270 of 1050.\n",
      "Processing row 271 of 1050.\n",
      "Processing row 272 of 1050.\n",
      "Processing row 273 of 1050.\n",
      "Processing row 274 of 1050.\n",
      "Processing row 275 of 1050.\n",
      "Processing row 276 of 1050.\n",
      "Processing row 277 of 1050.\n",
      "Processing row 278 of 1050.\n",
      "Processing row 279 of 1050.\n",
      "Processing row 280 of 1050.\n",
      "Processing row 281 of 1050.\n",
      "Processing row 282 of 1050.\n",
      "Processing row 283 of 1050.\n",
      "Processing row 284 of 1050.\n",
      "Processing row 285 of 1050.\n",
      "Processing row 286 of 1050.\n",
      "Processing row 287 of 1050.\n",
      "Processing row 288 of 1050.\n",
      "Processing row 289 of 1050.\n",
      "Processing row 290 of 1050.\n",
      "Processing row 291 of 1050.\n",
      "Processing row 292 of 1050.\n",
      "Processing row 293 of 1050.\n",
      "Processing row 294 of 1050.\n",
      "Processing row 295 of 1050.\n",
      "Processing row 296 of 1050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 297 of 1050.\n",
      "Processing row 298 of 1050.\n",
      "Processing row 299 of 1050.\n",
      "Processing row 300 of 1050.\n",
      "Processing row 301 of 1050.\n",
      "Processing row 302 of 1050.\n",
      "Processing row 303 of 1050.\n",
      "Processing row 304 of 1050.\n",
      "Processing row 305 of 1050.\n",
      "Processing row 306 of 1050.\n",
      "Processing row 307 of 1050.\n",
      "Processing row 308 of 1050.\n",
      "Processing row 309 of 1050.\n",
      "Processing row 310 of 1050.\n",
      "Processing row 311 of 1050.\n",
      "Processing row 312 of 1050.\n",
      "Processing row 313 of 1050.\n",
      "Processing row 314 of 1050.\n",
      "Processing row 315 of 1050.\n",
      "Processing row 316 of 1050.\n",
      "Processing row 317 of 1050.\n",
      "Processing row 318 of 1050.\n",
      "Processing row 319 of 1050.\n",
      "Processing row 320 of 1050.\n",
      "Processing row 321 of 1050.\n",
      "Processing row 322 of 1050.\n",
      "Processing row 323 of 1050.\n",
      "Processing row 324 of 1050.\n",
      "Processing row 325 of 1050.\n",
      "Processing row 326 of 1050.\n",
      "Processing row 327 of 1050.\n",
      "Processing row 328 of 1050.\n",
      "Processing row 329 of 1050.\n",
      "Processing row 330 of 1050.\n",
      "Processing row 331 of 1050.\n",
      "Processing row 332 of 1050.\n",
      "Processing row 333 of 1050.\n",
      "Processing row 334 of 1050.\n",
      "Processing row 335 of 1050.\n",
      "Processing row 336 of 1050.\n",
      "Processing row 337 of 1050.\n",
      "Processing row 338 of 1050.\n",
      "Processing row 339 of 1050.\n",
      "Processing row 340 of 1050.\n",
      "Processing row 341 of 1050.\n",
      "Processing row 342 of 1050.\n",
      "Processing row 343 of 1050.\n",
      "Processing row 344 of 1050.\n",
      "Processing row 345 of 1050.\n",
      "Processing row 346 of 1050.\n",
      "Processing row 347 of 1050.\n",
      "Processing row 348 of 1050.\n",
      "Processing row 349 of 1050.\n",
      "Processing row 350 of 1050.\n",
      "Processing row 351 of 1050.\n",
      "Processing row 352 of 1050.\n",
      "Processing row 353 of 1050.\n",
      "Processing row 354 of 1050.\n",
      "Processing row 355 of 1050.\n",
      "Processing row 356 of 1050.\n",
      "Processing row 357 of 1050.\n",
      "Processing row 358 of 1050.\n",
      "Processing row 359 of 1050.\n",
      "Processing row 360 of 1050.\n",
      "Processing row 361 of 1050.\n",
      "Processing row 362 of 1050.\n",
      "Processing row 363 of 1050.\n",
      "Processing row 364 of 1050.\n",
      "Processing row 365 of 1050.\n",
      "Processing row 366 of 1050.\n",
      "Processing row 367 of 1050.\n",
      "Processing row 368 of 1050.\n",
      "Processing row 369 of 1050.\n",
      "Processing row 370 of 1050.\n",
      "Processing row 371 of 1050.\n",
      "Processing row 372 of 1050.\n",
      "Processing row 373 of 1050.\n",
      "Processing row 374 of 1050.\n",
      "Processing row 375 of 1050.\n",
      "Processing row 376 of 1050.\n",
      "Processing row 377 of 1050.\n",
      "Processing row 378 of 1050.\n",
      "Processing row 379 of 1050.\n",
      "Processing row 380 of 1050.\n",
      "Processing row 381 of 1050.\n",
      "Processing row 382 of 1050.\n",
      "Processing row 383 of 1050.\n",
      "Processing row 384 of 1050.\n",
      "Processing row 385 of 1050.\n",
      "Processing row 386 of 1050.\n",
      "Processing row 387 of 1050.\n",
      "Processing row 388 of 1050.\n",
      "Processing row 389 of 1050.\n",
      "Processing row 390 of 1050.\n",
      "Processing row 391 of 1050.\n",
      "Processing row 392 of 1050.\n",
      "Processing row 393 of 1050.\n",
      "Processing row 394 of 1050.\n",
      "Processing row 395 of 1050.\n",
      "Processing row 396 of 1050.\n",
      "Processing row 397 of 1050.\n",
      "Processing row 398 of 1050.\n",
      "Processing row 399 of 1050.\n",
      "Processing row 400 of 1050.\n",
      "Processing row 401 of 1050.\n",
      "Processing row 402 of 1050.\n",
      "Processing row 403 of 1050.\n",
      "Processing row 404 of 1050.\n",
      "Processing row 405 of 1050.\n",
      "Processing row 406 of 1050.\n",
      "Processing row 407 of 1050.\n",
      "Processing row 408 of 1050.\n",
      "Processing row 409 of 1050.\n",
      "Processing row 410 of 1050.\n",
      "Processing row 411 of 1050.\n",
      "Processing row 412 of 1050.\n",
      "Processing row 413 of 1050.\n",
      "Processing row 414 of 1050.\n",
      "Processing row 415 of 1050.\n",
      "Processing row 416 of 1050.\n",
      "Processing row 417 of 1050.\n",
      "Processing row 418 of 1050.\n",
      "Processing row 419 of 1050.\n",
      "Processing row 420 of 1050.\n",
      "Processing row 421 of 1050.\n",
      "Processing row 422 of 1050.\n",
      "Processing row 423 of 1050.\n",
      "Processing row 424 of 1050.\n",
      "Processing row 425 of 1050.\n",
      "Processing row 426 of 1050.\n",
      "Processing row 427 of 1050.\n",
      "Processing row 428 of 1050.\n",
      "Processing row 429 of 1050.\n",
      "Processing row 430 of 1050.\n",
      "Processing row 431 of 1050.\n",
      "Processing row 432 of 1050.\n",
      "Processing row 433 of 1050.\n",
      "Processing row 434 of 1050.\n",
      "Processing row 435 of 1050.\n",
      "Processing row 436 of 1050.\n",
      "Processing row 437 of 1050.\n",
      "Processing row 438 of 1050.\n",
      "Processing row 439 of 1050.\n",
      "Processing row 440 of 1050.\n",
      "Processing row 441 of 1050.\n",
      "Processing row 442 of 1050.\n",
      "Processing row 443 of 1050.\n",
      "Processing row 444 of 1050.\n",
      "Processing row 445 of 1050.\n",
      "Processing row 446 of 1050.\n",
      "Processing row 447 of 1050.\n",
      "Processing row 448 of 1050.\n",
      "Processing row 449 of 1050.\n",
      "Processing row 450 of 1050.\n",
      "Processing row 451 of 1050.\n",
      "Processing row 452 of 1050.\n",
      "Processing row 453 of 1050.\n",
      "Processing row 454 of 1050.\n",
      "Processing row 455 of 1050.\n",
      "Processing row 456 of 1050.\n",
      "Processing row 457 of 1050.\n",
      "Processing row 458 of 1050.\n",
      "Processing row 459 of 1050.\n",
      "Processing row 460 of 1050.\n",
      "Processing row 461 of 1050.\n",
      "Processing row 462 of 1050.\n",
      "Processing row 463 of 1050.\n",
      "Processing row 464 of 1050.\n",
      "Processing row 465 of 1050.\n",
      "Processing row 466 of 1050.\n",
      "Processing row 467 of 1050.\n",
      "Processing row 468 of 1050.\n",
      "Processing row 469 of 1050.\n",
      "Processing row 470 of 1050.\n",
      "Processing row 471 of 1050.\n",
      "Processing row 472 of 1050.\n",
      "Processing row 473 of 1050.\n",
      "Processing row 474 of 1050.\n",
      "Processing row 475 of 1050.\n",
      "Processing row 476 of 1050.\n",
      "Processing row 477 of 1050.\n",
      "Processing row 478 of 1050.\n",
      "Processing row 479 of 1050.\n",
      "Processing row 480 of 1050.\n",
      "Processing row 481 of 1050.\n",
      "Processing row 482 of 1050.\n",
      "Processing row 483 of 1050.\n",
      "Processing row 484 of 1050.\n",
      "Processing row 485 of 1050.\n",
      "Processing row 486 of 1050.\n",
      "Processing row 487 of 1050.\n",
      "Processing row 488 of 1050.\n",
      "Processing row 489 of 1050.\n",
      "Processing row 490 of 1050.\n",
      "Processing row 491 of 1050.\n",
      "Processing row 492 of 1050.\n",
      "Processing row 493 of 1050.\n",
      "Processing row 494 of 1050.\n",
      "Processing row 495 of 1050.\n",
      "Processing row 496 of 1050.\n",
      "Processing row 497 of 1050.\n",
      "Processing row 498 of 1050.\n",
      "Processing row 499 of 1050.\n",
      "Processing row 500 of 1050.\n",
      "Processing row 501 of 1050.\n",
      "Processing row 502 of 1050.\n",
      "Processing row 503 of 1050.\n",
      "Processing row 504 of 1050.\n",
      "Processing row 505 of 1050.\n",
      "Processing row 506 of 1050.\n",
      "Processing row 507 of 1050.\n",
      "Processing row 508 of 1050.\n",
      "Processing row 509 of 1050.\n",
      "Processing row 510 of 1050.\n",
      "Processing row 511 of 1050.\n",
      "Processing row 512 of 1050.\n",
      "Processing row 513 of 1050.\n",
      "Processing row 514 of 1050.\n",
      "Processing row 515 of 1050.\n",
      "Processing row 516 of 1050.\n",
      "Processing row 517 of 1050.\n",
      "Processing row 518 of 1050.\n",
      "Processing row 519 of 1050.\n",
      "Processing row 520 of 1050.\n",
      "Processing row 521 of 1050.\n",
      "Processing row 522 of 1050.\n",
      "Processing row 523 of 1050.\n",
      "Processing row 524 of 1050.\n",
      "Processing row 525 of 1050.\n",
      "Processing row 526 of 1050.\n",
      "Processing row 527 of 1050.\n",
      "Processing row 528 of 1050.\n",
      "Processing row 529 of 1050.\n",
      "Processing row 530 of 1050.\n",
      "Processing row 531 of 1050.\n",
      "Processing row 532 of 1050.\n",
      "Processing row 533 of 1050.\n",
      "Processing row 534 of 1050.\n",
      "Processing row 535 of 1050.\n",
      "Processing row 536 of 1050.\n",
      "Processing row 537 of 1050.\n",
      "Processing row 538 of 1050.\n",
      "Processing row 539 of 1050.\n",
      "Processing row 540 of 1050.\n",
      "Processing row 541 of 1050.\n",
      "Processing row 542 of 1050.\n",
      "Processing row 543 of 1050.\n",
      "Processing row 544 of 1050.\n",
      "Processing row 545 of 1050.\n",
      "Processing row 546 of 1050.\n",
      "Processing row 547 of 1050.\n",
      "Processing row 548 of 1050.\n",
      "Processing row 549 of 1050.\n",
      "Processing row 550 of 1050.\n",
      "Processing row 551 of 1050.\n",
      "Processing row 552 of 1050.\n",
      "Processing row 553 of 1050.\n",
      "Processing row 554 of 1050.\n",
      "Processing row 555 of 1050.\n",
      "Processing row 556 of 1050.\n",
      "Processing row 557 of 1050.\n",
      "Processing row 558 of 1050.\n",
      "Processing row 559 of 1050.\n",
      "Processing row 560 of 1050.\n",
      "Processing row 561 of 1050.\n",
      "Processing row 562 of 1050.\n",
      "Processing row 563 of 1050.\n",
      "Processing row 564 of 1050.\n",
      "Processing row 565 of 1050.\n",
      "Processing row 566 of 1050.\n",
      "Processing row 567 of 1050.\n",
      "Processing row 568 of 1050.\n",
      "Processing row 569 of 1050.\n",
      "Processing row 570 of 1050.\n",
      "Processing row 571 of 1050.\n",
      "Processing row 572 of 1050.\n",
      "Processing row 573 of 1050.\n",
      "Processing row 574 of 1050.\n",
      "Processing row 575 of 1050.\n",
      "Processing row 576 of 1050.\n",
      "Processing row 577 of 1050.\n",
      "Processing row 578 of 1050.\n",
      "Processing row 579 of 1050.\n",
      "Processing row 580 of 1050.\n",
      "Processing row 581 of 1050.\n",
      "Processing row 582 of 1050.\n",
      "Processing row 583 of 1050.\n",
      "Processing row 584 of 1050.\n",
      "Processing row 585 of 1050.\n",
      "Processing row 586 of 1050.\n",
      "Processing row 587 of 1050.\n",
      "Processing row 588 of 1050.\n",
      "Processing row 589 of 1050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 590 of 1050.\n",
      "Processing row 591 of 1050.\n",
      "Processing row 592 of 1050.\n",
      "Processing row 593 of 1050.\n",
      "Processing row 594 of 1050.\n",
      "Processing row 595 of 1050.\n",
      "Processing row 596 of 1050.\n",
      "Processing row 597 of 1050.\n",
      "Processing row 598 of 1050.\n",
      "Processing row 599 of 1050.\n",
      "Processing row 600 of 1050.\n",
      "Processing row 601 of 1050.\n",
      "Processing row 602 of 1050.\n",
      "Processing row 603 of 1050.\n",
      "Processing row 604 of 1050.\n",
      "Processing row 605 of 1050.\n",
      "Processing row 606 of 1050.\n",
      "Processing row 607 of 1050.\n",
      "Processing row 608 of 1050.\n",
      "Processing row 609 of 1050.\n",
      "Processing row 610 of 1050.\n",
      "Processing row 611 of 1050.\n",
      "Processing row 612 of 1050.\n",
      "Processing row 613 of 1050.\n",
      "Processing row 614 of 1050.\n",
      "Processing row 615 of 1050.\n",
      "Processing row 616 of 1050.\n",
      "Processing row 617 of 1050.\n",
      "Processing row 618 of 1050.\n",
      "Processing row 619 of 1050.\n",
      "Processing row 620 of 1050.\n",
      "Processing row 621 of 1050.\n",
      "Processing row 622 of 1050.\n",
      "Processing row 623 of 1050.\n",
      "Processing row 624 of 1050.\n",
      "Processing row 625 of 1050.\n",
      "Processing row 626 of 1050.\n",
      "Processing row 627 of 1050.\n",
      "Processing row 628 of 1050.\n",
      "Processing row 629 of 1050.\n",
      "Processing row 630 of 1050.\n",
      "Processing row 631 of 1050.\n",
      "Processing row 632 of 1050.\n",
      "Processing row 633 of 1050.\n",
      "Processing row 634 of 1050.\n",
      "Processing row 635 of 1050.\n",
      "Processing row 636 of 1050.\n",
      "Processing row 637 of 1050.\n",
      "Processing row 638 of 1050.\n",
      "Processing row 639 of 1050.\n",
      "Processing row 640 of 1050.\n",
      "Processing row 641 of 1050.\n",
      "Processing row 642 of 1050.\n",
      "Processing row 643 of 1050.\n",
      "Processing row 644 of 1050.\n",
      "Processing row 645 of 1050.\n",
      "Processing row 646 of 1050.\n",
      "Processing row 647 of 1050.\n",
      "Processing row 648 of 1050.\n",
      "Processing row 649 of 1050.\n",
      "Processing row 650 of 1050.\n",
      "Processing row 651 of 1050.\n",
      "Processing row 652 of 1050.\n",
      "Processing row 653 of 1050.\n",
      "Processing row 654 of 1050.\n",
      "Processing row 655 of 1050.\n",
      "Processing row 656 of 1050.\n",
      "Processing row 657 of 1050.\n",
      "Processing row 658 of 1050.\n",
      "Processing row 659 of 1050.\n",
      "Processing row 660 of 1050.\n",
      "Processing row 661 of 1050.\n",
      "Processing row 662 of 1050.\n",
      "Processing row 663 of 1050.\n",
      "Processing row 664 of 1050.\n",
      "Processing row 665 of 1050.\n",
      "Processing row 666 of 1050.\n",
      "Processing row 667 of 1050.\n",
      "Processing row 668 of 1050.\n",
      "Processing row 669 of 1050.\n",
      "Processing row 670 of 1050.\n",
      "Processing row 671 of 1050.\n",
      "Processing row 672 of 1050.\n",
      "Processing row 673 of 1050.\n",
      "Processing row 674 of 1050.\n",
      "Processing row 675 of 1050.\n",
      "Processing row 676 of 1050.\n",
      "Processing row 677 of 1050.\n",
      "Processing row 678 of 1050.\n",
      "Processing row 679 of 1050.\n",
      "Processing row 680 of 1050.\n",
      "Processing row 681 of 1050.\n",
      "Processing row 682 of 1050.\n",
      "Processing row 683 of 1050.\n",
      "Processing row 684 of 1050.\n",
      "Processing row 685 of 1050.\n",
      "Processing row 686 of 1050.\n",
      "Processing row 687 of 1050.\n",
      "Processing row 688 of 1050.\n",
      "Processing row 689 of 1050.\n",
      "Processing row 690 of 1050.\n",
      "Processing row 691 of 1050.\n",
      "Processing row 692 of 1050.\n",
      "Processing row 693 of 1050.\n",
      "Processing row 694 of 1050.\n",
      "Processing row 695 of 1050.\n",
      "Processing row 696 of 1050.\n",
      "Processing row 697 of 1050.\n",
      "Processing row 698 of 1050.\n",
      "Processing row 699 of 1050.\n",
      "Processing row 700 of 1050.\n",
      "Processing row 701 of 1050.\n",
      "Processing row 702 of 1050.\n",
      "Processing row 703 of 1050.\n",
      "Processing row 704 of 1050.\n",
      "Processing row 705 of 1050.\n",
      "Processing row 706 of 1050.\n",
      "Processing row 707 of 1050.\n",
      "Processing row 708 of 1050.\n",
      "Processing row 709 of 1050.\n",
      "Processing row 710 of 1050.\n",
      "Processing row 711 of 1050.\n",
      "Processing row 712 of 1050.\n",
      "Processing row 713 of 1050.\n",
      "Processing row 714 of 1050.\n",
      "Processing row 715 of 1050.\n",
      "Processing row 716 of 1050.\n",
      "Processing row 717 of 1050.\n",
      "Processing row 718 of 1050.\n",
      "Processing row 719 of 1050.\n",
      "Processing row 720 of 1050.\n",
      "Processing row 721 of 1050.\n",
      "Processing row 722 of 1050.\n",
      "Processing row 723 of 1050.\n",
      "Processing row 724 of 1050.\n",
      "Processing row 725 of 1050.\n",
      "Processing row 726 of 1050.\n",
      "Processing row 727 of 1050.\n",
      "Processing row 728 of 1050.\n",
      "Processing row 729 of 1050.\n",
      "Processing row 730 of 1050.\n",
      "Processing row 731 of 1050.\n",
      "Processing row 732 of 1050.\n",
      "Processing row 733 of 1050.\n",
      "Processing row 734 of 1050.\n",
      "Processing row 735 of 1050.\n",
      "Processing row 736 of 1050.\n",
      "Processing row 737 of 1050.\n",
      "Processing row 738 of 1050.\n",
      "Processing row 739 of 1050.\n",
      "Processing row 740 of 1050.\n",
      "Processing row 741 of 1050.\n",
      "Processing row 742 of 1050.\n",
      "Processing row 743 of 1050.\n",
      "Processing row 744 of 1050.\n",
      "Processing row 745 of 1050.\n",
      "Processing row 746 of 1050.\n",
      "Processing row 747 of 1050.\n",
      "Processing row 748 of 1050.\n",
      "Processing row 749 of 1050.\n",
      "Processing row 750 of 1050.\n",
      "Processing row 751 of 1050.\n",
      "Processing row 752 of 1050.\n",
      "Processing row 753 of 1050.\n",
      "Processing row 754 of 1050.\n",
      "Processing row 755 of 1050.\n",
      "Processing row 756 of 1050.\n",
      "Processing row 757 of 1050.\n",
      "Processing row 758 of 1050.\n",
      "Processing row 759 of 1050.\n",
      "Processing row 760 of 1050.\n",
      "Processing row 761 of 1050.\n",
      "Processing row 762 of 1050.\n",
      "Processing row 763 of 1050.\n",
      "Processing row 764 of 1050.\n",
      "Processing row 765 of 1050.\n",
      "Processing row 766 of 1050.\n",
      "Processing row 767 of 1050.\n",
      "Processing row 768 of 1050.\n",
      "Processing row 769 of 1050.\n",
      "Processing row 770 of 1050.\n",
      "Processing row 771 of 1050.\n",
      "Processing row 772 of 1050.\n",
      "Processing row 773 of 1050.\n",
      "Processing row 774 of 1050.\n",
      "Processing row 775 of 1050.\n",
      "Processing row 776 of 1050.\n",
      "Processing row 777 of 1050.\n",
      "Processing row 778 of 1050.\n",
      "Processing row 779 of 1050.\n",
      "Processing row 780 of 1050.\n",
      "Processing row 781 of 1050.\n",
      "Processing row 782 of 1050.\n",
      "Processing row 783 of 1050.\n",
      "Processing row 784 of 1050.\n",
      "Processing row 785 of 1050.\n",
      "Processing row 786 of 1050.\n",
      "Processing row 787 of 1050.\n",
      "Processing row 788 of 1050.\n",
      "Processing row 789 of 1050.\n",
      "Processing row 790 of 1050.\n",
      "Processing row 791 of 1050.\n",
      "Processing row 792 of 1050.\n",
      "Processing row 793 of 1050.\n",
      "Processing row 794 of 1050.\n",
      "Processing row 795 of 1050.\n",
      "Processing row 796 of 1050.\n",
      "Processing row 797 of 1050.\n",
      "Processing row 798 of 1050.\n",
      "Processing row 799 of 1050.\n",
      "Processing row 800 of 1050.\n",
      "Processing row 801 of 1050.\n",
      "Processing row 802 of 1050.\n",
      "Processing row 803 of 1050.\n",
      "Processing row 804 of 1050.\n",
      "Processing row 805 of 1050.\n",
      "Processing row 806 of 1050.\n",
      "Processing row 807 of 1050.\n",
      "Processing row 808 of 1050.\n",
      "Processing row 809 of 1050.\n",
      "Processing row 810 of 1050.\n",
      "Processing row 811 of 1050.\n",
      "Processing row 812 of 1050.\n",
      "Processing row 813 of 1050.\n",
      "Processing row 814 of 1050.\n",
      "Processing row 815 of 1050.\n",
      "Processing row 816 of 1050.\n",
      "Processing row 817 of 1050.\n",
      "Processing row 818 of 1050.\n",
      "Processing row 819 of 1050.\n",
      "Processing row 820 of 1050.\n",
      "Processing row 821 of 1050.\n",
      "Processing row 822 of 1050.\n",
      "Processing row 823 of 1050.\n",
      "Processing row 824 of 1050.\n",
      "Processing row 825 of 1050.\n",
      "Processing row 826 of 1050.\n",
      "Processing row 827 of 1050.\n",
      "Processing row 828 of 1050.\n",
      "Processing row 829 of 1050.\n",
      "Processing row 830 of 1050.\n",
      "Processing row 831 of 1050.\n",
      "Processing row 832 of 1050.\n",
      "Processing row 833 of 1050.\n",
      "Processing row 834 of 1050.\n",
      "Processing row 835 of 1050.\n",
      "Processing row 836 of 1050.\n",
      "Processing row 837 of 1050.\n",
      "Processing row 838 of 1050.\n",
      "Processing row 839 of 1050.\n",
      "Processing row 840 of 1050.\n",
      "Processing row 841 of 1050.\n",
      "Processing row 842 of 1050.\n",
      "Processing row 843 of 1050.\n",
      "Processing row 844 of 1050.\n",
      "Processing row 845 of 1050.\n",
      "Processing row 846 of 1050.\n",
      "Processing row 847 of 1050.\n",
      "Processing row 848 of 1050.\n",
      "Processing row 849 of 1050.\n",
      "Processing row 850 of 1050.\n",
      "Processing row 851 of 1050.\n",
      "Processing row 852 of 1050.\n",
      "Processing row 853 of 1050.\n",
      "Processing row 854 of 1050.\n",
      "Processing row 855 of 1050.\n",
      "Processing row 856 of 1050.\n",
      "Processing row 857 of 1050.\n",
      "Processing row 858 of 1050.\n",
      "Processing row 859 of 1050.\n",
      "Processing row 860 of 1050.\n",
      "Processing row 861 of 1050.\n",
      "Processing row 862 of 1050.\n",
      "Processing row 863 of 1050.\n",
      "Processing row 864 of 1050.\n",
      "Processing row 865 of 1050.\n",
      "Processing row 866 of 1050.\n",
      "Processing row 867 of 1050.\n",
      "Processing row 868 of 1050.\n",
      "Processing row 869 of 1050.\n",
      "Processing row 870 of 1050.\n",
      "Processing row 871 of 1050.\n",
      "Processing row 872 of 1050.\n",
      "Processing row 873 of 1050.\n",
      "Processing row 874 of 1050.\n",
      "Processing row 875 of 1050.\n",
      "Processing row 876 of 1050.\n",
      "Processing row 877 of 1050.\n",
      "Processing row 878 of 1050.\n",
      "Processing row 879 of 1050.\n",
      "Processing row 880 of 1050.\n",
      "Processing row 881 of 1050.\n",
      "Processing row 882 of 1050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 883 of 1050.\n",
      "Processing row 884 of 1050.\n",
      "Processing row 885 of 1050.\n",
      "Processing row 886 of 1050.\n",
      "Processing row 887 of 1050.\n",
      "Processing row 888 of 1050.\n",
      "Processing row 889 of 1050.\n",
      "Processing row 890 of 1050.\n",
      "Processing row 891 of 1050.\n",
      "Processing row 892 of 1050.\n",
      "Processing row 893 of 1050.\n",
      "Processing row 894 of 1050.\n",
      "Processing row 895 of 1050.\n",
      "Processing row 896 of 1050.\n",
      "Processing row 897 of 1050.\n",
      "Processing row 898 of 1050.\n",
      "Processing row 899 of 1050.\n",
      "Processing row 900 of 1050.\n",
      "Processing row 901 of 1050.\n",
      "Processing row 902 of 1050.\n",
      "Processing row 903 of 1050.\n",
      "Processing row 904 of 1050.\n",
      "Processing row 905 of 1050.\n",
      "Processing row 906 of 1050.\n",
      "Processing row 907 of 1050.\n",
      "Processing row 908 of 1050.\n",
      "Processing row 909 of 1050.\n",
      "Processing row 910 of 1050.\n",
      "Processing row 911 of 1050.\n",
      "Processing row 912 of 1050.\n",
      "Processing row 913 of 1050.\n",
      "Processing row 914 of 1050.\n",
      "Processing row 915 of 1050.\n",
      "Processing row 916 of 1050.\n",
      "Processing row 917 of 1050.\n",
      "Processing row 918 of 1050.\n",
      "Processing row 919 of 1050.\n",
      "Processing row 920 of 1050.\n",
      "Processing row 921 of 1050.\n",
      "Processing row 922 of 1050.\n",
      "Processing row 923 of 1050.\n",
      "Processing row 924 of 1050.\n",
      "Processing row 925 of 1050.\n",
      "Processing row 926 of 1050.\n",
      "Processing row 927 of 1050.\n",
      "Processing row 928 of 1050.\n",
      "Processing row 929 of 1050.\n",
      "Processing row 930 of 1050.\n",
      "Processing row 931 of 1050.\n",
      "Processing row 932 of 1050.\n",
      "Processing row 933 of 1050.\n",
      "Processing row 934 of 1050.\n",
      "Processing row 935 of 1050.\n",
      "Processing row 936 of 1050.\n",
      "Processing row 937 of 1050.\n",
      "Processing row 938 of 1050.\n",
      "Processing row 939 of 1050.\n",
      "Processing row 940 of 1050.\n",
      "Processing row 941 of 1050.\n",
      "Processing row 942 of 1050.\n",
      "Processing row 943 of 1050.\n",
      "Processing row 944 of 1050.\n",
      "Processing row 945 of 1050.\n",
      "Processing row 946 of 1050.\n",
      "Processing row 947 of 1050.\n",
      "Processing row 948 of 1050.\n",
      "Processing row 949 of 1050.\n",
      "Processing row 950 of 1050.\n",
      "Processing row 951 of 1050.\n",
      "Processing row 952 of 1050.\n",
      "Processing row 953 of 1050.\n",
      "Processing row 954 of 1050.\n",
      "Processing row 955 of 1050.\n",
      "Processing row 956 of 1050.\n",
      "Processing row 957 of 1050.\n",
      "Processing row 958 of 1050.\n",
      "Processing row 959 of 1050.\n",
      "Processing row 960 of 1050.\n",
      "Processing row 961 of 1050.\n",
      "Processing row 962 of 1050.\n",
      "Processing row 963 of 1050.\n",
      "Processing row 964 of 1050.\n",
      "Processing row 965 of 1050.\n",
      "Processing row 966 of 1050.\n",
      "Processing row 967 of 1050.\n",
      "Processing row 968 of 1050.\n",
      "Processing row 969 of 1050.\n",
      "Processing row 970 of 1050.\n",
      "Processing row 971 of 1050.\n",
      "Processing row 972 of 1050.\n",
      "Processing row 973 of 1050.\n",
      "Processing row 974 of 1050.\n",
      "Processing row 975 of 1050.\n",
      "Processing row 976 of 1050.\n",
      "Processing row 977 of 1050.\n",
      "Processing row 978 of 1050.\n",
      "Processing row 979 of 1050.\n",
      "Processing row 980 of 1050.\n",
      "Processing row 981 of 1050.\n",
      "Processing row 982 of 1050.\n",
      "Processing row 983 of 1050.\n",
      "Processing row 984 of 1050.\n",
      "Processing row 985 of 1050.\n",
      "Processing row 986 of 1050.\n",
      "Processing row 987 of 1050.\n",
      "Processing row 988 of 1050.\n",
      "Processing row 989 of 1050.\n",
      "Processing row 990 of 1050.\n",
      "Processing row 991 of 1050.\n",
      "Processing row 992 of 1050.\n",
      "Processing row 993 of 1050.\n",
      "Processing row 994 of 1050.\n",
      "Processing row 995 of 1050.\n",
      "Processing row 996 of 1050.\n",
      "Processing row 997 of 1050.\n",
      "Processing row 998 of 1050.\n",
      "Processing row 999 of 1050.\n",
      "Processing row 1000 of 1050.\n",
      "Processing row 1001 of 1050.\n",
      "Processing row 1002 of 1050.\n",
      "Processing row 1003 of 1050.\n",
      "Processing row 1004 of 1050.\n",
      "Processing row 1005 of 1050.\n",
      "Processing row 1006 of 1050.\n",
      "Processing row 1007 of 1050.\n",
      "Processing row 1008 of 1050.\n",
      "Processing row 1009 of 1050.\n",
      "Processing row 1010 of 1050.\n",
      "Processing row 1011 of 1050.\n",
      "Processing row 1012 of 1050.\n",
      "Processing row 1013 of 1050.\n",
      "Processing row 1014 of 1050.\n",
      "Processing row 1015 of 1050.\n",
      "Processing row 1016 of 1050.\n",
      "Processing row 1017 of 1050.\n",
      "Processing row 1018 of 1050.\n",
      "Processing row 1019 of 1050.\n",
      "Processing row 1020 of 1050.\n",
      "Processing row 1021 of 1050.\n",
      "Processing row 1022 of 1050.\n",
      "Processing row 1023 of 1050.\n",
      "Processing row 1024 of 1050.\n",
      "Processing row 1025 of 1050.\n",
      "Processing row 1026 of 1050.\n",
      "Processing row 1027 of 1050.\n",
      "Processing row 1028 of 1050.\n",
      "Processing row 1029 of 1050.\n",
      "Processing row 1030 of 1050.\n",
      "Processing row 1031 of 1050.\n",
      "Processing row 1032 of 1050.\n",
      "Processing row 1033 of 1050.\n",
      "Processing row 1034 of 1050.\n",
      "Processing row 1035 of 1050.\n",
      "Processing row 1036 of 1050.\n",
      "Processing row 1037 of 1050.\n",
      "Processing row 1038 of 1050.\n",
      "Processing row 1039 of 1050.\n",
      "Processing row 1040 of 1050.\n",
      "Processing row 1041 of 1050.\n",
      "Processing row 1042 of 1050.\n",
      "Processing row 1043 of 1050.\n",
      "Processing row 1044 of 1050.\n",
      "Processing row 1045 of 1050.\n",
      "Processing row 1046 of 1050.\n",
      "Processing row 1047 of 1050.\n",
      "Processing row 1048 of 1050.\n",
      "Processing row 1049 of 1050.\n",
      "Processing row 1050 of 1050.\n",
      "30872.925096035004\n"
     ]
    }
   ],
   "source": [
    "# Iterations. Update the similar score values to article_article\n",
    "\n",
    "# ATTENTION: THE ITERATION TAKES time, better to track progress\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Number of articles\n",
    "len_articles = article_article.shape[0]\n",
    "\n",
    "# List of index of articles\n",
    "idx_articles = list(article_article.index)\n",
    "\n",
    "\n",
    "# Loop thru each article: for each row, loop each columns. \n",
    "for i in idx_articles:\n",
    "    print(f\"Processing row {i} of {len_articles-1}.\")\n",
    "    \n",
    "    for j in idx_articles:\n",
    "        article_article.loc[i, j] = compute_article_similarity(i, j)\n",
    "        \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1041</th>\n",
       "      <th>1042</th>\n",
       "      <th>1043</th>\n",
       "      <th>1044</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032471</td>\n",
       "      <td>0.072460</td>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.259333</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>0.046419</td>\n",
       "      <td>0.047824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063201</td>\n",
       "      <td>0.051671</td>\n",
       "      <td>0.143709</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.032471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186967</td>\n",
       "      <td>0.028671</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>0.098205</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>0.155317</td>\n",
       "      <td>0.206362</td>\n",
       "      <td>0.103742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104562</td>\n",
       "      <td>0.152361</td>\n",
       "      <td>0.093032</td>\n",
       "      <td>0.030101</td>\n",
       "      <td>0.082490</td>\n",
       "      <td>0.014128</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>0.090637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072460</td>\n",
       "      <td>0.186967</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053018</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>0.041646</td>\n",
       "      <td>0.204603</td>\n",
       "      <td>0.339824</td>\n",
       "      <td>0.153497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149979</td>\n",
       "      <td>0.208527</td>\n",
       "      <td>0.139453</td>\n",
       "      <td>0.036274</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.104403</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.160396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.028671</td>\n",
       "      <td>0.053018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>0.037732</td>\n",
       "      <td>0.084109</td>\n",
       "      <td>0.048761</td>\n",
       "      <td>0.031276</td>\n",
       "      <td>0.049991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.033499</td>\n",
       "      <td>0.010938</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.036531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.259333</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095990</td>\n",
       "      <td>0.026990</td>\n",
       "      <td>0.154261</td>\n",
       "      <td>0.135846</td>\n",
       "      <td>0.152328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122135</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.137278</td>\n",
       "      <td>0.024670</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.072108</td>\n",
       "      <td>0.138237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.014128</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>0.009555</td>\n",
       "      <td>0.018015</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.010529</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.006396</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.017164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.104403</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.026182</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>0.069507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074214</td>\n",
       "      <td>0.024484</td>\n",
       "      <td>0.034961</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>0.015387</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.072108</td>\n",
       "      <td>0.010935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012977</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>0.066733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>0.069338</td>\n",
       "      <td>0.068079</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.078722</td>\n",
       "      <td>0.090637</td>\n",
       "      <td>0.160396</td>\n",
       "      <td>0.036531</td>\n",
       "      <td>0.138237</td>\n",
       "      <td>0.030091</td>\n",
       "      <td>0.038946</td>\n",
       "      <td>0.130746</td>\n",
       "      <td>0.084648</td>\n",
       "      <td>0.095793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188039</td>\n",
       "      <td>0.113751</td>\n",
       "      <td>0.140985</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>0.023651</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.114996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows Ã 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id      0         1         2         3         4         5     \\\n",
       "article_id                                                               \n",
       "0           1.000000  0.032471  0.072460  0.020439  0.259333  0.033410   \n",
       "1           0.032471  1.000000  0.186967  0.028671  0.093232  0.098205   \n",
       "2           0.072460  0.186967  1.000000  0.053018  0.172458  0.106455   \n",
       "3           0.020439  0.028671  0.053018  1.000000  0.027609  0.037732   \n",
       "4           0.259333  0.093232  0.172458  0.027609  1.000000  0.095990   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "1046        0.003492  0.014128  0.009416  0.012851  0.006955  0.019682   \n",
       "1047        0.004095  0.019350  0.013930  0.011922  0.003037  0.010529   \n",
       "1048        0.028436  0.056505  0.104403  0.010652  0.038724  0.026182   \n",
       "1049        0.000000  0.010927  0.018914  0.009425  0.072108  0.010935   \n",
       "1050        0.078722  0.090637  0.160396  0.036531  0.138237  0.030091   \n",
       "\n",
       "article_id      6         7         8         9     ...      1041      1042  \\\n",
       "article_id                                          ...                       \n",
       "0           0.016156  0.051355  0.046419  0.047824  ...  0.063201  0.051671   \n",
       "1           0.035012  0.155317  0.206362  0.103742  ...  0.104562  0.152361   \n",
       "2           0.041646  0.204603  0.339824  0.153497  ...  0.149979  0.208527   \n",
       "3           0.084109  0.048761  0.031276  0.049991  ...  0.027299  0.026927   \n",
       "4           0.026990  0.154261  0.135846  0.152328  ...  0.122135  0.141956   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "1046        0.009555  0.018015  0.004764  0.006668  ...  0.013567  0.003230   \n",
       "1047        0.002925  0.006396  0.007393  0.017164  ...  0.007295  0.002512   \n",
       "1048        0.017823  0.050228  0.041883  0.069507  ...  0.074214  0.024484   \n",
       "1049        0.000000  0.012977  0.014043  0.066733  ...  0.005548  0.069338   \n",
       "1050        0.038946  0.130746  0.084648  0.095793  ...  0.188039  0.113751   \n",
       "\n",
       "article_id      1043      1044      1045      1046      1047      1048  \\\n",
       "article_id                                                               \n",
       "0           0.143709  0.020163  0.010646  0.003492  0.004095  0.028436   \n",
       "1           0.093032  0.030101  0.082490  0.014128  0.019350  0.056505   \n",
       "2           0.139453  0.036274  0.009098  0.009416  0.013930  0.104403   \n",
       "3           0.033499  0.010938  0.036107  0.012851  0.011922  0.010652   \n",
       "4           0.137278  0.024670  0.007071  0.006955  0.003037  0.038724   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "1046        0.003622  0.014652  0.005776  1.000000  0.000000  0.001758   \n",
       "1047        0.005704  0.001240  0.006689  0.000000  1.000000  0.058308   \n",
       "1048        0.034961  0.020447  0.015387  0.001758  0.058308  1.000000   \n",
       "1049        0.068079  0.005126  0.000000  0.000000  0.000000  0.000000   \n",
       "1050        0.140985  0.025935  0.023651  0.005407  0.006413  0.114996   \n",
       "\n",
       "article_id      1049      1050  \n",
       "article_id                      \n",
       "0           0.000000  0.078722  \n",
       "1           0.010927  0.090637  \n",
       "2           0.018914  0.160396  \n",
       "3           0.009425  0.036531  \n",
       "4           0.072108  0.138237  \n",
       "...              ...       ...  \n",
       "1046        0.000000  0.005407  \n",
       "1047        0.000000  0.006413  \n",
       "1048        0.000000  0.114996  \n",
       "1049        1.000000  0.000000  \n",
       "1050        0.000000  1.000000  \n",
       "\n",
       "[1051 rows x 1051 columns]"
      ]
     },
     "execution_count": 1694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check updated article_article with calculated results. \n",
    "\n",
    "article_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to pickle\n",
    "\n",
    "article_article.to_pickle('article_article_similarity_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1427 in article_article.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of finding in real time. Use a query method from pre-calculated \n",
    "# article_article df. Save time.\n",
    "\n",
    "def loopup_similar_articles(article_id, data=article_article, n=20):\n",
    "    \n",
    "    # input: n - number of top similar to return\n",
    "    \n",
    "    if article_id in article_article.index.values:\n",
    "    \n",
    "        ids = list(data.loc[article_id][data.loc[article_id].index != article_id].sort_values(ascending=False).head(n).index)\n",
    "        names = list(df_nlp.loc[ids].doc_full_name.values)\n",
    "        \n",
    "    else:\n",
    "        # print(f\"[article_id] {article_id} not found in df_content\")\n",
    "        ids = []\n",
    "        names = []\n",
    "    \n",
    "    \n",
    "    return ids, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([800,\n",
       "  1035,\n",
       "  313,\n",
       "  805,\n",
       "  444,\n",
       "  721,\n",
       "  260,\n",
       "  967,\n",
       "  122,\n",
       "  96,\n",
       "  384,\n",
       "  809,\n",
       "  124,\n",
       "  723,\n",
       "  234,\n",
       "  892,\n",
       "  54,\n",
       "  253,\n",
       "  812,\n",
       "  861,\n",
       "  412,\n",
       "  732,\n",
       "  871,\n",
       "  74,\n",
       "  221,\n",
       "  89,\n",
       "  479,\n",
       "  616,\n",
       "  500,\n",
       "  567],\n",
       " ['Machine Learning for the Enterprise',\n",
       "  'Machine Learning for the Enterprise.',\n",
       "  'What is machine learning?',\n",
       "  'Machine Learning for everyone',\n",
       "  'Declarative Machine Learning',\n",
       "  'The power of machine learning in Spark',\n",
       "  'The Machine Learning Database',\n",
       "  'ML Algorithm != Learning Machine',\n",
       "  'Watson Machine Learning for Developers',\n",
       "  'Improving quality of life with Spark-empowered machine learning',\n",
       "  'Continuous Learning on Watson',\n",
       "  'Use the Machine Learning Library',\n",
       "  'Python Machine Learning: Scikit-Learn Tutorial',\n",
       "  '10 Essential Algorithms For Machine Learning Engineers',\n",
       "  '3 Scenarios for Machine Learning on Multicloud',\n",
       "  'Breaking the 80/20 rule: How data catalogs transform data scientistsâ productivity',\n",
       "  '8 ways to turn data into value with Apache Spark machine learning',\n",
       "  'Lifelong (machine) learning: how automation can help your models get smarter over time',\n",
       "  'Machine Learning Exercises In Python, Part 1',\n",
       "  'Cleaning the swamp: Turn your data lake into a source of crystal-clear insight',\n",
       "  'Adoption of machine learning to software failure prediction',\n",
       "  'Rapidly build Machine Learning flows with DSX',\n",
       "  'Overfitting in Machine Learning: What It Is and How to Prevent It',\n",
       "  'The 3 Kinds of Context: Machine Learning and the Art of the Frame',\n",
       "  'How smart catalogs can turn the big data flood into an ocean of opportunity',\n",
       "  'Top 20 R Machine Learning and Data Science packages',\n",
       "  'Drowning in data sources: How data cataloging could fix your findability problems',\n",
       "  'Three reasons machine learning models go out of sync',\n",
       "  'The Difference Between AI, Machine Learning, and Deep Learning?',\n",
       "  'You could be looking at it all wrong'])"
      ]
     },
     "execution_count": 1785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test result: look up most relavant articles for 455th article\n",
    "\n",
    "relavant_for_455th_article = loopup_similar_articles(455, n=30)\n",
    "relavant_for_455th_article\n",
    "\n",
    "# Revealing that they are about on 'machine learning'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on datasets inconsistency: \n",
    "\n",
    "Since that orginal df_content / df_nlp (cleaned version) only contain article from \\[0 - 1050\\].\n",
    "\n",
    "However, df (the interaction) dataset showing more article ids that beyone 1050. e,g 1429.\n",
    "\n",
    "So these two dataset's article are not up to dated.  \n",
    "The given df_content is 'late' (not catch up), which means: **articles that are in df (interaction) dataset might \n",
    "not be found in df_content dataset**. \n",
    "\n",
    "Therefore, due to this inconsistency, we will not be able to find content details for some articles mentioned in df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ideas to explore: \n",
    "- Based on the relavant article for a given article, can you get common words on them. Extract topic perhaps\n",
    "- What are the top contents? (that also in df_content/df_nlp)\n",
    "- What are the common words / terms of those top content? word bag counter\n",
    "- Can these words/terms represent trending topic?\n",
    "- Make a list of most trending topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics from relavant content.\n",
    "# Try extract common terms represented in top relavant articles. \n",
    "# e.g the relavant results showing they all contain 'machine learning'. \n",
    "# Approach with intersection of tokens for those relavant documents.\n",
    "# bag of words, 1-gram, 2-gram, 3-gram? counter? then intersections \n",
    "\n",
    "def extract_topics_from_relavant_articles(ids, data=df_nlp):\n",
    "    \n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " 'enterprise',\n",
       " 'last',\n",
       " 'blog',\n",
       " 'business',\n",
       " 'differentiation',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'introduced',\n",
       " 'described',\n",
       " 'concept',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'traced',\n",
       " 'origin',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'project',\n",
       " 'watson',\n",
       " 'show',\n",
       " 'skip',\n",
       " 'contentdinesh',\n",
       " 'nirmal',\n",
       " 'blog',\n",
       " 'blog',\n",
       " 'big',\n",
       " 'data',\n",
       " 'analytics',\n",
       " 'digital',\n",
       " 'technology',\n",
       " 'impacting',\n",
       " 'business',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'economy',\n",
       " 'menu',\n",
       " 'welcome',\n",
       " 'twitter',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'enterprise',\n",
       " 'last',\n",
       " 'blog',\n",
       " 'business',\n",
       " 'differentiation',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'introduced',\n",
       " 'described',\n",
       " 'concept',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'traced',\n",
       " 'origin',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'project',\n",
       " 'watson',\n",
       " 'showcasing',\n",
       " 'winning',\n",
       " 'jeopardy',\n",
       " 'tv',\n",
       " 'quiz',\n",
       " 'show',\n",
       " 'real',\n",
       " 'world',\n",
       " 'use',\n",
       " 'across',\n",
       " 'numerous',\n",
       " 'industry',\n",
       " 'including',\n",
       " 'health',\n",
       " 'care',\n",
       " 'concluded',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'potential',\n",
       " 'help',\n",
       " 'make',\n",
       " 'world',\n",
       " 'better',\n",
       " 'safer',\n",
       " 'place',\n",
       " 'however',\n",
       " 'yep',\n",
       " 'always',\n",
       " 'however',\n",
       " 'order',\n",
       " 'become',\n",
       " 'reality',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'form',\n",
       " 'enterprise',\n",
       " 'ready',\n",
       " 'enterprise',\n",
       " 'requirement',\n",
       " 'use',\n",
       " 'word',\n",
       " 'enterprise',\n",
       " 'mean',\n",
       " 'organization',\n",
       " 'business',\n",
       " 'critical',\n",
       " 'requirement',\n",
       " 'organization',\n",
       " 'size',\n",
       " 'volume',\n",
       " 'transaction',\n",
       " 'data',\n",
       " 'velocity',\n",
       " 'interaction',\n",
       " 'variety',\n",
       " 'data',\n",
       " 'yes',\n",
       " 'v',\n",
       " 'big',\n",
       " 'data',\n",
       " 'key',\n",
       " 'factor',\n",
       " 'might',\n",
       " 'impact',\n",
       " 'organization',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'requirement',\n",
       " 'also',\n",
       " 'collaboration',\n",
       " 'across',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'engineer',\n",
       " 'developer',\n",
       " 'creating',\n",
       " 'testing',\n",
       " 'training',\n",
       " 'deploying',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'level',\n",
       " 'different',\n",
       " 'audience',\n",
       " 'want',\n",
       " 'exposed',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'let',\n",
       " 'look',\n",
       " 'factor',\n",
       " 'make',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ibm',\n",
       " 'truly',\n",
       " 'enterprise',\n",
       " 'ready',\n",
       " 'collaboration',\n",
       " 'large',\n",
       " 'enterprise',\n",
       " 'tend',\n",
       " 'significant',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'team',\n",
       " 'often',\n",
       " 'multiple',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'engaged',\n",
       " 'single',\n",
       " 'project',\n",
       " 'collaboration',\n",
       " 'across',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'maybe',\n",
       " 'persona',\n",
       " 'required',\n",
       " 'maximize',\n",
       " 'productivity',\n",
       " 'agility',\n",
       " 'effectiveness',\n",
       " 'today',\n",
       " 'part',\n",
       " 'ibm',\n",
       " 'data',\n",
       " 'science',\n",
       " 'experience',\n",
       " 'bring',\n",
       " 'concept',\n",
       " 'project',\n",
       " 'wherein',\n",
       " 'various',\n",
       " 'persona',\n",
       " 'user',\n",
       " 'safely',\n",
       " 'collaborate',\n",
       " 'project',\n",
       " 'build',\n",
       " 'test',\n",
       " 'use',\n",
       " 'deploy',\n",
       " 'many',\n",
       " 'artefact',\n",
       " 'group',\n",
       " 'people',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'technology',\n",
       " 'adopt',\n",
       " 'concept',\n",
       " 'able',\n",
       " 'share',\n",
       " 'analytic',\n",
       " 'artefact',\n",
       " 'notebook',\n",
       " 'pipeline',\n",
       " 'model',\n",
       " 'etc',\n",
       " 'example',\n",
       " 'group',\n",
       " 'people',\n",
       " 'collaborate',\n",
       " 'single',\n",
       " 'notebook',\n",
       " 'wherein',\n",
       " 'one',\n",
       " 'person',\n",
       " 'curation',\n",
       " 'transformation',\n",
       " 'hand',\n",
       " 'another',\n",
       " 'person',\n",
       " 'creating',\n",
       " 'algorithm',\n",
       " 'testing',\n",
       " 'training',\n",
       " 'model',\n",
       " 'team',\n",
       " 'member',\n",
       " 'evaluate',\n",
       " 'model',\n",
       " 'deploy',\n",
       " 'individual',\n",
       " 'user',\n",
       " 'authenticated',\n",
       " 'separately',\n",
       " 'authorized',\n",
       " 'part',\n",
       " 'role',\n",
       " 'defined',\n",
       " 'project',\n",
       " 'limiting',\n",
       " 'granting',\n",
       " 'access',\n",
       " 'part',\n",
       " 'overall',\n",
       " 'process',\n",
       " 'experience',\n",
       " 'accordingly',\n",
       " 'consumption',\n",
       " 'everyone',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'want',\n",
       " 'need',\n",
       " 'know',\n",
       " 'model',\n",
       " 'design',\n",
       " 'statistical',\n",
       " 'theory',\n",
       " 'training',\n",
       " 'model',\n",
       " 'developer',\n",
       " 'example',\n",
       " 'may',\n",
       " 'varying',\n",
       " 'level',\n",
       " 'need',\n",
       " 'may',\n",
       " 'want',\n",
       " 'able',\n",
       " 'use',\n",
       " 'known',\n",
       " 'model',\n",
       " 'work',\n",
       " 'well',\n",
       " 'deploy',\n",
       " 'app',\n",
       " 'figure',\n",
       " '1',\n",
       " 'show',\n",
       " 'ibm',\n",
       " 'designed',\n",
       " 'work',\n",
       " 'space',\n",
       " 'allows',\n",
       " 'application',\n",
       " 'developer',\n",
       " 'choose',\n",
       " 'deploy',\n",
       " 'model',\n",
       " 'actually',\n",
       " 'create',\n",
       " 'pipeline',\n",
       " 'via',\n",
       " 'step',\n",
       " 'step',\n",
       " 'process',\n",
       " 'take',\n",
       " 'one',\n",
       " 'step',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'developer',\n",
       " 'may',\n",
       " 'want',\n",
       " 'choose',\n",
       " 'collection',\n",
       " 'pre',\n",
       " 'packaged',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'fraud',\n",
       " 'detection',\n",
       " 'weather',\n",
       " 'prediction',\n",
       " 'manufacturing',\n",
       " 'model',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'emotional',\n",
       " 'analysis',\n",
       " 'ibm',\n",
       " 'provides',\n",
       " 'today',\n",
       " 'bluemix',\n",
       " 'service',\n",
       " 'integrated',\n",
       " 'part',\n",
       " 'data',\n",
       " 'science',\n",
       " 'experience',\n",
       " 'figure',\n",
       " '1',\n",
       " 'integrated',\n",
       " 'workspace',\n",
       " 'creating',\n",
       " 'pipeline',\n",
       " 'commoditizing',\n",
       " 'automating',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'enterprise',\n",
       " 'environment',\n",
       " 'challenging',\n",
       " 'start',\n",
       " 'assumption',\n",
       " 'model',\n",
       " 'becomes',\n",
       " 'stale',\n",
       " 'minute',\n",
       " 'stop',\n",
       " 'training',\n",
       " 'time',\n",
       " 'accuracy',\n",
       " 'model',\n",
       " 'worsen',\n",
       " 'take',\n",
       " 'significant',\n",
       " 'time',\n",
       " 'understand',\n",
       " 'happening',\n",
       " 'retrain',\n",
       " 'existing',\n",
       " 'model',\n",
       " 'deploy',\n",
       " 'new',\n",
       " 'version',\n",
       " 'come',\n",
       " 'revenue',\n",
       " 'enterprise',\n",
       " 'may',\n",
       " 'hard',\n",
       " 'time',\n",
       " 'adopting',\n",
       " 'necessary',\n",
       " 'discipline',\n",
       " 'gauge',\n",
       " 'impact',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'lot',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'use',\n",
       " 'case',\n",
       " 'might',\n",
       " 'intuitive',\n",
       " 'sense',\n",
       " 'set',\n",
       " 'clear',\n",
       " 'control',\n",
       " 'point',\n",
       " 'flow',\n",
       " 'people',\n",
       " 'logically',\n",
       " 'relate',\n",
       " 'term',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'may',\n",
       " 'send',\n",
       " 'le',\n",
       " 'scientific',\n",
       " 'people',\n",
       " 'community',\n",
       " 'running',\n",
       " 'opposite',\n",
       " 'direction',\n",
       " 'often',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'perform',\n",
       " 'number',\n",
       " 'tedious',\n",
       " 'time',\n",
       " 'consuming',\n",
       " 'step',\n",
       " 'derive',\n",
       " 'insight',\n",
       " 'raw',\n",
       " 'data',\n",
       " 'set',\n",
       " 'process',\n",
       " 'involve',\n",
       " 'data',\n",
       " 'ingestion',\n",
       " 'cleaning',\n",
       " 'transformation',\n",
       " 'e',\n",
       " 'g',\n",
       " 'outlier',\n",
       " 'removal',\n",
       " 'missing',\n",
       " 'value',\n",
       " 'imputation',\n",
       " 'proceed',\n",
       " 'model',\n",
       " 'building',\n",
       " 'finally',\n",
       " 'presentation',\n",
       " 'prediction',\n",
       " 'align',\n",
       " 'end',\n",
       " 'user',\n",
       " 'objective',\n",
       " 'preference',\n",
       " 'long',\n",
       " 'complex',\n",
       " 'sometimes',\n",
       " 'artful',\n",
       " 'process',\n",
       " 'requiring',\n",
       " 'substantial',\n",
       " 'time',\n",
       " 'effort',\n",
       " 'especially',\n",
       " 'combinatorial',\n",
       " 'explosion',\n",
       " 'choice',\n",
       " 'algorithm',\n",
       " 'platform',\n",
       " 'parameter',\n",
       " 'composition',\n",
       " 'tool',\n",
       " 'help',\n",
       " 'automate',\n",
       " 'step',\n",
       " 'process',\n",
       " 'potential',\n",
       " 'accelerate',\n",
       " 'time',\n",
       " 'delivery',\n",
       " 'useful',\n",
       " 'result',\n",
       " 'expand',\n",
       " 'reach',\n",
       " 'data',\n",
       " 'science',\n",
       " 'non',\n",
       " 'expert',\n",
       " 'offer',\n",
       " 'systematic',\n",
       " 'exploration',\n",
       " 'available',\n",
       " 'option',\n",
       " 'cognitive',\n",
       " 'automation',\n",
       " 'data',\n",
       " 'science',\n",
       " 'cad',\n",
       " 'help',\n",
       " 'integrate',\n",
       " 'learning',\n",
       " 'planning',\n",
       " 'composition',\n",
       " 'orchestration',\n",
       " 'technique',\n",
       " 'automatically',\n",
       " 'efficiently',\n",
       " 'build',\n",
       " 'model',\n",
       " 'done',\n",
       " 'deploying',\n",
       " 'analytic',\n",
       " 'flow',\n",
       " 'interactively',\n",
       " 'support',\n",
       " 'would',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'task',\n",
       " 'cad',\n",
       " 'also',\n",
       " 'provides',\n",
       " 'capability',\n",
       " 'run',\n",
       " 'multiple',\n",
       " 'predefined',\n",
       " 'algorithm',\n",
       " 'parallel',\n",
       " 'identify',\n",
       " 'best',\n",
       " 'suitable',\n",
       " 'algorithm',\n",
       " 'particular',\n",
       " 'use',\n",
       " 'case',\n",
       " 'short',\n",
       " 'cad',\n",
       " 'selects',\n",
       " 'best',\n",
       " 'algorithm',\n",
       " 'given',\n",
       " 'use',\n",
       " 'case',\n",
       " 'click',\n",
       " 'read',\n",
       " 'ibm',\n",
       " 'paper',\n",
       " 'cad',\n",
       " 'training',\n",
       " 'tuning',\n",
       " 'model',\n",
       " 'optimization',\n",
       " 'time',\n",
       " 'model',\n",
       " 'algorithm',\n",
       " 'used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'becomes',\n",
       " 'good',\n",
       " 'true',\n",
       " 'training',\n",
       " 'data',\n",
       " 'model',\n",
       " 'predicts',\n",
       " 'training',\n",
       " 'data',\n",
       " 'well',\n",
       " 'performs',\n",
       " 'poorly',\n",
       " 'new',\n",
       " 'data',\n",
       " 'known',\n",
       " 'overfitting',\n",
       " 'extreme',\n",
       " 'case',\n",
       " 'occur',\n",
       " 'rote',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'achieves',\n",
       " '100',\n",
       " 'performance',\n",
       " 'data',\n",
       " 'already',\n",
       " 'seen',\n",
       " 'probably',\n",
       " 'better',\n",
       " 'random',\n",
       " 'guess',\n",
       " 'new',\n",
       " 'data',\n",
       " 'imagine',\n",
       " 'would',\n",
       " 'business',\n",
       " 'could',\n",
       " 'ruin',\n",
       " 'upset',\n",
       " 'customer',\n",
       " 'set',\n",
       " 'wrong',\n",
       " 'price',\n",
       " 'point',\n",
       " 'miss',\n",
       " 'many',\n",
       " 'business',\n",
       " 'opportunity',\n",
       " 'ibm',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'help',\n",
       " 'counter',\n",
       " 'clean',\n",
       " 'separation',\n",
       " 'training',\n",
       " 'data',\n",
       " 'holdout',\n",
       " 'data',\n",
       " 'used',\n",
       " 'evaluate',\n",
       " 'model',\n",
       " 'performance',\n",
       " 'well',\n",
       " 'careful',\n",
       " 'use',\n",
       " 'cross',\n",
       " 'validation',\n",
       " 'technique',\n",
       " 'data',\n",
       " 'sovereignty',\n",
       " 'isolation',\n",
       " 'enterprise',\n",
       " 'organization',\n",
       " 'fear',\n",
       " 'psychological',\n",
       " 'otherwise',\n",
       " 'come',\n",
       " 'putting',\n",
       " 'data',\n",
       " 'application',\n",
       " 'hardware',\n",
       " 'storage',\n",
       " 'network',\n",
       " 'infrastructure',\n",
       " 'shared',\n",
       " 'organization',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " 'first',\n",
       " 'strategy',\n",
       " 'provides',\n",
       " 'necessary',\n",
       " 'sovereignty',\n",
       " 'multi',\n",
       " 'tenancy',\n",
       " 'isolation',\n",
       " 'help',\n",
       " 'ensure',\n",
       " 'data',\n",
       " 'application',\n",
       " 'managed',\n",
       " 'privately',\n",
       " 'across',\n",
       " 'ibm',\n",
       " 'world',\n",
       " 'wide',\n",
       " 'data',\n",
       " 'center',\n",
       " 'variety',\n",
       " 'type',\n",
       " 'data',\n",
       " 'used',\n",
       " 'time',\n",
       " 'data',\n",
       " 'simpler',\n",
       " 'structured',\n",
       " 'relational',\n",
       " 'hierarchical',\n",
       " 'data',\n",
       " 'stored',\n",
       " 'database',\n",
       " 'big',\n",
       " 'data',\n",
       " 'simply',\n",
       " 'mean',\n",
       " 'data',\n",
       " 'includes',\n",
       " 'volume',\n",
       " 'raw',\n",
       " 'content',\n",
       " 'structured',\n",
       " 'form',\n",
       " 'part',\n",
       " 'unstructured',\n",
       " 'add',\n",
       " 'internet',\n",
       " 'thing',\n",
       " 'sending',\n",
       " 'massive',\n",
       " 'amount',\n",
       " 'sensor',\n",
       " 'data',\n",
       " 'world',\n",
       " 'simple',\n",
       " 'ibm',\n",
       " 'data',\n",
       " 'strategy',\n",
       " 'make',\n",
       " 'data',\n",
       " 'simple',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'capability',\n",
       " 'leverage',\n",
       " 'strategy',\n",
       " 'able',\n",
       " 'process',\n",
       " 'structured',\n",
       " 'semi',\n",
       " 'structured',\n",
       " 'unstructured',\n",
       " 'data',\n",
       " 'set',\n",
       " 'using',\n",
       " 'many',\n",
       " 'connector',\n",
       " 'abstracting',\n",
       " 'complexity',\n",
       " 'exploiting',\n",
       " 'spark',\n",
       " 'r',\n",
       " 'python',\n",
       " 'runtimes',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ibm',\n",
       " 'provides',\n",
       " '20',\n",
       " 'different',\n",
       " 'data',\n",
       " 'source',\n",
       " 'connector',\n",
       " 'organization',\n",
       " 'ingest',\n",
       " 'data',\n",
       " 'large',\n",
       " 'compute',\n",
       " 'power',\n",
       " 'enterprise',\n",
       " 'need',\n",
       " 'high',\n",
       " 'compute',\n",
       " 'power',\n",
       " 'since',\n",
       " 'process',\n",
       " 'ever',\n",
       " 'increasing',\n",
       " 'work',\n",
       " 'load',\n",
       " 'data',\n",
       " 'transaction',\n",
       " 'process',\n",
       " 'since',\n",
       " 'spark',\n",
       " 'service',\n",
       " 'single',\n",
       " 'multi',\n",
       " 'tenant',\n",
       " 'cluster',\n",
       " 'resource',\n",
       " 'utilization',\n",
       " 'wasted',\n",
       " 'repurpose',\n",
       " 'computer',\n",
       " 'power',\n",
       " 'scaling',\n",
       " 'scaling',\n",
       " 'capability',\n",
       " 'built',\n",
       " 'part',\n",
       " 'service',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'able',\n",
       " 'transparently',\n",
       " 'enable',\n",
       " 'scale',\n",
       " 'scale',\n",
       " 'capability',\n",
       " 'information',\n",
       " 'governance',\n",
       " 'ibm',\n",
       " 'strong',\n",
       " 'heritage',\n",
       " 'information',\n",
       " 'governance',\n",
       " 'managing',\n",
       " 'data',\n",
       " 'lifecycle',\n",
       " 'cleansing',\n",
       " 'quality',\n",
       " 'data',\n",
       " 'data',\n",
       " 'wrangling',\n",
       " 'shaping',\n",
       " 'security',\n",
       " 'privacy',\n",
       " 'data',\n",
       " 'information',\n",
       " 'governance',\n",
       " 'catalogue',\n",
       " 'us',\n",
       " 'policy',\n",
       " 'help',\n",
       " 'ensure',\n",
       " 'right',\n",
       " 'people',\n",
       " 'see',\n",
       " 'access',\n",
       " 'execute',\n",
       " 'data',\n",
       " 'service',\n",
       " 'ibm',\n",
       " 'also',\n",
       " 'help',\n",
       " 'provide',\n",
       " 'real',\n",
       " 'time',\n",
       " 'monitoring',\n",
       " 'threat',\n",
       " 'detection',\n",
       " 'prevention',\n",
       " 'intervention',\n",
       " 'well',\n",
       " 'forensics',\n",
       " 'compliance',\n",
       " 'detailed',\n",
       " 'audit',\n",
       " 'obfuscation',\n",
       " 'masking',\n",
       " 'data',\n",
       " 'encryption',\n",
       " 'applied',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'training',\n",
       " 'data',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'one',\n",
       " 'trick',\n",
       " 'pony',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'mlaas',\n",
       " 'many',\n",
       " 'form',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'system',\n",
       " 'ml',\n",
       " 'ibm',\n",
       " 'donated',\n",
       " 'apache',\n",
       " 'foundation',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'vision',\n",
       " 'personality',\n",
       " 'emotional',\n",
       " 'insight',\n",
       " 'customer',\n",
       " 'sentiment',\n",
       " 'retrieve',\n",
       " 'rank',\n",
       " 'ibm',\n",
       " 'makeit',\n",
       " 'simple',\n",
       " 'enterprise',\n",
       " 'size',\n",
       " 'pick',\n",
       " 'choose',\n",
       " 'number',\n",
       " 'predefined',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'bluemix',\n",
       " 'tile',\n",
       " 'figure',\n",
       " '2',\n",
       " 'ran',\n",
       " 'earlier',\n",
       " 'blog',\n",
       " 'personality',\n",
       " 'analyser',\n",
       " 'knew',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'guy',\n",
       " 'reassuring',\n",
       " 'hear',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'believe',\n",
       " 'try',\n",
       " 'figure',\n",
       " '2',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'bluemix',\n",
       " 'service',\n",
       " 'using',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'reduce',\n",
       " 'cost',\n",
       " 'risk',\n",
       " 'many',\n",
       " 'customer',\n",
       " 'across',\n",
       " 'different',\n",
       " 'industry',\n",
       " 'used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'capability',\n",
       " 'help',\n",
       " 'reduce',\n",
       " 'cost',\n",
       " 'improve',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'reduce',\n",
       " 'risk',\n",
       " 'vermont',\n",
       " 'electrical',\n",
       " 'power',\n",
       " 'company',\n",
       " 'velco',\n",
       " 'worked',\n",
       " 'ibm',\n",
       " 'research',\n",
       " 'develop',\n",
       " 'integrated',\n",
       " 'weather',\n",
       " 'forecasting',\n",
       " 'system',\n",
       " 'help',\n",
       " 'deliver',\n",
       " 'reliable',\n",
       " 'clean',\n",
       " 'affordable',\n",
       " 'power',\n",
       " 'consumer',\n",
       " 'integrating',\n",
       " 'renewable',\n",
       " 'energy',\n",
       " 'grid',\n",
       " 'solution',\n",
       " 'combine',\n",
       " 'high',\n",
       " 'resolution',\n",
       " 'weather',\n",
       " 'multiple',\n",
       " 'forecasting',\n",
       " 'tool',\n",
       " 'based',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'trained',\n",
       " 'hindcasts',\n",
       " 'weather',\n",
       " 'correlated',\n",
       " 'historical',\n",
       " 'energy',\n",
       " 'production',\n",
       " 'historical',\n",
       " 'net',\n",
       " 'demand',\n",
       " 'result',\n",
       " 'precise',\n",
       " 'accurate',\n",
       " 'wind',\n",
       " 'solar',\n",
       " ...]"
      ]
     },
     "execution_count": 1808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_id = 455\n",
    "relevant_articles = loopup_similar_articles(target_id, n=30)\n",
    "\n",
    "r_ids = relevant_articles[0]\n",
    "r_ids\n",
    "\n",
    "# concat tokens: for an article, combine token of title, desc, body all together to represent its tokened text\n",
    "test_id = 800\n",
    "\n",
    "title = df_nlp.loc[test_id].doc_full_name\n",
    "desc = df_nlp.loc[test_id].doc_description\n",
    "body = df_nlp.loc[test_id].doc_body\n",
    "\n",
    "tokened_text = tokenize(title) + tokenize(desc) + tokenize(body)\n",
    "tokened_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1984,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning',\n",
       " 'learning enterprise',\n",
       " 'enterprise last',\n",
       " 'last blog',\n",
       " 'blog business',\n",
       " 'business differentiation',\n",
       " 'differentiation machine',\n",
       " 'machine learning',\n",
       " 'learning introduced',\n",
       " 'introduced described',\n",
       " 'described concept',\n",
       " 'concept machine',\n",
       " 'machine learning',\n",
       " 'learning traced',\n",
       " 'traced origin',\n",
       " 'origin computer',\n",
       " 'computer science',\n",
       " 'science project',\n",
       " 'project watson',\n",
       " 'watson show',\n",
       " 'show skip',\n",
       " 'skip contentdinesh',\n",
       " 'contentdinesh nirmal',\n",
       " 'nirmal blog',\n",
       " 'blog blog',\n",
       " 'blog big',\n",
       " 'big data',\n",
       " 'data analytics',\n",
       " 'analytics digital',\n",
       " 'digital technology',\n",
       " 'technology impacting',\n",
       " 'impacting business',\n",
       " 'business data',\n",
       " 'data driven',\n",
       " 'driven economy',\n",
       " 'economy menu',\n",
       " 'menu welcome',\n",
       " 'welcome twitter',\n",
       " 'twitter machine',\n",
       " 'machine learning',\n",
       " 'learning enterprise',\n",
       " 'enterprise last',\n",
       " 'last blog',\n",
       " 'blog business',\n",
       " 'business differentiation',\n",
       " 'differentiation machine',\n",
       " 'machine learning',\n",
       " 'learning introduced',\n",
       " 'introduced described',\n",
       " 'described concept',\n",
       " 'concept machine',\n",
       " 'machine learning',\n",
       " 'learning traced',\n",
       " 'traced origin',\n",
       " 'origin computer',\n",
       " 'computer science',\n",
       " 'science project',\n",
       " 'project watson',\n",
       " 'watson showcasing',\n",
       " 'showcasing winning',\n",
       " 'winning jeopardy',\n",
       " 'jeopardy tv',\n",
       " 'tv quiz',\n",
       " 'quiz show',\n",
       " 'show real',\n",
       " 'real world',\n",
       " 'world use',\n",
       " 'use across',\n",
       " 'across numerous',\n",
       " 'numerous industry',\n",
       " 'industry including',\n",
       " 'including health',\n",
       " 'health care',\n",
       " 'care concluded',\n",
       " 'concluded machine',\n",
       " 'machine learning',\n",
       " 'learning potential',\n",
       " 'potential help',\n",
       " 'help make',\n",
       " 'make world',\n",
       " 'world better',\n",
       " 'better safer',\n",
       " 'safer place',\n",
       " 'place however',\n",
       " 'however yep',\n",
       " 'yep always',\n",
       " 'always however',\n",
       " 'however order',\n",
       " 'order become',\n",
       " 'become reality',\n",
       " 'reality machine',\n",
       " 'machine learning',\n",
       " 'learning form',\n",
       " 'form enterprise',\n",
       " 'enterprise ready',\n",
       " 'ready enterprise',\n",
       " 'enterprise requirement',\n",
       " 'requirement use',\n",
       " 'use word',\n",
       " 'word enterprise',\n",
       " 'enterprise mean',\n",
       " 'mean organization',\n",
       " 'organization business',\n",
       " 'business critical',\n",
       " 'critical requirement',\n",
       " 'requirement organization',\n",
       " 'organization size',\n",
       " 'size volume',\n",
       " 'volume transaction',\n",
       " 'transaction data',\n",
       " 'data velocity',\n",
       " 'velocity interaction',\n",
       " 'interaction variety',\n",
       " 'variety data',\n",
       " 'data yes',\n",
       " 'yes v',\n",
       " 'v big',\n",
       " 'big data',\n",
       " 'data key',\n",
       " 'key factor',\n",
       " 'factor might',\n",
       " 'might impact',\n",
       " 'impact organization',\n",
       " 'organization machine',\n",
       " 'machine learning',\n",
       " 'learning requirement',\n",
       " 'requirement also',\n",
       " 'also collaboration',\n",
       " 'collaboration across',\n",
       " 'across data',\n",
       " 'data scientist',\n",
       " 'scientist engineer',\n",
       " 'engineer developer',\n",
       " 'developer creating',\n",
       " 'creating testing',\n",
       " 'testing training',\n",
       " 'training deploying',\n",
       " 'deploying machine',\n",
       " 'machine learning',\n",
       " 'learning model',\n",
       " 'model level',\n",
       " 'level different',\n",
       " 'different audience',\n",
       " 'audience want',\n",
       " 'want exposed',\n",
       " 'exposed machine',\n",
       " 'machine learning',\n",
       " 'learning let',\n",
       " 'let look',\n",
       " 'look factor',\n",
       " 'factor make',\n",
       " 'make machine',\n",
       " 'machine learning',\n",
       " 'learning ibm',\n",
       " 'ibm truly',\n",
       " 'truly enterprise',\n",
       " 'enterprise ready',\n",
       " 'ready collaboration',\n",
       " 'collaboration large',\n",
       " 'large enterprise',\n",
       " 'enterprise tend',\n",
       " 'tend significant',\n",
       " 'significant data',\n",
       " 'data scientist',\n",
       " 'scientist team',\n",
       " 'team often',\n",
       " 'often multiple',\n",
       " 'multiple data',\n",
       " 'data scientist',\n",
       " 'scientist engaged',\n",
       " 'engaged single',\n",
       " 'single project',\n",
       " 'project collaboration',\n",
       " 'collaboration across',\n",
       " 'across data',\n",
       " 'data scientist',\n",
       " 'scientist maybe',\n",
       " 'maybe persona',\n",
       " 'persona required',\n",
       " 'required maximize',\n",
       " 'maximize productivity',\n",
       " 'productivity agility',\n",
       " 'agility effectiveness',\n",
       " 'effectiveness today',\n",
       " 'today part',\n",
       " 'part ibm',\n",
       " 'ibm data',\n",
       " 'data science',\n",
       " 'science experience',\n",
       " 'experience bring',\n",
       " 'bring concept',\n",
       " 'concept project',\n",
       " 'project wherein',\n",
       " 'wherein various',\n",
       " 'various persona',\n",
       " 'persona user',\n",
       " 'user safely',\n",
       " 'safely collaborate',\n",
       " 'collaborate project',\n",
       " 'project build',\n",
       " 'build test',\n",
       " 'test use',\n",
       " 'use deploy',\n",
       " 'deploy many',\n",
       " 'many artefact',\n",
       " 'artefact group',\n",
       " 'group people',\n",
       " 'people machine',\n",
       " 'machine learning',\n",
       " 'learning technology',\n",
       " 'technology adopt',\n",
       " 'adopt concept',\n",
       " 'concept able',\n",
       " 'able share',\n",
       " 'share analytic',\n",
       " 'analytic artefact',\n",
       " 'artefact notebook',\n",
       " 'notebook pipeline',\n",
       " 'pipeline model',\n",
       " 'model etc',\n",
       " 'etc example',\n",
       " 'example group',\n",
       " 'group people',\n",
       " 'people collaborate',\n",
       " 'collaborate single',\n",
       " 'single notebook',\n",
       " 'notebook wherein',\n",
       " 'wherein one',\n",
       " 'one person',\n",
       " 'person curation',\n",
       " 'curation transformation',\n",
       " 'transformation hand',\n",
       " 'hand another',\n",
       " 'another person',\n",
       " 'person creating',\n",
       " 'creating algorithm',\n",
       " 'algorithm testing',\n",
       " 'testing training',\n",
       " 'training model',\n",
       " 'model team',\n",
       " 'team member',\n",
       " 'member evaluate',\n",
       " 'evaluate model',\n",
       " 'model deploy',\n",
       " 'deploy individual',\n",
       " 'individual user',\n",
       " 'user authenticated',\n",
       " 'authenticated separately',\n",
       " 'separately authorized',\n",
       " 'authorized part',\n",
       " 'part role',\n",
       " 'role defined',\n",
       " 'defined project',\n",
       " 'project limiting',\n",
       " 'limiting granting',\n",
       " 'granting access',\n",
       " 'access part',\n",
       " 'part overall',\n",
       " 'overall process',\n",
       " 'process experience',\n",
       " 'experience accordingly',\n",
       " 'accordingly consumption',\n",
       " 'consumption everyone',\n",
       " 'everyone data',\n",
       " 'data scientist',\n",
       " 'scientist want',\n",
       " 'want need',\n",
       " 'need know',\n",
       " 'know model',\n",
       " 'model design',\n",
       " 'design statistical',\n",
       " 'statistical theory',\n",
       " 'theory training',\n",
       " 'training model',\n",
       " 'model developer',\n",
       " 'developer example',\n",
       " 'example may',\n",
       " 'may varying',\n",
       " 'varying level',\n",
       " 'level need',\n",
       " 'need may',\n",
       " 'may want',\n",
       " 'want able',\n",
       " 'able use',\n",
       " 'use known',\n",
       " 'known model',\n",
       " 'model work',\n",
       " 'work well',\n",
       " 'well deploy',\n",
       " 'deploy app',\n",
       " 'app figure',\n",
       " 'figure 1',\n",
       " '1 show',\n",
       " 'show ibm',\n",
       " 'ibm designed',\n",
       " 'designed work',\n",
       " 'work space',\n",
       " 'space allows',\n",
       " 'allows application',\n",
       " 'application developer',\n",
       " 'developer choose',\n",
       " 'choose deploy',\n",
       " 'deploy model',\n",
       " 'model actually',\n",
       " 'actually create',\n",
       " 'create pipeline',\n",
       " 'pipeline via',\n",
       " 'via step',\n",
       " 'step step',\n",
       " 'step process',\n",
       " 'process take',\n",
       " 'take one',\n",
       " 'one step',\n",
       " 'step higher',\n",
       " 'higher level',\n",
       " 'level developer',\n",
       " 'developer may',\n",
       " 'may want',\n",
       " 'want choose',\n",
       " 'choose collection',\n",
       " 'collection pre',\n",
       " 'pre packaged',\n",
       " 'packaged machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service fraud',\n",
       " 'fraud detection',\n",
       " 'detection weather',\n",
       " 'weather prediction',\n",
       " 'prediction manufacturing',\n",
       " 'manufacturing model',\n",
       " 'model sentiment',\n",
       " 'sentiment analysis',\n",
       " 'analysis emotional',\n",
       " 'emotional analysis',\n",
       " 'analysis ibm',\n",
       " 'ibm provides',\n",
       " 'provides today',\n",
       " 'today bluemix',\n",
       " 'bluemix service',\n",
       " 'service integrated',\n",
       " 'integrated part',\n",
       " 'part data',\n",
       " 'data science',\n",
       " 'science experience',\n",
       " 'experience figure',\n",
       " 'figure 1',\n",
       " '1 integrated',\n",
       " 'integrated workspace',\n",
       " 'workspace creating',\n",
       " 'creating pipeline',\n",
       " 'pipeline commoditizing',\n",
       " 'commoditizing automating',\n",
       " 'automating machine',\n",
       " 'machine learning',\n",
       " 'learning machine',\n",
       " 'machine learning',\n",
       " 'learning enterprise',\n",
       " 'enterprise environment',\n",
       " 'environment challenging',\n",
       " 'challenging start',\n",
       " 'start assumption',\n",
       " 'assumption model',\n",
       " 'model becomes',\n",
       " 'becomes stale',\n",
       " 'stale minute',\n",
       " 'minute stop',\n",
       " 'stop training',\n",
       " 'training time',\n",
       " 'time accuracy',\n",
       " 'accuracy model',\n",
       " 'model worsen',\n",
       " 'worsen take',\n",
       " 'take significant',\n",
       " 'significant time',\n",
       " 'time understand',\n",
       " 'understand happening',\n",
       " 'happening retrain',\n",
       " 'retrain existing',\n",
       " 'existing model',\n",
       " 'model deploy',\n",
       " 'deploy new',\n",
       " 'new version',\n",
       " 'version come',\n",
       " 'come revenue',\n",
       " 'revenue enterprise',\n",
       " 'enterprise may',\n",
       " 'may hard',\n",
       " 'hard time',\n",
       " 'time adopting',\n",
       " 'adopting necessary',\n",
       " 'necessary discipline',\n",
       " 'discipline gauge',\n",
       " 'gauge impact',\n",
       " 'impact bottom',\n",
       " 'bottom line',\n",
       " 'line lot',\n",
       " 'lot machine',\n",
       " 'machine learning',\n",
       " 'learning use',\n",
       " 'use case',\n",
       " 'case might',\n",
       " 'might intuitive',\n",
       " 'intuitive sense',\n",
       " 'sense set',\n",
       " 'set clear',\n",
       " 'clear control',\n",
       " 'control point',\n",
       " 'point flow',\n",
       " 'flow people',\n",
       " 'people logically',\n",
       " 'logically relate',\n",
       " 'relate term',\n",
       " 'term machine',\n",
       " 'machine learning',\n",
       " 'learning may',\n",
       " 'may send',\n",
       " 'send le',\n",
       " 'le scientific',\n",
       " 'scientific people',\n",
       " 'people community',\n",
       " 'community running',\n",
       " 'running opposite',\n",
       " 'opposite direction',\n",
       " 'direction often',\n",
       " 'often data',\n",
       " 'data scientist',\n",
       " 'scientist perform',\n",
       " 'perform number',\n",
       " 'number tedious',\n",
       " 'tedious time',\n",
       " 'time consuming',\n",
       " 'consuming step',\n",
       " 'step derive',\n",
       " 'derive insight',\n",
       " 'insight raw',\n",
       " 'raw data',\n",
       " 'data set',\n",
       " 'set process',\n",
       " 'process involve',\n",
       " 'involve data',\n",
       " 'data ingestion',\n",
       " 'ingestion cleaning',\n",
       " 'cleaning transformation',\n",
       " 'transformation e',\n",
       " 'e g',\n",
       " 'g outlier',\n",
       " 'outlier removal',\n",
       " 'removal missing',\n",
       " 'missing value',\n",
       " 'value imputation',\n",
       " 'imputation proceed',\n",
       " 'proceed model',\n",
       " 'model building',\n",
       " 'building finally',\n",
       " 'finally presentation',\n",
       " 'presentation prediction',\n",
       " 'prediction align',\n",
       " 'align end',\n",
       " 'end user',\n",
       " 'user objective',\n",
       " 'objective preference',\n",
       " 'preference long',\n",
       " 'long complex',\n",
       " 'complex sometimes',\n",
       " 'sometimes artful',\n",
       " 'artful process',\n",
       " 'process requiring',\n",
       " 'requiring substantial',\n",
       " 'substantial time',\n",
       " 'time effort',\n",
       " 'effort especially',\n",
       " 'especially combinatorial',\n",
       " 'combinatorial explosion',\n",
       " 'explosion choice',\n",
       " 'choice algorithm',\n",
       " 'algorithm platform',\n",
       " 'platform parameter',\n",
       " 'parameter composition',\n",
       " 'composition tool',\n",
       " 'tool help',\n",
       " 'help automate',\n",
       " 'automate step',\n",
       " 'step process',\n",
       " 'process potential',\n",
       " 'potential accelerate',\n",
       " 'accelerate time',\n",
       " 'time delivery',\n",
       " 'delivery useful',\n",
       " 'useful result',\n",
       " 'result expand',\n",
       " 'expand reach',\n",
       " 'reach data',\n",
       " 'data science',\n",
       " 'science non',\n",
       " 'non expert',\n",
       " 'expert offer',\n",
       " 'offer systematic',\n",
       " 'systematic exploration',\n",
       " 'exploration available',\n",
       " 'available option',\n",
       " 'option cognitive',\n",
       " 'cognitive automation',\n",
       " 'automation data',\n",
       " 'data science',\n",
       " 'science cad',\n",
       " 'cad help',\n",
       " 'help integrate',\n",
       " 'integrate learning',\n",
       " 'learning planning',\n",
       " 'planning composition',\n",
       " 'composition orchestration',\n",
       " 'orchestration technique',\n",
       " 'technique automatically',\n",
       " 'automatically efficiently',\n",
       " 'efficiently build',\n",
       " 'build model',\n",
       " 'model done',\n",
       " 'done deploying',\n",
       " 'deploying analytic',\n",
       " 'analytic flow',\n",
       " 'flow interactively',\n",
       " 'interactively support',\n",
       " 'support would',\n",
       " 'would data',\n",
       " 'data scientist',\n",
       " 'scientist task',\n",
       " 'task cad',\n",
       " 'cad also',\n",
       " 'also provides',\n",
       " 'provides capability',\n",
       " 'capability run',\n",
       " 'run multiple',\n",
       " 'multiple predefined',\n",
       " 'predefined algorithm',\n",
       " 'algorithm parallel',\n",
       " 'parallel identify',\n",
       " 'identify best',\n",
       " 'best suitable',\n",
       " 'suitable algorithm',\n",
       " 'algorithm particular',\n",
       " 'particular use',\n",
       " 'use case',\n",
       " 'case short',\n",
       " 'short cad',\n",
       " 'cad selects',\n",
       " 'selects best',\n",
       " 'best algorithm',\n",
       " 'algorithm given',\n",
       " 'given use',\n",
       " 'use case',\n",
       " 'case click',\n",
       " 'click read',\n",
       " 'read ibm',\n",
       " 'ibm paper',\n",
       " 'paper cad',\n",
       " 'cad training',\n",
       " 'training tuning',\n",
       " 'tuning model',\n",
       " 'model optimization',\n",
       " 'optimization time',\n",
       " 'time model',\n",
       " 'model algorithm',\n",
       " 'algorithm used',\n",
       " 'used machine',\n",
       " 'machine learning',\n",
       " 'learning becomes',\n",
       " 'becomes good',\n",
       " 'good true',\n",
       " 'true training',\n",
       " 'training data',\n",
       " 'data model',\n",
       " 'model predicts',\n",
       " 'predicts training',\n",
       " 'training data',\n",
       " 'data well',\n",
       " 'well performs',\n",
       " 'performs poorly',\n",
       " 'poorly new',\n",
       " 'new data',\n",
       " 'data known',\n",
       " 'known overfitting',\n",
       " 'overfitting extreme',\n",
       " 'extreme case',\n",
       " 'case occur',\n",
       " 'occur rote',\n",
       " 'rote learning',\n",
       " 'learning model',\n",
       " 'model achieves',\n",
       " 'achieves 100',\n",
       " '100 performance',\n",
       " 'performance data',\n",
       " 'data already',\n",
       " 'already seen',\n",
       " 'seen probably',\n",
       " 'probably better',\n",
       " 'better random',\n",
       " 'random guess',\n",
       " 'guess new',\n",
       " 'new data',\n",
       " 'data imagine',\n",
       " 'imagine would',\n",
       " 'would business',\n",
       " 'business could',\n",
       " 'could ruin',\n",
       " 'ruin upset',\n",
       " 'upset customer',\n",
       " 'customer set',\n",
       " 'set wrong',\n",
       " 'wrong price',\n",
       " 'price point',\n",
       " 'point miss',\n",
       " 'miss many',\n",
       " 'many business',\n",
       " 'business opportunity',\n",
       " 'opportunity ibm',\n",
       " 'ibm machine',\n",
       " 'machine learning',\n",
       " 'learning help',\n",
       " 'help counter',\n",
       " 'counter clean',\n",
       " 'clean separation',\n",
       " 'separation training',\n",
       " 'training data',\n",
       " 'data holdout',\n",
       " 'holdout data',\n",
       " 'data used',\n",
       " 'used evaluate',\n",
       " 'evaluate model',\n",
       " 'model performance',\n",
       " 'performance well',\n",
       " 'well careful',\n",
       " 'careful use',\n",
       " 'use cross',\n",
       " 'cross validation',\n",
       " 'validation technique',\n",
       " 'technique data',\n",
       " 'data sovereignty',\n",
       " 'sovereignty isolation',\n",
       " 'isolation enterprise',\n",
       " 'enterprise organization',\n",
       " 'organization fear',\n",
       " 'fear psychological',\n",
       " 'psychological otherwise',\n",
       " 'otherwise come',\n",
       " 'come putting',\n",
       " 'putting data',\n",
       " 'data application',\n",
       " 'application hardware',\n",
       " 'hardware storage',\n",
       " 'storage network',\n",
       " 'network infrastructure',\n",
       " 'infrastructure shared',\n",
       " 'shared organization',\n",
       " 'organization ibm',\n",
       " 'ibm cloud',\n",
       " 'cloud first',\n",
       " 'first strategy',\n",
       " 'strategy provides',\n",
       " 'provides necessary',\n",
       " 'necessary sovereignty',\n",
       " 'sovereignty multi',\n",
       " 'multi tenancy',\n",
       " 'tenancy isolation',\n",
       " 'isolation help',\n",
       " 'help ensure',\n",
       " 'ensure data',\n",
       " 'data application',\n",
       " 'application managed',\n",
       " 'managed privately',\n",
       " 'privately across',\n",
       " 'across ibm',\n",
       " 'ibm world',\n",
       " 'world wide',\n",
       " 'wide data',\n",
       " 'data center',\n",
       " 'center variety',\n",
       " 'variety type',\n",
       " 'type data',\n",
       " 'data used',\n",
       " 'used time',\n",
       " 'time data',\n",
       " 'data simpler',\n",
       " 'simpler structured',\n",
       " 'structured relational',\n",
       " 'relational hierarchical',\n",
       " 'hierarchical data',\n",
       " 'data stored',\n",
       " 'stored database',\n",
       " 'database big',\n",
       " 'big data',\n",
       " 'data simply',\n",
       " 'simply mean',\n",
       " 'mean data',\n",
       " 'data includes',\n",
       " 'includes volume',\n",
       " 'volume raw',\n",
       " 'raw content',\n",
       " 'content structured',\n",
       " 'structured form',\n",
       " 'form part',\n",
       " 'part unstructured',\n",
       " 'unstructured add',\n",
       " 'add internet',\n",
       " 'internet thing',\n",
       " 'thing sending',\n",
       " 'sending massive',\n",
       " 'massive amount',\n",
       " 'amount sensor',\n",
       " 'sensor data',\n",
       " 'data world',\n",
       " 'world simple',\n",
       " 'simple ibm',\n",
       " 'ibm data',\n",
       " 'data strategy',\n",
       " 'strategy make',\n",
       " 'make data',\n",
       " 'data simple',\n",
       " 'simple machine',\n",
       " 'machine learning',\n",
       " 'learning capability',\n",
       " 'capability leverage',\n",
       " 'leverage strategy',\n",
       " 'strategy able',\n",
       " 'able process',\n",
       " 'process structured',\n",
       " 'structured semi',\n",
       " 'semi structured',\n",
       " 'structured unstructured',\n",
       " 'unstructured data',\n",
       " 'data set',\n",
       " 'set using',\n",
       " 'using many',\n",
       " 'many connector',\n",
       " 'connector abstracting',\n",
       " 'abstracting complexity',\n",
       " 'complexity exploiting',\n",
       " 'exploiting spark',\n",
       " 'spark r',\n",
       " 'r python',\n",
       " 'python runtimes',\n",
       " 'runtimes machine',\n",
       " 'machine learning',\n",
       " 'learning ibm',\n",
       " 'ibm provides',\n",
       " 'provides 20',\n",
       " '20 different',\n",
       " 'different data',\n",
       " 'data source',\n",
       " 'source connector',\n",
       " 'connector organization',\n",
       " 'organization ingest',\n",
       " 'ingest data',\n",
       " 'data large',\n",
       " 'large compute',\n",
       " 'compute power',\n",
       " 'power enterprise',\n",
       " 'enterprise need',\n",
       " 'need high',\n",
       " 'high compute',\n",
       " 'compute power',\n",
       " 'power since',\n",
       " 'since process',\n",
       " 'process ever',\n",
       " 'ever increasing',\n",
       " 'increasing work',\n",
       " 'work load',\n",
       " 'load data',\n",
       " 'data transaction',\n",
       " 'transaction process',\n",
       " 'process since',\n",
       " 'since spark',\n",
       " 'spark service',\n",
       " 'service single',\n",
       " 'single multi',\n",
       " 'multi tenant',\n",
       " 'tenant cluster',\n",
       " 'cluster resource',\n",
       " 'resource utilization',\n",
       " 'utilization wasted',\n",
       " 'wasted repurpose',\n",
       " 'repurpose computer',\n",
       " 'computer power',\n",
       " 'power scaling',\n",
       " 'scaling scaling',\n",
       " 'scaling capability',\n",
       " 'capability built',\n",
       " 'built part',\n",
       " 'part service',\n",
       " 'service machine',\n",
       " 'machine learning',\n",
       " 'learning able',\n",
       " 'able transparently',\n",
       " 'transparently enable',\n",
       " 'enable scale',\n",
       " 'scale scale',\n",
       " 'scale capability',\n",
       " 'capability information',\n",
       " 'information governance',\n",
       " 'governance ibm',\n",
       " 'ibm strong',\n",
       " 'strong heritage',\n",
       " 'heritage information',\n",
       " 'information governance',\n",
       " 'governance managing',\n",
       " 'managing data',\n",
       " 'data lifecycle',\n",
       " 'lifecycle cleansing',\n",
       " 'cleansing quality',\n",
       " 'quality data',\n",
       " 'data data',\n",
       " 'data wrangling',\n",
       " 'wrangling shaping',\n",
       " 'shaping security',\n",
       " 'security privacy',\n",
       " 'privacy data',\n",
       " 'data information',\n",
       " 'information governance',\n",
       " 'governance catalogue',\n",
       " 'catalogue us',\n",
       " 'us policy',\n",
       " 'policy help',\n",
       " 'help ensure',\n",
       " 'ensure right',\n",
       " 'right people',\n",
       " 'people see',\n",
       " 'see access',\n",
       " 'access execute',\n",
       " 'execute data',\n",
       " 'data service',\n",
       " 'service ibm',\n",
       " 'ibm also',\n",
       " 'also help',\n",
       " 'help provide',\n",
       " 'provide real',\n",
       " 'real time',\n",
       " 'time monitoring',\n",
       " 'monitoring threat',\n",
       " 'threat detection',\n",
       " 'detection prevention',\n",
       " 'prevention intervention',\n",
       " 'intervention well',\n",
       " 'well forensics',\n",
       " 'forensics compliance',\n",
       " 'compliance detailed',\n",
       " 'detailed audit',\n",
       " 'audit obfuscation',\n",
       " 'obfuscation masking',\n",
       " 'masking data',\n",
       " 'data encryption',\n",
       " 'encryption applied',\n",
       " 'applied machine',\n",
       " 'machine learning',\n",
       " 'learning training',\n",
       " 'training data',\n",
       " 'data machine',\n",
       " 'machine learning',\n",
       " 'learning one',\n",
       " 'one trick',\n",
       " 'trick pony',\n",
       " 'pony machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service mlaas',\n",
       " 'mlaas many',\n",
       " 'many form',\n",
       " 'form machine',\n",
       " 'machine learning',\n",
       " 'learning system',\n",
       " 'system ml',\n",
       " 'ml ibm',\n",
       " 'ibm donated',\n",
       " 'donated apache',\n",
       " 'apache foundation',\n",
       " 'foundation natural',\n",
       " 'natural language',\n",
       " 'language processing',\n",
       " 'processing vision',\n",
       " 'vision personality',\n",
       " 'personality emotional',\n",
       " 'emotional insight',\n",
       " 'insight customer',\n",
       " 'customer sentiment',\n",
       " 'sentiment retrieve',\n",
       " 'retrieve rank',\n",
       " 'rank ibm',\n",
       " 'ibm makeit',\n",
       " 'makeit simple',\n",
       " 'simple enterprise',\n",
       " 'enterprise size',\n",
       " 'size pick',\n",
       " 'pick choose',\n",
       " 'choose number',\n",
       " 'number predefined',\n",
       " 'predefined machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service bluemix',\n",
       " 'bluemix tile',\n",
       " 'tile figure',\n",
       " 'figure 2',\n",
       " '2 ran',\n",
       " 'ran earlier',\n",
       " 'earlier blog',\n",
       " 'blog personality',\n",
       " 'personality analyser',\n",
       " 'analyser knew',\n",
       " 'knew really',\n",
       " 'really nice',\n",
       " 'nice guy',\n",
       " 'guy reassuring',\n",
       " 'reassuring hear',\n",
       " 'hear machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service believe',\n",
       " 'believe try',\n",
       " 'try figure',\n",
       " 'figure 2',\n",
       " '2 machine',\n",
       " 'machine learning',\n",
       " 'learning bluemix',\n",
       " 'bluemix service',\n",
       " 'service using',\n",
       " 'using machine',\n",
       " 'machine learning',\n",
       " 'learning reduce',\n",
       " 'reduce cost',\n",
       " 'cost risk',\n",
       " 'risk many',\n",
       " 'many customer',\n",
       " 'customer across',\n",
       " 'across different',\n",
       " 'different industry',\n",
       " 'industry used',\n",
       " 'used machine',\n",
       " 'machine learning',\n",
       " 'learning capability',\n",
       " 'capability help',\n",
       " 'help reduce',\n",
       " 'reduce cost',\n",
       " 'cost improve',\n",
       " 'improve customer',\n",
       " 'customer service',\n",
       " 'service reduce',\n",
       " 'reduce risk',\n",
       " 'risk vermont',\n",
       " 'vermont electrical',\n",
       " 'electrical power',\n",
       " 'power company',\n",
       " 'company velco',\n",
       " 'velco worked',\n",
       " 'worked ibm',\n",
       " 'ibm research',\n",
       " 'research develop',\n",
       " 'develop integrated',\n",
       " 'integrated weather',\n",
       " 'weather forecasting',\n",
       " 'forecasting system',\n",
       " 'system help',\n",
       " 'help deliver',\n",
       " 'deliver reliable',\n",
       " 'reliable clean',\n",
       " 'clean affordable',\n",
       " 'affordable power',\n",
       " 'power consumer',\n",
       " 'consumer integrating',\n",
       " 'integrating renewable',\n",
       " 'renewable energy',\n",
       " 'energy grid',\n",
       " 'grid solution',\n",
       " 'solution combine',\n",
       " 'combine high',\n",
       " 'high resolution',\n",
       " 'resolution weather',\n",
       " 'weather multiple',\n",
       " 'multiple forecasting',\n",
       " 'forecasting tool',\n",
       " 'tool based',\n",
       " 'based machine',\n",
       " 'machine learning',\n",
       " 'learning machine',\n",
       " 'machine learning',\n",
       " 'learning model',\n",
       " 'model trained',\n",
       " 'trained hindcasts',\n",
       " 'hindcasts weather',\n",
       " 'weather correlated',\n",
       " 'correlated historical',\n",
       " 'historical energy',\n",
       " 'energy production',\n",
       " 'production historical',\n",
       " 'historical net',\n",
       " 'net demand',\n",
       " 'demand result',\n",
       " 'result precise',\n",
       " 'precise accurate',\n",
       " 'accurate wind',\n",
       " 'wind solar',\n",
       " 'solar generation',\n",
       " ...]"
      ]
     },
     "execution_count": 1823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two grams\n",
    "\n",
    "[f\"{x[0]} {x[1]}\" for x in list(bigrams(tokened_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2001,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: get two grams\n",
    "\n",
    "def get_bigrams_by_article_id(id):\n",
    "    title = df_nlp.loc[id].doc_full_name\n",
    "    desc = df_nlp.loc[id].doc_description\n",
    "    body = df_nlp.loc[id].doc_body\n",
    "    \n",
    "    # a list of tokens\n",
    "    tokened_text = tokenize(title) + tokenize(desc) + tokenize(body)\n",
    "    \n",
    "    # ngrams 2\n",
    "    grams = [f\"{x[0]} {x[1]}\" for x in list(bigrams(tokened_text))]\n",
    "    \n",
    "    # sort with frequency descending\n",
    "    sorted_grams = [x[0] for x in Counter(grams).most_common()]\n",
    "    \n",
    "    return sorted_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2002,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logical plan',\n",
       " 'apache spark',\n",
       " 'spark sql',\n",
       " 'c 3',\n",
       " 'sort operator',\n",
       " 'parsed logical',\n",
       " 'make sure',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'spark spark',\n",
       " 'aggregate operator',\n",
       " 'a1 14',\n",
       " 'a2 15',\n",
       " 'sql analyzer',\n",
       " 'analyzer resolve',\n",
       " 'resolve order',\n",
       " 'order column',\n",
       " 'column apache',\n",
       " 'sql component',\n",
       " 'component several',\n",
       " 'several sub',\n",
       " 'sub component',\n",
       " 'component including',\n",
       " 'including analyzer',\n",
       " 'analyzer play',\n",
       " 'play important',\n",
       " 'important role',\n",
       " 'role making',\n",
       " 'making sure',\n",
       " 'sure logical',\n",
       " 'plan fully',\n",
       " 'fully resolved',\n",
       " 'resolved end',\n",
       " 'end analysis',\n",
       " 'analysis phase',\n",
       " 'phase analyzer',\n",
       " 'analyzer take',\n",
       " 'take parsed',\n",
       " 'plan input',\n",
       " 'input make',\n",
       " 'sure table',\n",
       " 'table reference',\n",
       " 'reference attribute',\n",
       " 'attribute column',\n",
       " 'column reference',\n",
       " 'reference function',\n",
       " 'function reference',\n",
       " 'reference resolved',\n",
       " 'resolved looking',\n",
       " 'looking metadata',\n",
       " 'metadata catalog',\n",
       " 'catalog work',\n",
       " 'work applying',\n",
       " 'applying set',\n",
       " 'set rule',\n",
       " 'rule logical',\n",
       " 'plan transforming',\n",
       " 'transforming stage',\n",
       " 'stage order',\n",
       " 'order resolve',\n",
       " 'resolve specific',\n",
       " 'specific portion',\n",
       " 'portion plan',\n",
       " 'blog resource',\n",
       " 'resource code',\n",
       " 'code contribution',\n",
       " 'contribution university',\n",
       " 'university ibm',\n",
       " 'ibm design',\n",
       " 'design apache',\n",
       " 'apache systemml',\n",
       " 'systemml apache',\n",
       " 'sql apache',\n",
       " 'working analyzer',\n",
       " 'a1 c',\n",
       " 'c a2',\n",
       " 'attribute referenced',\n",
       " 'sort a1',\n",
       " 'asc true',\n",
       " 'true aggregate',\n",
       " 'aggregate c',\n",
       " 'a1 17',\n",
       " 'a2 18',\n",
       " 'mode complete',\n",
       " 'complete isdistinct',\n",
       " 'isdistinct false',\n",
       " 'false a3',\n",
       " 'a3 19',\n",
       " 'localrelation 1',\n",
       " '1 b',\n",
       " 'b 2',\n",
       " '2 c',\n",
       " '3 4',\n",
       " '4 e',\n",
       " 'e 5',\n",
       " 'operator resolved',\n",
       " 'referenced sort',\n",
       " 'child aggregate',\n",
       " 'order properly',\n",
       " 'properly resolve',\n",
       " 'resolve sort',\n",
       " 'sort resolved',\n",
       " 'a3 16l',\n",
       " 'dilip biswal',\n",
       " 'technology center',\n",
       " 'apache software',\n",
       " 'software foundation',\n",
       " 'plan home',\n",
       " 'home community',\n",
       " 'spark tc',\n",
       " 'tc community',\n",
       " 'plan examine',\n",
       " 'examine working',\n",
       " 'analyzer taking',\n",
       " 'taking example',\n",
       " 'example defect',\n",
       " 'defect describing',\n",
       " 'describing addressed',\n",
       " 'addressed problem',\n",
       " 'problem example',\n",
       " 'example query',\n",
       " 'query select',\n",
       " 'select a1',\n",
       " 'a2 count',\n",
       " 'count c',\n",
       " 'c a3',\n",
       " 'a3 tab',\n",
       " 'tab group',\n",
       " 'group b',\n",
       " 'b order',\n",
       " 'order a1',\n",
       " 'c problem',\n",
       " 'problem description',\n",
       " 'description case',\n",
       " 'case analyzer',\n",
       " 'analyzer unable',\n",
       " 'unable resolve',\n",
       " 'resolve attribute',\n",
       " 'referenced order',\n",
       " 'order clause',\n",
       " 'clause see',\n",
       " 'see let',\n",
       " 'let look',\n",
       " 'look underlying',\n",
       " 'underlying parsed',\n",
       " 'plan parsed',\n",
       " 'plan sort',\n",
       " 'a1 asc',\n",
       " 'asc c',\n",
       " 'c asc',\n",
       " 'c a1',\n",
       " '17 c',\n",
       " '18 count',\n",
       " 'count mode',\n",
       " '19 localrelation',\n",
       " '5 case',\n",
       " 'case localrelation',\n",
       " 'localrelation resolved',\n",
       " 'resolved none',\n",
       " 'none plan',\n",
       " 'plan operator',\n",
       " 'resolved since',\n",
       " 'since underlying',\n",
       " 'underlying attribute',\n",
       " 'attribute refer',\n",
       " 'refer resolved',\n",
       " 'resolved however',\n",
       " 'however see',\n",
       " 'see sort',\n",
       " 'operator aggregate',\n",
       " 'operator attribute',\n",
       " 'resolved output',\n",
       " 'output child',\n",
       " 'operator output',\n",
       " 'output aggregate',\n",
       " 'operator a1',\n",
       " '17 a2',\n",
       " '18 a3',\n",
       " '19 plan',\n",
       " 'plan missing',\n",
       " 'missing attribute',\n",
       " 'attribute c',\n",
       " '3 referenced',\n",
       " 'operator cause',\n",
       " 'cause failure',\n",
       " 'failure analysis',\n",
       " 'analysis process',\n",
       " 'process turn',\n",
       " 'turn result',\n",
       " 'result query',\n",
       " 'query failure',\n",
       " 'failure order',\n",
       " 'operator need',\n",
       " 'need make',\n",
       " 'sure a1',\n",
       " 'a1 sort',\n",
       " 'resolved immediate',\n",
       " 'immediate child',\n",
       " 'c sort',\n",
       " 'resolved grandchild',\n",
       " 'grandchild local',\n",
       " 'local relation',\n",
       " 'relation spark',\n",
       " 'spark analyzer',\n",
       " 'analyzer resolveaggregatefunctions',\n",
       " 'resolveaggregatefunctions rule',\n",
       " 'rule modified',\n",
       " 'modified order',\n",
       " 'operator query',\n",
       " 'query result',\n",
       " 'result following',\n",
       " 'following analyzed',\n",
       " 'analyzed logical',\n",
       " 'plan fix',\n",
       " 'fix project',\n",
       " 'project a1',\n",
       " '14 a2',\n",
       " '15 a3',\n",
       " '16l sort',\n",
       " '14 asc',\n",
       " 'asc a2',\n",
       " '15 asc',\n",
       " 'aggregate 1',\n",
       " '1 c',\n",
       " '3 1',\n",
       " '1 a1',\n",
       " '14 c',\n",
       " '3 a2',\n",
       " '15 count',\n",
       " 'count 1',\n",
       " '1 mode',\n",
       " '16l localrelation',\n",
       " '5 conclusion',\n",
       " 'conclusion hopefully',\n",
       " 'hopefully blog',\n",
       " 'blog give',\n",
       " 'give brief',\n",
       " 'brief insight',\n",
       " 'insight working',\n",
       " 'analyzer post',\n",
       " 'post extended',\n",
       " 'extended description',\n",
       " 'description analyzer',\n",
       " 'analyzer future',\n",
       " 'future general',\n",
       " 'general handling',\n",
       " 'handling analyzer',\n",
       " 'analyzer issue',\n",
       " 'issue requires',\n",
       " 'requires deep',\n",
       " 'deep understanding',\n",
       " 'understanding spark',\n",
       " 'spark logical',\n",
       " 'plan author',\n",
       " 'author dilip',\n",
       " 'biswal senior',\n",
       " 'senior software',\n",
       " 'software engineer',\n",
       " 'engineer spark',\n",
       " 'spark technology',\n",
       " 'center ibm',\n",
       " 'ibm active',\n",
       " 'active apache',\n",
       " 'spark contributor',\n",
       " 'contributor work',\n",
       " 'work open',\n",
       " 'open source',\n",
       " 'source community',\n",
       " 'community experienced',\n",
       " 'experienced relational',\n",
       " 'relational database',\n",
       " 'database distributed',\n",
       " 'distributed computing',\n",
       " 'computing big',\n",
       " 'big data',\n",
       " 'data analytics',\n",
       " 'analytics extensively',\n",
       " 'extensively worked',\n",
       " 'worked sql',\n",
       " 'sql engine',\n",
       " 'engine like',\n",
       " 'like informix',\n",
       " 'informix derby',\n",
       " 'derby big',\n",
       " 'big sql',\n",
       " 'sql share',\n",
       " 'share share',\n",
       " 'share dilip',\n",
       " 'biswal date',\n",
       " 'date 05',\n",
       " '05 june',\n",
       " 'june 2016tags',\n",
       " '2016tags spark',\n",
       " 'apache sparkspark',\n",
       " 'sparkspark technology',\n",
       " 'center community',\n",
       " 'blog apache',\n",
       " 'foundation affiliation',\n",
       " 'affiliation endorse',\n",
       " 'endorse review',\n",
       " 'review material',\n",
       " 'material provided',\n",
       " 'provided website',\n",
       " 'website managed',\n",
       " 'managed ibm',\n",
       " 'ibm apache',\n",
       " 'apache apache',\n",
       " 'spark trademark',\n",
       " 'trademark apache',\n",
       " 'foundation united',\n",
       " 'united state',\n",
       " 'state country']"
      ]
     },
     "execution_count": 2002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bigrams_by_article_id(949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2004,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract possible topics (terms) for a given article.\n",
    "# Algorithm explain:\n",
    "# - Fetch highly relevant articles\n",
    "# - Get each article's ngrams results\n",
    "# - Union the ngrams, find intersection terms from these relevant articles.\n",
    "# Since relevant similarity score is calculated with TFIDF (term frequency),\n",
    "# so the more relevant, the more highly possible to have intersected ngrams terms.\n",
    "\n",
    "def extract_topics_bigrams_by_article_id(article_id, m=3):\n",
    "    \n",
    "    # m: threadhold of how many most relevant articles to look for\n",
    "    # (generally, the more instances to union, the less likely to have intersection, \n",
    "    # so don't set m too large.)\n",
    "    \n",
    "    \n",
    "    # Get relavant articles ids. (also append self)\n",
    "    ids = [article_id] + loopup_similar_articles(article_id)[0][:m]\n",
    "    \n",
    "    # Get ngrams for each article, and assemble to documents list. \n",
    "    documents = [get_bigrams_by_article_id(i) for i in ids]\n",
    "    \n",
    "    \n",
    "    # Find intersected. \n",
    "    # Since np.intersect1d only take two instances, use reduce() to\n",
    "    # do 'batch' processing iteration. \n",
    "    topics = functools.reduce(lambda x, y: np.intersect1d(x, y), documents)\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2008,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['machine learning'], dtype='<U52')"
      ]
     },
     "execution_count": 2008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the result: \n",
    "\n",
    "extract_topics_bigrams_by_article_id(788, m=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2014,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['logistic regression'], dtype='<U29')"
      ]
     },
     "execution_count": 2014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the result: \n",
    "\n",
    "extract_topics_bigrams_by_article_id(1047, m=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2020,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to Perform a Logistic Regression in R',\n",
       " 'Build a logistic regression model with WML & DSX',\n",
       " 'Higher-order Logistic Regression for Large Datasets',\n",
       " 'Build a Naive-Bayes Model with WML & DSX',\n",
       " 'Simple Linear Regression? Do It The Bayesian Way',\n",
       " 'Empirical Bayes for multiple sample sizes',\n",
       " 'Modern Machine Learning Algorithms',\n",
       " 'Understanding empirical Bayes estimation (using baseball statistics)',\n",
       " 'Ensemble Learning to Improve Machine Learning Results',\n",
       " 'Which One to Choose for Your Problem',\n",
       " 'Top 10 Machine Learning Algorithms for Beginners',\n",
       " 'What I Learned Implementing a Classifier from Scratch in Python Â· Jean-Nicholas Hould',\n",
       " 'Essentials of Machine Learning Algorithms (with Python and R Codes)',\n",
       " '10 Essential Algorithms For Machine Learning Engineers',\n",
       " 'Deep Learning Achievements Over the Past Year ',\n",
       " 'ML Algorithm != Learning Machine',\n",
       " 'Learn TensorFlow and Deep Learning Together and Now!',\n",
       " 'Dimensionality Reduction Algorithms',\n",
       " 'Best Practices for Custom Models in Watson Visual Recognition',\n",
       " 'The Two Phases of Gradient Descent in Deep Learning']"
      ]
     },
     "execution_count": 2020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check topic extracted against articles\n",
    "\n",
    "loopup_similar_articles(1047)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2052,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most viewed (popular) articles? \n",
    "\n",
    "def get_most_popular_articles(data=df, n=10):\n",
    "    # input: n - number of top viewed to return\n",
    "    \n",
    "    ids = data.groupby('article_id').size().sort_values(ascending=False).head(n).index\n",
    "    \n",
    "    return list(ids.values)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1429.0,\n",
       " 1330.0,\n",
       " 1431.0,\n",
       " 1427.0,\n",
       " 1364.0,\n",
       " 1314.0,\n",
       " 1293.0,\n",
       " 1170.0,\n",
       " 1162.0,\n",
       " 1304.0,\n",
       " 1436.0,\n",
       " 1271.0,\n",
       " 1398.0,\n",
       " 43.0,\n",
       " 1351.0,\n",
       " 1393.0,\n",
       " 1185.0,\n",
       " 1160.0,\n",
       " 1354.0,\n",
       " 1368.0]"
      ]
     },
     "execution_count": 2053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_list = list(get_most_popular_articles(n=20))\n",
    "most_popular_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "use deep learning for image classification                            937\n",
       "insights from new york car accident reports                           927\n",
       "visualize car data with brunel                                        671\n",
       "use xgboost, scikit-learn & ibm watson machine learning apis          643\n",
       "predicting churn with the spss random tree algorithm                  627\n",
       "healthcare python streaming application demo                          614\n",
       "finding optimal locations of new store using decision optimization    572\n",
       "apache spark lab, part 1: basic concepts                              565\n",
       "analyze energy consumption in buildings                               512\n",
       "gosales transactions for logistic regression model                    483\n",
       "welcome to pixiedust                                                  481\n",
       "customer demographics and sales                                       473\n",
       "total population by country                                           465\n",
       "deep learning with tensorflow course by big data university           460\n",
       "model bike sharing data with spss                                     457\n",
       "the nurse assignment problem                                          455\n",
       "classify tumors with machine learning                                 442\n",
       "analyze accident reports on amazon emr spark                          433\n",
       "movie recommender system with spark machine learning                  426\n",
       "putting a human face on machine learning                              418\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 2056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.article_id.isin(most_popular_list)].title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2074,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep learning with tensorflow course by big data university'"
      ]
     },
     "execution_count": 2074,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.article_id==43.0].title.head(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2078,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep learning with tensorflow course by big data university'"
      ]
     },
     "execution_count": 2078,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp[df_nlp.article_id==43].doc_full_name.values[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2079,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2079,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.article_id==43.0].title.head(1).values[0] == df_nlp[df_nlp.article_id==43].doc_full_name.values[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2080,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use xgboost, scikit-learn & ibm watson machine learning apis'"
      ]
     },
     "execution_count": 2080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.article_id==1427.0].title.head(1).values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "Deal with limit info of article in df. And inconsistent article instance in df, df_content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2091,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 25,\n",
       " 26,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 36,\n",
       " 39,\n",
       " 40,\n",
       " 43,\n",
       " 48,\n",
       " 50,\n",
       " 51,\n",
       " 53,\n",
       " 54,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 62,\n",
       " 64,\n",
       " 65,\n",
       " 68,\n",
       " 74,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 87,\n",
       " 89,\n",
       " 92,\n",
       " 96,\n",
       " 98,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 120,\n",
       " 122,\n",
       " 124,\n",
       " 125,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 134,\n",
       " 136,\n",
       " 138,\n",
       " 142,\n",
       " 143,\n",
       " 145,\n",
       " 146,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 157,\n",
       " 158,\n",
       " 162,\n",
       " 164,\n",
       " 173,\n",
       " 176,\n",
       " 181,\n",
       " 183,\n",
       " 184,\n",
       " 188,\n",
       " 189,\n",
       " 191,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 202,\n",
       " 205,\n",
       " 210,\n",
       " 213,\n",
       " 215,\n",
       " 221,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 230,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 236,\n",
       " 237,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 244,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 256,\n",
       " 258,\n",
       " 260,\n",
       " 263,\n",
       " 266,\n",
       " 268,\n",
       " 270,\n",
       " 273,\n",
       " 277,\n",
       " 278,\n",
       " 283,\n",
       " 284,\n",
       " 288,\n",
       " 291,\n",
       " 295,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 302,\n",
       " 303,\n",
       " 310,\n",
       " 311,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 319,\n",
       " 323,\n",
       " 324,\n",
       " 329,\n",
       " 330,\n",
       " 336,\n",
       " 337,\n",
       " 339,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 355,\n",
       " 356,\n",
       " 359,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 366,\n",
       " 367,\n",
       " 369,\n",
       " 373,\n",
       " 375,\n",
       " 376,\n",
       " 379,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 389,\n",
       " 390,\n",
       " 395,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 404,\n",
       " 409,\n",
       " 411,\n",
       " 412,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 420,\n",
       " 422,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 437,\n",
       " 440,\n",
       " 443,\n",
       " 444,\n",
       " 446,\n",
       " 448,\n",
       " 455,\n",
       " 460,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 468,\n",
       " 470,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 477,\n",
       " 479,\n",
       " 480,\n",
       " 482,\n",
       " 485,\n",
       " 486,\n",
       " 491,\n",
       " 492,\n",
       " 494,\n",
       " 495,\n",
       " 499,\n",
       " 500,\n",
       " 502,\n",
       " 504,\n",
       " 508,\n",
       " 510,\n",
       " 515,\n",
       " 517,\n",
       " 521,\n",
       " 522,\n",
       " 524,\n",
       " 525,\n",
       " 528,\n",
       " 532,\n",
       " 534,\n",
       " 542,\n",
       " 544,\n",
       " 547,\n",
       " 553,\n",
       " 555,\n",
       " 557,\n",
       " 559,\n",
       " 563,\n",
       " 564,\n",
       " 566,\n",
       " 567,\n",
       " 569,\n",
       " 575,\n",
       " 583,\n",
       " 585,\n",
       " 586,\n",
       " 588,\n",
       " 593,\n",
       " 599,\n",
       " 600,\n",
       " 606,\n",
       " 607,\n",
       " 609,\n",
       " 610,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 622,\n",
       " 626,\n",
       " 631,\n",
       " 632,\n",
       " 634,\n",
       " 636,\n",
       " 641,\n",
       " 644,\n",
       " 645,\n",
       " 647,\n",
       " 651,\n",
       " 653,\n",
       " 655,\n",
       " 656,\n",
       " 658,\n",
       " 659,\n",
       " 662,\n",
       " 665,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 673,\n",
       " 675,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 684,\n",
       " 686,\n",
       " 692,\n",
       " 693,\n",
       " 695,\n",
       " 701,\n",
       " 705,\n",
       " 708,\n",
       " 714,\n",
       " 715,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 727,\n",
       " 729,\n",
       " 730,\n",
       " 732,\n",
       " 735,\n",
       " 740,\n",
       " 744,\n",
       " 749,\n",
       " 751,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 763,\n",
       " 764,\n",
       " 766,\n",
       " 768,\n",
       " 778,\n",
       " 781,\n",
       " 782,\n",
       " 784,\n",
       " 785,\n",
       " 788,\n",
       " 793,\n",
       " 795,\n",
       " 800,\n",
       " 805,\n",
       " 809,\n",
       " 812,\n",
       " 813,\n",
       " 821,\n",
       " 825,\n",
       " 833,\n",
       " 843,\n",
       " 844,\n",
       " 846,\n",
       " 853,\n",
       " 855,\n",
       " 857,\n",
       " 858,\n",
       " 861,\n",
       " 862,\n",
       " 864,\n",
       " 865,\n",
       " 868,\n",
       " 870,\n",
       " 871,\n",
       " 876,\n",
       " 878,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 884,\n",
       " 887,\n",
       " 891,\n",
       " 892,\n",
       " 896,\n",
       " 898,\n",
       " 903,\n",
       " 905,\n",
       " 906,\n",
       " 910,\n",
       " 911,\n",
       " 919,\n",
       " 926,\n",
       " 928,\n",
       " 930,\n",
       " 932,\n",
       " 933,\n",
       " 935,\n",
       " 936,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 943,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 951,\n",
       " 952,\n",
       " 955,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 961,\n",
       " 962,\n",
       " 965,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 977,\n",
       " 981,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 990,\n",
       " 993,\n",
       " 996,\n",
       " 997,\n",
       " 1000,\n",
       " 1004,\n",
       " 1006,\n",
       " 1008,\n",
       " 1014,\n",
       " 1015,\n",
       " 1016,\n",
       " 1017,\n",
       " 1018,\n",
       " 1024,\n",
       " 1025,\n",
       " 1028,\n",
       " 1030,\n",
       " 1035,\n",
       " 1038,\n",
       " 1042,\n",
       " 1043,\n",
       " 1044,\n",
       " 1047,\n",
       " 1048,\n",
       " 1050,\n",
       " 1051,\n",
       " 1052,\n",
       " 1053,\n",
       " 1054,\n",
       " 1055,\n",
       " 1056,\n",
       " 1057,\n",
       " 1058,\n",
       " 1059,\n",
       " 1060,\n",
       " 1061,\n",
       " 1062,\n",
       " 1063,\n",
       " 1064,\n",
       " 1065,\n",
       " 1066,\n",
       " 1067,\n",
       " 1068,\n",
       " 1069,\n",
       " 1070,\n",
       " 1071,\n",
       " 1072,\n",
       " 1073,\n",
       " 1074,\n",
       " 1075,\n",
       " 1077,\n",
       " 1078,\n",
       " 1079,\n",
       " 1080,\n",
       " 1083,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1089,\n",
       " 1091,\n",
       " 1092,\n",
       " 1097,\n",
       " 1101,\n",
       " 1106,\n",
       " 1108,\n",
       " 1112,\n",
       " 1113,\n",
       " 1114,\n",
       " 1116,\n",
       " 1119,\n",
       " 1120,\n",
       " 1121,\n",
       " 1122,\n",
       " 1123,\n",
       " 1124,\n",
       " 1125,\n",
       " 1127,\n",
       " 1128,\n",
       " 1130,\n",
       " 1134,\n",
       " 1135,\n",
       " 1137,\n",
       " 1138,\n",
       " 1139,\n",
       " 1140,\n",
       " 1141,\n",
       " 1142,\n",
       " 1143,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1148,\n",
       " 1149,\n",
       " 1150,\n",
       " 1151,\n",
       " 1152,\n",
       " 1153,\n",
       " 1154,\n",
       " 1155,\n",
       " 1156,\n",
       " 1157,\n",
       " 1158,\n",
       " 1159,\n",
       " 1160,\n",
       " 1161,\n",
       " 1162,\n",
       " 1163,\n",
       " 1164,\n",
       " 1165,\n",
       " 1166,\n",
       " 1167,\n",
       " 1168,\n",
       " 1169,\n",
       " 1170,\n",
       " 1171,\n",
       " 1172,\n",
       " 1173,\n",
       " 1174,\n",
       " 1175,\n",
       " 1176,\n",
       " 1177,\n",
       " 1178,\n",
       " 1179,\n",
       " 1180,\n",
       " 1181,\n",
       " 1183,\n",
       " 1184,\n",
       " 1185,\n",
       " 1186,\n",
       " 1187,\n",
       " 1188,\n",
       " 1189,\n",
       " 1190,\n",
       " 1191,\n",
       " 1192,\n",
       " 1195,\n",
       " 1196,\n",
       " 1197,\n",
       " 1198,\n",
       " 1199,\n",
       " 1200,\n",
       " 1202,\n",
       " 1203,\n",
       " 1206,\n",
       " 1208,\n",
       " 1210,\n",
       " 1219,\n",
       " 1221,\n",
       " 1225,\n",
       " 1226,\n",
       " 1227,\n",
       " 1228,\n",
       " 1230,\n",
       " 1232,\n",
       " 1233,\n",
       " 1234,\n",
       " 1235,\n",
       " 1237,\n",
       " 1244,\n",
       " 1247,\n",
       " 1251,\n",
       " 1252,\n",
       " 1253,\n",
       " 1254,\n",
       " 1257,\n",
       " 1260,\n",
       " 1261,\n",
       " 1263,\n",
       " 1266,\n",
       " 1267,\n",
       " 1271,\n",
       " 1273,\n",
       " 1274,\n",
       " 1276,\n",
       " 1277,\n",
       " 1278,\n",
       " 1279,\n",
       " 1280,\n",
       " 1281,\n",
       " 1282,\n",
       " 1283,\n",
       " 1285,\n",
       " 1286,\n",
       " 1289,\n",
       " 1290,\n",
       " 1291,\n",
       " 1292,\n",
       " 1293,\n",
       " 1294,\n",
       " 1295,\n",
       " 1296,\n",
       " 1297,\n",
       " 1298,\n",
       " 1299,\n",
       " 1303,\n",
       " 1304,\n",
       " 1305,\n",
       " 1306,\n",
       " 1307,\n",
       " 1308,\n",
       " 1313,\n",
       " 1314,\n",
       " 1315,\n",
       " 1317,\n",
       " 1318,\n",
       " 1319,\n",
       " 1320,\n",
       " 1321,\n",
       " 1324,\n",
       " 1326,\n",
       " 1328,\n",
       " 1329,\n",
       " 1330,\n",
       " 1331,\n",
       " 1332,\n",
       " 1333,\n",
       " 1334,\n",
       " 1335,\n",
       " 1336,\n",
       " 1337,\n",
       " 1338,\n",
       " 1340,\n",
       " 1343,\n",
       " 1344,\n",
       " 1346,\n",
       " 1348,\n",
       " 1349,\n",
       " 1350,\n",
       " 1351,\n",
       " 1354,\n",
       " 1355,\n",
       " 1356,\n",
       " 1357,\n",
       " 1358,\n",
       " 1359,\n",
       " 1360,\n",
       " 1361,\n",
       " 1362,\n",
       " 1363,\n",
       " 1364,\n",
       " 1365,\n",
       " 1366,\n",
       " 1367,\n",
       " 1368,\n",
       " 1369,\n",
       " 1371,\n",
       " 1372,\n",
       " 1377,\n",
       " 1378,\n",
       " 1386,\n",
       " 1387,\n",
       " 1390,\n",
       " 1391,\n",
       " 1393,\n",
       " 1394,\n",
       " 1395,\n",
       " 1396,\n",
       " 1397,\n",
       " 1398,\n",
       " 1400,\n",
       " 1401,\n",
       " 1402,\n",
       " 1403,\n",
       " 1404,\n",
       " 1405,\n",
       " 1406,\n",
       " 1407,\n",
       " 1408,\n",
       " 1409,\n",
       " 1410,\n",
       " 1411,\n",
       " 1412,\n",
       " 1414,\n",
       " 1415,\n",
       " 1416,\n",
       " 1418,\n",
       " 1419,\n",
       " 1420,\n",
       " 1421,\n",
       " 1422,\n",
       " 1423,\n",
       " 1424,\n",
       " 1425,\n",
       " 1426,\n",
       " 1427,\n",
       " 1428,\n",
       " 1429,\n",
       " 1430,\n",
       " 1431,\n",
       " 1432,\n",
       " 1433,\n",
       " 1434,\n",
       " 1435,\n",
       " 1436,\n",
       " 1437,\n",
       " 1439,\n",
       " 1440,\n",
       " 1441,\n",
       " 1442,\n",
       " 1443,\n",
       " 1444]"
      ]
     },
     "execution_count": 2091,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ids_in_df = sorted(list(df.article_id.astype('int64').value_counts().index))\n",
    "\n",
    "unique_ids_in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2094,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 2094,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ids_in_df_content = sorted(list(df_content.article_id.astype('int64').value_counts().index))\n",
    "unique_ids_in_df_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2096,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1051,\n",
       " 1052,\n",
       " 1053,\n",
       " 1054,\n",
       " 1055,\n",
       " 1056,\n",
       " 1057,\n",
       " 1058,\n",
       " 1059,\n",
       " 1060,\n",
       " 1061,\n",
       " 1062,\n",
       " 1063,\n",
       " 1064,\n",
       " 1065,\n",
       " 1066,\n",
       " 1067,\n",
       " 1068,\n",
       " 1069,\n",
       " 1070,\n",
       " 1071,\n",
       " 1072,\n",
       " 1073,\n",
       " 1074,\n",
       " 1075,\n",
       " 1077,\n",
       " 1078,\n",
       " 1079,\n",
       " 1080,\n",
       " 1083,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1089,\n",
       " 1091,\n",
       " 1092,\n",
       " 1097,\n",
       " 1101,\n",
       " 1106,\n",
       " 1108,\n",
       " 1112,\n",
       " 1113,\n",
       " 1114,\n",
       " 1116,\n",
       " 1119,\n",
       " 1120,\n",
       " 1121,\n",
       " 1122,\n",
       " 1123,\n",
       " 1124,\n",
       " 1125,\n",
       " 1127,\n",
       " 1128,\n",
       " 1130,\n",
       " 1134,\n",
       " 1135,\n",
       " 1137,\n",
       " 1138,\n",
       " 1139,\n",
       " 1140,\n",
       " 1141,\n",
       " 1142,\n",
       " 1143,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1148,\n",
       " 1149,\n",
       " 1150,\n",
       " 1151,\n",
       " 1152,\n",
       " 1153,\n",
       " 1154,\n",
       " 1155,\n",
       " 1156,\n",
       " 1157,\n",
       " 1158,\n",
       " 1159,\n",
       " 1160,\n",
       " 1161,\n",
       " 1162,\n",
       " 1163,\n",
       " 1164,\n",
       " 1165,\n",
       " 1166,\n",
       " 1167,\n",
       " 1168,\n",
       " 1169,\n",
       " 1170,\n",
       " 1171,\n",
       " 1172,\n",
       " 1173,\n",
       " 1174,\n",
       " 1175,\n",
       " 1176,\n",
       " 1177,\n",
       " 1178,\n",
       " 1179,\n",
       " 1180,\n",
       " 1181,\n",
       " 1183,\n",
       " 1184,\n",
       " 1185,\n",
       " 1186,\n",
       " 1187,\n",
       " 1188,\n",
       " 1189,\n",
       " 1190,\n",
       " 1191,\n",
       " 1192,\n",
       " 1195,\n",
       " 1196,\n",
       " 1197,\n",
       " 1198,\n",
       " 1199,\n",
       " 1200,\n",
       " 1202,\n",
       " 1203,\n",
       " 1206,\n",
       " 1208,\n",
       " 1210,\n",
       " 1219,\n",
       " 1221,\n",
       " 1225,\n",
       " 1226,\n",
       " 1227,\n",
       " 1228,\n",
       " 1230,\n",
       " 1232,\n",
       " 1233,\n",
       " 1234,\n",
       " 1235,\n",
       " 1237,\n",
       " 1244,\n",
       " 1247,\n",
       " 1251,\n",
       " 1252,\n",
       " 1253,\n",
       " 1254,\n",
       " 1257,\n",
       " 1260,\n",
       " 1261,\n",
       " 1263,\n",
       " 1266,\n",
       " 1267,\n",
       " 1271,\n",
       " 1273,\n",
       " 1274,\n",
       " 1276,\n",
       " 1277,\n",
       " 1278,\n",
       " 1279,\n",
       " 1280,\n",
       " 1281,\n",
       " 1282,\n",
       " 1283,\n",
       " 1285,\n",
       " 1286,\n",
       " 1289,\n",
       " 1290,\n",
       " 1291,\n",
       " 1292,\n",
       " 1293,\n",
       " 1294,\n",
       " 1295,\n",
       " 1296,\n",
       " 1297,\n",
       " 1298,\n",
       " 1299,\n",
       " 1303,\n",
       " 1304,\n",
       " 1305,\n",
       " 1306,\n",
       " 1307,\n",
       " 1308,\n",
       " 1313,\n",
       " 1314,\n",
       " 1315,\n",
       " 1317,\n",
       " 1318,\n",
       " 1319,\n",
       " 1320,\n",
       " 1321,\n",
       " 1324,\n",
       " 1326,\n",
       " 1328,\n",
       " 1329,\n",
       " 1330,\n",
       " 1331,\n",
       " 1332,\n",
       " 1333,\n",
       " 1334,\n",
       " 1335,\n",
       " 1336,\n",
       " 1337,\n",
       " 1338,\n",
       " 1340,\n",
       " 1343,\n",
       " 1344,\n",
       " 1346,\n",
       " 1348,\n",
       " 1349,\n",
       " 1350,\n",
       " 1351,\n",
       " 1354,\n",
       " 1355,\n",
       " 1356,\n",
       " 1357,\n",
       " 1358,\n",
       " 1359,\n",
       " 1360,\n",
       " 1361,\n",
       " 1362,\n",
       " 1363,\n",
       " 1364,\n",
       " 1365,\n",
       " 1366,\n",
       " 1367,\n",
       " 1368,\n",
       " 1369,\n",
       " 1371,\n",
       " 1372,\n",
       " 1377,\n",
       " 1378,\n",
       " 1386,\n",
       " 1387,\n",
       " 1390,\n",
       " 1391,\n",
       " 1393,\n",
       " 1394,\n",
       " 1395,\n",
       " 1396,\n",
       " 1397,\n",
       " 1398,\n",
       " 1400,\n",
       " 1401,\n",
       " 1402,\n",
       " 1403,\n",
       " 1404,\n",
       " 1405,\n",
       " 1406,\n",
       " 1407,\n",
       " 1408,\n",
       " 1409,\n",
       " 1410,\n",
       " 1411,\n",
       " 1412,\n",
       " 1414,\n",
       " 1415,\n",
       " 1416,\n",
       " 1418,\n",
       " 1419,\n",
       " 1420,\n",
       " 1421,\n",
       " 1422,\n",
       " 1423,\n",
       " 1424,\n",
       " 1425,\n",
       " 1426,\n",
       " 1427,\n",
       " 1428,\n",
       " 1429,\n",
       " 1430,\n",
       " 1431,\n",
       " 1432,\n",
       " 1433,\n",
       " 1434,\n",
       " 1435,\n",
       " 1436,\n",
       " 1437,\n",
       " 1439,\n",
       " 1440,\n",
       " 1441,\n",
       " 1442,\n",
       " 1443,\n",
       " 1444]"
      ]
     },
     "execution_count": 2096,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ids that are not in df_content, but appear in df\n",
    "\n",
    "ids_not_in_df_content = list(set(unique_ids_in_df) - set(unique_ids_in_df_content))\n",
    "ids_not_in_df_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2098,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 2098,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 277 articles that mentioned in df, but not shown in df_content. \n",
    "# (inconsistency: the given df_content is not up to date)\n",
    "# For these 277 articles, we only have 'page title' information. \n",
    "\n",
    "len(ids_not_in_df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>1051</td>\n",
       "      <td>a tensorflow regression model to predict house...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>1052</td>\n",
       "      <td>access db2 warehouse on cloud and db2 with python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>1053</td>\n",
       "      <td>access mysql with python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>1054</td>\n",
       "      <td>access mysql with r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>1055</td>\n",
       "      <td>access postgresql with python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1440</td>\n",
       "      <td>world marriage data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1441</td>\n",
       "      <td>world tourism data by the world tourism organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1442</td>\n",
       "      <td>worldwide county and region - national account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1443</td>\n",
       "      <td>worldwide electricity demand and production 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1444</td>\n",
       "      <td>worldwide fuel oil consumption by household (i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id                                              title\n",
       "1051        1051  a tensorflow regression model to predict house...\n",
       "1052        1052  access db2 warehouse on cloud and db2 with python\n",
       "1053        1053                           access mysql with python\n",
       "1054        1054                                access mysql with r\n",
       "1055        1055                      access postgresql with python\n",
       "...          ...                                                ...\n",
       "1440        1440                                world marriage data\n",
       "1441        1441  world tourism data by the world tourism organi...\n",
       "1442        1442  worldwide county and region - national account...\n",
       "1443        1443  worldwide electricity demand and production 19...\n",
       "1444        1444  worldwide fuel oil consumption by household (i...\n",
       "\n",
       "[277 rows x 2 columns]"
      ]
     },
     "execution_count": 2144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset the diff articles dataframe\n",
    "\n",
    "ids_not_in_df_content_float = [float(x) for x in ids_not_in_df_content]\n",
    "\n",
    "df_diff = df[df.article_id.isin(ids_not_in_df_content_float)][['article_id', 'title']].drop_duplicates()\n",
    "\n",
    "df_diff.article_id = df_diff.article_id.astype('int64')\n",
    "\n",
    "df_diff = df_diff.sort_values(by='article_id', ascending=True)\n",
    "\n",
    "df_diff.index = df_diff.article_id.values.flatten()\n",
    "\n",
    "df_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As these diff articles only have title info, we can build a title-title similary model based on TFIDF of title-title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>1046</td>\n",
       "      <td>A look under the covers of PouchDB-find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>1047</td>\n",
       "      <td>A comparison of logistic regression and naive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>1048</td>\n",
       "      <td>What I Learned Implementing a Classifier from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>1049</td>\n",
       "      <td>Use dashDB with Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>1050</td>\n",
       "      <td>Jupyter Notebooks with Scala, Python, or R Ker...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                              title\n",
       "0             0  Detect Malfunctioning IoT Sensors with Streami...\n",
       "1             1  Communicating data science: A guide to present...\n",
       "2             2         This Week in Data Science (April 18, 2017)\n",
       "3             3  DataLayer Conference: Boost the performance of...\n",
       "4             4      Analyze NY Restaurant data using Spark in DSX\n",
       "...         ...                                                ...\n",
       "1046       1046            A look under the covers of PouchDB-find\n",
       "1047       1047  A comparison of logistic regression and naive ...\n",
       "1048       1048  What I Learned Implementing a Classifier from ...\n",
       "1049       1049                              Use dashDB with Spark\n",
       "1050       1050  Jupyter Notebooks with Scala, Python, or R Ker...\n",
       "\n",
       "[1051 rows x 2 columns]"
      ]
     },
     "execution_count": 2143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_titles_subset = df_content[['article_id', 'doc_full_name']]\n",
    "\n",
    "df_content_titles_subset.columns = [['article_id', 'title']]\n",
    "\n",
    "df_content_titles_subset.index = df_content_titles_subset['article_id'].values.flatten()\n",
    "\n",
    "df_content_titles_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2171,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_content_titles_subset.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2172,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df_diff.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 'Detect Malfunctioning IoT Sensors with Streaming Analytics'],\n",
       "       [1, 'Communicating data science: A guide to presenting your work'],\n",
       "       [2, 'This Week in Data Science (April 18, 2017)'],\n",
       "       ...,\n",
       "       [1442,\n",
       "        'worldwide county and region - national accounts - gross national income 1948-2010'],\n",
       "       [1443, 'worldwide electricity demand and production 1990-2012'],\n",
       "       [1444,\n",
       "        'worldwide fuel oil consumption by household (in 1000 metric tons)']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_titles_np = np.concatenate((a, b))\n",
    "all_titles_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1328"
      ]
     },
     "execution_count": 2180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_titles_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1390</td>\n",
       "      <td>style transfer experiments with watson machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1391</td>\n",
       "      <td>sudoku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1393</td>\n",
       "      <td>the nurse assignment problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>1394</td>\n",
       "      <td>the nurse assignment problem data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>1395</td>\n",
       "      <td>the unit commitment problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1396</td>\n",
       "      <td>times world university ranking analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>1397</td>\n",
       "      <td>total employment, by economic activity (thousa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1398</td>\n",
       "      <td>total population by country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>1400</td>\n",
       "      <td>uci ml repository: chronic kidney disease data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>1401</td>\n",
       "      <td>uci: abalone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1402</td>\n",
       "      <td>uci: adult - predict income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>1403</td>\n",
       "      <td>uci: car evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>1404</td>\n",
       "      <td>uci: forest fires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>1405</td>\n",
       "      <td>uci: heart disease - cleveland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>1406</td>\n",
       "      <td>uci: iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1407</td>\n",
       "      <td>uci: poker hand - testing data set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>1408</td>\n",
       "      <td>uci: poker hand - training data set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>1409</td>\n",
       "      <td>uci: red wine quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>1410</td>\n",
       "      <td>uci: sms spam collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>1411</td>\n",
       "      <td>uci: white wine quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>1412</td>\n",
       "      <td>uci: wine recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>1414</td>\n",
       "      <td>united states demographic measures: education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>1415</td>\n",
       "      <td>united states demographic measures: income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>1416</td>\n",
       "      <td>united states demographic measures: population...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>1418</td>\n",
       "      <td>united states demographic measures: zip code t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>1419</td>\n",
       "      <td>unmet need for family planning, spacing, perce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>1420</td>\n",
       "      <td>use apache systemml and spark for machine lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>1421</td>\n",
       "      <td>use pmml to predict iris species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>1422</td>\n",
       "      <td>use r dataframes &amp; ibm watson natural language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>1423</td>\n",
       "      <td>use sql with data in hadoop python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>1424</td>\n",
       "      <td>use spark for python to load data and run sql ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>1425</td>\n",
       "      <td>use spark for r to load data and run sql queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>1426</td>\n",
       "      <td>use spark for scala to load data and run sql q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>1427</td>\n",
       "      <td>use xgboost, scikit-learn &amp; ibm watson machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>1428</td>\n",
       "      <td>use decision optimization to schedule league g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>1429</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1430</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1431</td>\n",
       "      <td>visualize car data with brunel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>1432</td>\n",
       "      <td>visualize data with the matplotlib library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1433</td>\n",
       "      <td>visualize the 1854 london cholera outbreak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1434</td>\n",
       "      <td>wages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1435</td>\n",
       "      <td>watson assistant workspace analysis with user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>1436</td>\n",
       "      <td>welcome to pixiedust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1437</td>\n",
       "      <td>what caused the challenger disaster?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1439</td>\n",
       "      <td>working with ibm cloud object storage in r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1440</td>\n",
       "      <td>world marriage data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1441</td>\n",
       "      <td>world tourism data by the world tourism organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1442</td>\n",
       "      <td>worldwide county and region - national account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1443</td>\n",
       "      <td>worldwide electricity demand and production 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1444</td>\n",
       "      <td>worldwide fuel oil consumption by household (i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                              title\n",
       "article_id                                                              \n",
       "1390             1390  style transfer experiments with watson machine...\n",
       "1391             1391                                             sudoku\n",
       "1393             1393                       the nurse assignment problem\n",
       "1394             1394                  the nurse assignment problem data\n",
       "1395             1395                        the unit commitment problem\n",
       "1396             1396            times world university ranking analysis\n",
       "1397             1397  total employment, by economic activity (thousa...\n",
       "1398             1398                        total population by country\n",
       "1400             1400  uci ml repository: chronic kidney disease data...\n",
       "1401             1401                                       uci: abalone\n",
       "1402             1402                        uci: adult - predict income\n",
       "1403             1403                                uci: car evaluation\n",
       "1404             1404                                  uci: forest fires\n",
       "1405             1405                     uci: heart disease - cleveland\n",
       "1406             1406                                          uci: iris\n",
       "1407             1407                 uci: poker hand - testing data set\n",
       "1408             1408                uci: poker hand - training data set\n",
       "1409             1409                              uci: red wine quality\n",
       "1410             1410                           uci: sms spam collection\n",
       "1411             1411                            uci: white wine quality\n",
       "1412             1412                              uci: wine recognition\n",
       "1414             1414      united states demographic measures: education\n",
       "1415             1415         united states demographic measures: income\n",
       "1416             1416  united states demographic measures: population...\n",
       "1418             1418  united states demographic measures: zip code t...\n",
       "1419             1419  unmet need for family planning, spacing, perce...\n",
       "1420             1420  use apache systemml and spark for machine lear...\n",
       "1421             1421                   use pmml to predict iris species\n",
       "1422             1422  use r dataframes & ibm watson natural language...\n",
       "1423             1423                 use sql with data in hadoop python\n",
       "1424             1424  use spark for python to load data and run sql ...\n",
       "1425             1425   use spark for r to load data and run sql queries\n",
       "1426             1426  use spark for scala to load data and run sql q...\n",
       "1427             1427  use xgboost, scikit-learn & ibm watson machine...\n",
       "1428             1428  use decision optimization to schedule league g...\n",
       "1429             1429         use deep learning for image classification\n",
       "1430             1430  using pixiedust for fast, flexible, and easier...\n",
       "1431             1431                     visualize car data with brunel\n",
       "1432             1432         visualize data with the matplotlib library\n",
       "1433             1433         visualize the 1854 london cholera outbreak\n",
       "1434             1434                                              wages\n",
       "1435             1435  watson assistant workspace analysis with user ...\n",
       "1436             1436                               welcome to pixiedust\n",
       "1437             1437               what caused the challenger disaster?\n",
       "1439             1439         working with ibm cloud object storage in r\n",
       "1440             1440                                world marriage data\n",
       "1441             1441  world tourism data by the world tourism organi...\n",
       "1442             1442  worldwide county and region - national account...\n",
       "1443             1443  worldwide electricity demand and production 19...\n",
       "1444             1444  worldwide fuel oil consumption by household (i..."
      ]
     },
     "execution_count": 2185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_df = pd.DataFrame(all_titles_np)\n",
    "titles_df.columns = ['article_id', 'title']\n",
    "titles_df.index = titles_df.article_id\n",
    "titles_df.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataframe, store title-title-similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1328"
      ]
     },
     "execution_count": 2187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1434</th>\n",
       "      <th>1435</th>\n",
       "      <th>1436</th>\n",
       "      <th>1437</th>\n",
       "      <th>1439</th>\n",
       "      <th>1440</th>\n",
       "      <th>1441</th>\n",
       "      <th>1442</th>\n",
       "      <th>1443</th>\n",
       "      <th>1444</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1328 rows Ã 1328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "article_id                                                              ...   \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "1440         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1441         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1442         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1443         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1444         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "article_id  1434  1435  1436  1437  1439  1440  1441  1442  1443  1444  \n",
       "article_id                                                              \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "1440         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1441         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1442         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1443         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1444         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[1328 rows x 1328 columns]"
      ]
     },
     "execution_count": 2188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a title-title dummy dataframe.\n",
    "title_title = pd.DataFrame(\n",
    "                data=np.zeros((titles_df.shape[0],titles_df.shape[0])), \n",
    "                index=titles_df.index, \n",
    "                columns=titles_df.index\n",
    "            )\n",
    "\n",
    "title_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_title_similarity(article_id_1, article_id_2, data=titles_df):\n",
    "    # Compute the consine similarity based on tfidf of title (doc_full_name)\n",
    "    # Based on the titles df (union all titles in df and df_content)\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(data.loc[article_id_1].title))\n",
    "    doc_b = ' '.join(tokenize(data.loc[article_id_2].title))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22576484600261607"
      ]
     },
     "execution_count": 2197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_title_similarity(284, 977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0 of 1444.\n",
      "Processing row 1 of 1444.\n",
      "Processing row 2 of 1444.\n",
      "Processing row 3 of 1444.\n",
      "Processing row 4 of 1444.\n",
      "Processing row 5 of 1444.\n",
      "Processing row 6 of 1444.\n",
      "Processing row 7 of 1444.\n",
      "Processing row 8 of 1444.\n",
      "Processing row 9 of 1444.\n",
      "Processing row 10 of 1444.\n",
      "Processing row 11 of 1444.\n",
      "Processing row 12 of 1444.\n",
      "Processing row 13 of 1444.\n",
      "Processing row 14 of 1444.\n",
      "Processing row 15 of 1444.\n",
      "Processing row 16 of 1444.\n",
      "Processing row 17 of 1444.\n",
      "Processing row 18 of 1444.\n",
      "Processing row 19 of 1444.\n",
      "Processing row 20 of 1444.\n",
      "Processing row 21 of 1444.\n",
      "Processing row 22 of 1444.\n",
      "Processing row 23 of 1444.\n",
      "Processing row 24 of 1444.\n",
      "Processing row 25 of 1444.\n",
      "Processing row 26 of 1444.\n",
      "Processing row 27 of 1444.\n",
      "Processing row 28 of 1444.\n",
      "Processing row 29 of 1444.\n",
      "Processing row 30 of 1444.\n",
      "Processing row 31 of 1444.\n",
      "Processing row 32 of 1444.\n",
      "Processing row 33 of 1444.\n",
      "Processing row 34 of 1444.\n",
      "Processing row 35 of 1444.\n",
      "Processing row 36 of 1444.\n",
      "Processing row 37 of 1444.\n",
      "Processing row 38 of 1444.\n",
      "Processing row 39 of 1444.\n",
      "Processing row 40 of 1444.\n",
      "Processing row 41 of 1444.\n",
      "Processing row 42 of 1444.\n",
      "Processing row 43 of 1444.\n",
      "Processing row 44 of 1444.\n",
      "Processing row 45 of 1444.\n",
      "Processing row 46 of 1444.\n",
      "Processing row 47 of 1444.\n",
      "Processing row 48 of 1444.\n",
      "Processing row 49 of 1444.\n",
      "Processing row 50 of 1444.\n",
      "Processing row 51 of 1444.\n",
      "Processing row 52 of 1444.\n",
      "Processing row 53 of 1444.\n",
      "Processing row 54 of 1444.\n",
      "Processing row 55 of 1444.\n",
      "Processing row 56 of 1444.\n",
      "Processing row 57 of 1444.\n",
      "Processing row 58 of 1444.\n",
      "Processing row 59 of 1444.\n",
      "Processing row 60 of 1444.\n",
      "Processing row 61 of 1444.\n",
      "Processing row 62 of 1444.\n",
      "Processing row 63 of 1444.\n",
      "Processing row 64 of 1444.\n",
      "Processing row 65 of 1444.\n",
      "Processing row 66 of 1444.\n",
      "Processing row 67 of 1444.\n",
      "Processing row 68 of 1444.\n",
      "Processing row 69 of 1444.\n",
      "Processing row 70 of 1444.\n",
      "Processing row 71 of 1444.\n",
      "Processing row 72 of 1444.\n",
      "Processing row 73 of 1444.\n",
      "Processing row 74 of 1444.\n",
      "Processing row 75 of 1444.\n",
      "Processing row 76 of 1444.\n",
      "Processing row 77 of 1444.\n",
      "Processing row 78 of 1444.\n",
      "Processing row 79 of 1444.\n",
      "Processing row 80 of 1444.\n",
      "Processing row 81 of 1444.\n",
      "Processing row 82 of 1444.\n",
      "Processing row 83 of 1444.\n",
      "Processing row 84 of 1444.\n",
      "Processing row 85 of 1444.\n",
      "Processing row 86 of 1444.\n",
      "Processing row 87 of 1444.\n",
      "Processing row 88 of 1444.\n",
      "Processing row 89 of 1444.\n",
      "Processing row 90 of 1444.\n",
      "Processing row 91 of 1444.\n",
      "Processing row 92 of 1444.\n",
      "Processing row 93 of 1444.\n",
      "Processing row 94 of 1444.\n",
      "Processing row 95 of 1444.\n",
      "Processing row 96 of 1444.\n",
      "Processing row 97 of 1444.\n",
      "Processing row 98 of 1444.\n",
      "Processing row 99 of 1444.\n",
      "Processing row 100 of 1444.\n",
      "Processing row 101 of 1444.\n",
      "Processing row 102 of 1444.\n",
      "Processing row 103 of 1444.\n",
      "Processing row 104 of 1444.\n",
      "Processing row 105 of 1444.\n",
      "Processing row 106 of 1444.\n",
      "Processing row 107 of 1444.\n",
      "Processing row 108 of 1444.\n",
      "Processing row 109 of 1444.\n",
      "Processing row 110 of 1444.\n",
      "Processing row 111 of 1444.\n",
      "Processing row 112 of 1444.\n",
      "Processing row 113 of 1444.\n",
      "Processing row 114 of 1444.\n",
      "Processing row 115 of 1444.\n",
      "Processing row 116 of 1444.\n",
      "Processing row 117 of 1444.\n",
      "Processing row 118 of 1444.\n",
      "Processing row 119 of 1444.\n",
      "Processing row 120 of 1444.\n",
      "Processing row 121 of 1444.\n",
      "Processing row 122 of 1444.\n",
      "Processing row 123 of 1444.\n",
      "Processing row 124 of 1444.\n",
      "Processing row 125 of 1444.\n",
      "Processing row 126 of 1444.\n",
      "Processing row 127 of 1444.\n",
      "Processing row 128 of 1444.\n",
      "Processing row 129 of 1444.\n",
      "Processing row 130 of 1444.\n",
      "Processing row 131 of 1444.\n",
      "Processing row 132 of 1444.\n",
      "Processing row 133 of 1444.\n",
      "Processing row 134 of 1444.\n",
      "Processing row 135 of 1444.\n",
      "Processing row 136 of 1444.\n",
      "Processing row 137 of 1444.\n",
      "Processing row 138 of 1444.\n",
      "Processing row 139 of 1444.\n",
      "Processing row 140 of 1444.\n",
      "Processing row 141 of 1444.\n",
      "Processing row 142 of 1444.\n",
      "Processing row 143 of 1444.\n",
      "Processing row 144 of 1444.\n",
      "Processing row 145 of 1444.\n",
      "Processing row 146 of 1444.\n",
      "Processing row 147 of 1444.\n",
      "Processing row 148 of 1444.\n",
      "Processing row 149 of 1444.\n",
      "Processing row 150 of 1444.\n",
      "Processing row 151 of 1444.\n",
      "Processing row 152 of 1444.\n",
      "Processing row 153 of 1444.\n",
      "Processing row 154 of 1444.\n",
      "Processing row 155 of 1444.\n",
      "Processing row 156 of 1444.\n",
      "Processing row 157 of 1444.\n",
      "Processing row 158 of 1444.\n",
      "Processing row 159 of 1444.\n",
      "Processing row 160 of 1444.\n",
      "Processing row 161 of 1444.\n",
      "Processing row 162 of 1444.\n",
      "Processing row 163 of 1444.\n",
      "Processing row 164 of 1444.\n",
      "Processing row 165 of 1444.\n",
      "Processing row 166 of 1444.\n",
      "Processing row 167 of 1444.\n",
      "Processing row 168 of 1444.\n",
      "Processing row 169 of 1444.\n",
      "Processing row 170 of 1444.\n",
      "Processing row 171 of 1444.\n",
      "Processing row 172 of 1444.\n",
      "Processing row 173 of 1444.\n",
      "Processing row 174 of 1444.\n",
      "Processing row 175 of 1444.\n",
      "Processing row 176 of 1444.\n",
      "Processing row 177 of 1444.\n",
      "Processing row 178 of 1444.\n",
      "Processing row 179 of 1444.\n",
      "Processing row 180 of 1444.\n",
      "Processing row 181 of 1444.\n",
      "Processing row 182 of 1444.\n",
      "Processing row 183 of 1444.\n",
      "Processing row 184 of 1444.\n",
      "Processing row 185 of 1444.\n",
      "Processing row 186 of 1444.\n",
      "Processing row 187 of 1444.\n",
      "Processing row 188 of 1444.\n",
      "Processing row 189 of 1444.\n",
      "Processing row 190 of 1444.\n",
      "Processing row 191 of 1444.\n",
      "Processing row 192 of 1444.\n",
      "Processing row 193 of 1444.\n",
      "Processing row 194 of 1444.\n",
      "Processing row 195 of 1444.\n",
      "Processing row 196 of 1444.\n",
      "Processing row 197 of 1444.\n",
      "Processing row 198 of 1444.\n",
      "Processing row 199 of 1444.\n",
      "Processing row 200 of 1444.\n",
      "Processing row 201 of 1444.\n",
      "Processing row 202 of 1444.\n",
      "Processing row 203 of 1444.\n",
      "Processing row 204 of 1444.\n",
      "Processing row 205 of 1444.\n",
      "Processing row 206 of 1444.\n",
      "Processing row 207 of 1444.\n",
      "Processing row 208 of 1444.\n",
      "Processing row 209 of 1444.\n",
      "Processing row 210 of 1444.\n",
      "Processing row 211 of 1444.\n",
      "Processing row 212 of 1444.\n",
      "Processing row 213 of 1444.\n",
      "Processing row 214 of 1444.\n",
      "Processing row 215 of 1444.\n",
      "Processing row 216 of 1444.\n",
      "Processing row 217 of 1444.\n",
      "Processing row 218 of 1444.\n",
      "Processing row 219 of 1444.\n",
      "Processing row 220 of 1444.\n",
      "Processing row 221 of 1444.\n",
      "Processing row 222 of 1444.\n",
      "Processing row 223 of 1444.\n",
      "Processing row 224 of 1444.\n",
      "Processing row 225 of 1444.\n",
      "Processing row 226 of 1444.\n",
      "Processing row 227 of 1444.\n",
      "Processing row 228 of 1444.\n",
      "Processing row 229 of 1444.\n",
      "Processing row 230 of 1444.\n",
      "Processing row 231 of 1444.\n",
      "Processing row 232 of 1444.\n",
      "Processing row 233 of 1444.\n",
      "Processing row 234 of 1444.\n",
      "Processing row 235 of 1444.\n",
      "Processing row 236 of 1444.\n",
      "Processing row 237 of 1444.\n",
      "Processing row 238 of 1444.\n",
      "Processing row 239 of 1444.\n",
      "Processing row 240 of 1444.\n",
      "Processing row 241 of 1444.\n",
      "Processing row 242 of 1444.\n",
      "Processing row 243 of 1444.\n",
      "Processing row 244 of 1444.\n",
      "Processing row 245 of 1444.\n",
      "Processing row 246 of 1444.\n",
      "Processing row 247 of 1444.\n",
      "Processing row 248 of 1444.\n",
      "Processing row 249 of 1444.\n",
      "Processing row 250 of 1444.\n",
      "Processing row 251 of 1444.\n",
      "Processing row 252 of 1444.\n",
      "Processing row 253 of 1444.\n",
      "Processing row 254 of 1444.\n",
      "Processing row 255 of 1444.\n",
      "Processing row 256 of 1444.\n",
      "Processing row 257 of 1444.\n",
      "Processing row 258 of 1444.\n",
      "Processing row 259 of 1444.\n",
      "Processing row 260 of 1444.\n",
      "Processing row 261 of 1444.\n",
      "Processing row 262 of 1444.\n",
      "Processing row 263 of 1444.\n",
      "Processing row 264 of 1444.\n",
      "Processing row 265 of 1444.\n",
      "Processing row 266 of 1444.\n",
      "Processing row 267 of 1444.\n",
      "Processing row 268 of 1444.\n",
      "Processing row 269 of 1444.\n",
      "Processing row 270 of 1444.\n",
      "Processing row 271 of 1444.\n",
      "Processing row 272 of 1444.\n",
      "Processing row 273 of 1444.\n",
      "Processing row 274 of 1444.\n",
      "Processing row 275 of 1444.\n",
      "Processing row 276 of 1444.\n",
      "Processing row 277 of 1444.\n",
      "Processing row 278 of 1444.\n",
      "Processing row 279 of 1444.\n",
      "Processing row 280 of 1444.\n",
      "Processing row 281 of 1444.\n",
      "Processing row 282 of 1444.\n",
      "Processing row 283 of 1444.\n",
      "Processing row 284 of 1444.\n",
      "Processing row 285 of 1444.\n",
      "Processing row 286 of 1444.\n",
      "Processing row 287 of 1444.\n",
      "Processing row 288 of 1444.\n",
      "Processing row 289 of 1444.\n",
      "Processing row 290 of 1444.\n",
      "Processing row 291 of 1444.\n",
      "Processing row 292 of 1444.\n",
      "Processing row 293 of 1444.\n",
      "Processing row 294 of 1444.\n",
      "Processing row 295 of 1444.\n",
      "Processing row 296 of 1444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 297 of 1444.\n",
      "Processing row 298 of 1444.\n",
      "Processing row 299 of 1444.\n",
      "Processing row 300 of 1444.\n",
      "Processing row 301 of 1444.\n",
      "Processing row 302 of 1444.\n",
      "Processing row 303 of 1444.\n",
      "Processing row 304 of 1444.\n",
      "Processing row 305 of 1444.\n",
      "Processing row 306 of 1444.\n",
      "Processing row 307 of 1444.\n",
      "Processing row 308 of 1444.\n",
      "Processing row 309 of 1444.\n",
      "Processing row 310 of 1444.\n",
      "Processing row 311 of 1444.\n",
      "Processing row 312 of 1444.\n",
      "Processing row 313 of 1444.\n",
      "Processing row 314 of 1444.\n",
      "Processing row 315 of 1444.\n",
      "Processing row 316 of 1444.\n",
      "Processing row 317 of 1444.\n",
      "Processing row 318 of 1444.\n",
      "Processing row 319 of 1444.\n",
      "Processing row 320 of 1444.\n",
      "Processing row 321 of 1444.\n",
      "Processing row 322 of 1444.\n",
      "Processing row 323 of 1444.\n",
      "Processing row 324 of 1444.\n",
      "Processing row 325 of 1444.\n",
      "Processing row 326 of 1444.\n",
      "Processing row 327 of 1444.\n",
      "Processing row 328 of 1444.\n",
      "Processing row 329 of 1444.\n",
      "Processing row 330 of 1444.\n",
      "Processing row 331 of 1444.\n",
      "Processing row 332 of 1444.\n",
      "Processing row 333 of 1444.\n",
      "Processing row 334 of 1444.\n",
      "Processing row 335 of 1444.\n",
      "Processing row 336 of 1444.\n",
      "Processing row 337 of 1444.\n",
      "Processing row 338 of 1444.\n",
      "Processing row 339 of 1444.\n",
      "Processing row 340 of 1444.\n",
      "Processing row 341 of 1444.\n",
      "Processing row 342 of 1444.\n",
      "Processing row 343 of 1444.\n",
      "Processing row 344 of 1444.\n",
      "Processing row 345 of 1444.\n",
      "Processing row 346 of 1444.\n",
      "Processing row 347 of 1444.\n",
      "Processing row 348 of 1444.\n",
      "Processing row 349 of 1444.\n",
      "Processing row 350 of 1444.\n",
      "Processing row 351 of 1444.\n",
      "Processing row 352 of 1444.\n",
      "Processing row 353 of 1444.\n",
      "Processing row 354 of 1444.\n",
      "Processing row 355 of 1444.\n",
      "Processing row 356 of 1444.\n",
      "Processing row 357 of 1444.\n",
      "Processing row 358 of 1444.\n",
      "Processing row 359 of 1444.\n",
      "Processing row 360 of 1444.\n",
      "Processing row 361 of 1444.\n",
      "Processing row 362 of 1444.\n",
      "Processing row 363 of 1444.\n",
      "Processing row 364 of 1444.\n",
      "Processing row 365 of 1444.\n",
      "Processing row 366 of 1444.\n",
      "Processing row 367 of 1444.\n",
      "Processing row 368 of 1444.\n",
      "Processing row 369 of 1444.\n",
      "Processing row 370 of 1444.\n",
      "Processing row 371 of 1444.\n",
      "Processing row 372 of 1444.\n",
      "Processing row 373 of 1444.\n",
      "Processing row 374 of 1444.\n",
      "Processing row 375 of 1444.\n",
      "Processing row 376 of 1444.\n",
      "Processing row 377 of 1444.\n",
      "Processing row 378 of 1444.\n",
      "Processing row 379 of 1444.\n",
      "Processing row 380 of 1444.\n",
      "Processing row 381 of 1444.\n",
      "Processing row 382 of 1444.\n",
      "Processing row 383 of 1444.\n",
      "Processing row 384 of 1444.\n",
      "Processing row 385 of 1444.\n",
      "Processing row 386 of 1444.\n",
      "Processing row 387 of 1444.\n",
      "Processing row 388 of 1444.\n",
      "Processing row 389 of 1444.\n",
      "Processing row 390 of 1444.\n",
      "Processing row 391 of 1444.\n",
      "Processing row 392 of 1444.\n",
      "Processing row 393 of 1444.\n",
      "Processing row 394 of 1444.\n",
      "Processing row 395 of 1444.\n",
      "Processing row 396 of 1444.\n",
      "Processing row 397 of 1444.\n",
      "Processing row 398 of 1444.\n",
      "Processing row 399 of 1444.\n",
      "Processing row 400 of 1444.\n",
      "Processing row 401 of 1444.\n",
      "Processing row 402 of 1444.\n",
      "Processing row 403 of 1444.\n",
      "Processing row 404 of 1444.\n",
      "Processing row 405 of 1444.\n",
      "Processing row 406 of 1444.\n",
      "Processing row 407 of 1444.\n",
      "Processing row 408 of 1444.\n",
      "Processing row 409 of 1444.\n",
      "Processing row 410 of 1444.\n",
      "Processing row 411 of 1444.\n",
      "Processing row 412 of 1444.\n",
      "Processing row 413 of 1444.\n",
      "Processing row 414 of 1444.\n",
      "Processing row 415 of 1444.\n",
      "Processing row 416 of 1444.\n",
      "Processing row 417 of 1444.\n",
      "Processing row 418 of 1444.\n",
      "Processing row 419 of 1444.\n",
      "Processing row 420 of 1444.\n",
      "Processing row 421 of 1444.\n",
      "Processing row 422 of 1444.\n",
      "Processing row 423 of 1444.\n",
      "Processing row 424 of 1444.\n",
      "Processing row 425 of 1444.\n",
      "Processing row 426 of 1444.\n",
      "Processing row 427 of 1444.\n",
      "Processing row 428 of 1444.\n",
      "Processing row 429 of 1444.\n",
      "Processing row 430 of 1444.\n",
      "Processing row 431 of 1444.\n",
      "Processing row 432 of 1444.\n",
      "Processing row 433 of 1444.\n",
      "Processing row 434 of 1444.\n",
      "Processing row 435 of 1444.\n",
      "Processing row 436 of 1444.\n",
      "Processing row 437 of 1444.\n",
      "Processing row 438 of 1444.\n",
      "Processing row 439 of 1444.\n",
      "Processing row 440 of 1444.\n",
      "Processing row 441 of 1444.\n",
      "Processing row 442 of 1444.\n",
      "Processing row 443 of 1444.\n",
      "Processing row 444 of 1444.\n",
      "Processing row 445 of 1444.\n",
      "Processing row 446 of 1444.\n",
      "Processing row 447 of 1444.\n",
      "Processing row 448 of 1444.\n",
      "Processing row 449 of 1444.\n",
      "Processing row 450 of 1444.\n",
      "Processing row 451 of 1444.\n",
      "Processing row 452 of 1444.\n",
      "Processing row 453 of 1444.\n",
      "Processing row 454 of 1444.\n",
      "Processing row 455 of 1444.\n",
      "Processing row 456 of 1444.\n",
      "Processing row 457 of 1444.\n",
      "Processing row 458 of 1444.\n",
      "Processing row 459 of 1444.\n",
      "Processing row 460 of 1444.\n",
      "Processing row 461 of 1444.\n",
      "Processing row 462 of 1444.\n",
      "Processing row 463 of 1444.\n",
      "Processing row 464 of 1444.\n",
      "Processing row 465 of 1444.\n",
      "Processing row 466 of 1444.\n",
      "Processing row 467 of 1444.\n",
      "Processing row 468 of 1444.\n",
      "Processing row 469 of 1444.\n",
      "Processing row 470 of 1444.\n",
      "Processing row 471 of 1444.\n",
      "Processing row 472 of 1444.\n",
      "Processing row 473 of 1444.\n",
      "Processing row 474 of 1444.\n",
      "Processing row 475 of 1444.\n",
      "Processing row 476 of 1444.\n",
      "Processing row 477 of 1444.\n",
      "Processing row 478 of 1444.\n",
      "Processing row 479 of 1444.\n",
      "Processing row 480 of 1444.\n",
      "Processing row 481 of 1444.\n",
      "Processing row 482 of 1444.\n",
      "Processing row 483 of 1444.\n",
      "Processing row 484 of 1444.\n",
      "Processing row 485 of 1444.\n",
      "Processing row 486 of 1444.\n",
      "Processing row 487 of 1444.\n",
      "Processing row 488 of 1444.\n",
      "Processing row 489 of 1444.\n",
      "Processing row 490 of 1444.\n",
      "Processing row 491 of 1444.\n",
      "Processing row 492 of 1444.\n",
      "Processing row 493 of 1444.\n",
      "Processing row 494 of 1444.\n",
      "Processing row 495 of 1444.\n",
      "Processing row 496 of 1444.\n",
      "Processing row 497 of 1444.\n",
      "Processing row 498 of 1444.\n",
      "Processing row 499 of 1444.\n",
      "Processing row 500 of 1444.\n",
      "Processing row 501 of 1444.\n",
      "Processing row 502 of 1444.\n",
      "Processing row 503 of 1444.\n",
      "Processing row 504 of 1444.\n",
      "Processing row 505 of 1444.\n",
      "Processing row 506 of 1444.\n",
      "Processing row 507 of 1444.\n",
      "Processing row 508 of 1444.\n",
      "Processing row 509 of 1444.\n",
      "Processing row 510 of 1444.\n",
      "Processing row 511 of 1444.\n",
      "Processing row 512 of 1444.\n",
      "Processing row 513 of 1444.\n",
      "Processing row 514 of 1444.\n",
      "Processing row 515 of 1444.\n",
      "Processing row 516 of 1444.\n",
      "Processing row 517 of 1444.\n",
      "Processing row 518 of 1444.\n",
      "Processing row 519 of 1444.\n",
      "Processing row 520 of 1444.\n",
      "Processing row 521 of 1444.\n",
      "Processing row 522 of 1444.\n",
      "Processing row 523 of 1444.\n",
      "Processing row 524 of 1444.\n",
      "Processing row 525 of 1444.\n",
      "Processing row 526 of 1444.\n",
      "Processing row 527 of 1444.\n",
      "Processing row 528 of 1444.\n",
      "Processing row 529 of 1444.\n",
      "Processing row 530 of 1444.\n",
      "Processing row 531 of 1444.\n",
      "Processing row 532 of 1444.\n",
      "Processing row 533 of 1444.\n",
      "Processing row 534 of 1444.\n",
      "Processing row 535 of 1444.\n",
      "Processing row 536 of 1444.\n",
      "Processing row 537 of 1444.\n",
      "Processing row 538 of 1444.\n",
      "Processing row 539 of 1444.\n",
      "Processing row 540 of 1444.\n",
      "Processing row 541 of 1444.\n",
      "Processing row 542 of 1444.\n",
      "Processing row 543 of 1444.\n",
      "Processing row 544 of 1444.\n",
      "Processing row 545 of 1444.\n",
      "Processing row 546 of 1444.\n",
      "Processing row 547 of 1444.\n",
      "Processing row 548 of 1444.\n",
      "Processing row 549 of 1444.\n",
      "Processing row 550 of 1444.\n",
      "Processing row 551 of 1444.\n",
      "Processing row 552 of 1444.\n",
      "Processing row 553 of 1444.\n",
      "Processing row 554 of 1444.\n",
      "Processing row 555 of 1444.\n",
      "Processing row 556 of 1444.\n",
      "Processing row 557 of 1444.\n",
      "Processing row 558 of 1444.\n",
      "Processing row 559 of 1444.\n",
      "Processing row 560 of 1444.\n",
      "Processing row 561 of 1444.\n",
      "Processing row 562 of 1444.\n",
      "Processing row 563 of 1444.\n",
      "Processing row 564 of 1444.\n",
      "Processing row 565 of 1444.\n",
      "Processing row 566 of 1444.\n",
      "Processing row 567 of 1444.\n",
      "Processing row 568 of 1444.\n",
      "Processing row 569 of 1444.\n",
      "Processing row 570 of 1444.\n",
      "Processing row 571 of 1444.\n",
      "Processing row 572 of 1444.\n",
      "Processing row 573 of 1444.\n",
      "Processing row 574 of 1444.\n",
      "Processing row 575 of 1444.\n",
      "Processing row 576 of 1444.\n",
      "Processing row 577 of 1444.\n",
      "Processing row 578 of 1444.\n",
      "Processing row 579 of 1444.\n",
      "Processing row 580 of 1444.\n",
      "Processing row 581 of 1444.\n",
      "Processing row 582 of 1444.\n",
      "Processing row 583 of 1444.\n",
      "Processing row 584 of 1444.\n",
      "Processing row 585 of 1444.\n",
      "Processing row 586 of 1444.\n",
      "Processing row 587 of 1444.\n",
      "Processing row 588 of 1444.\n",
      "Processing row 589 of 1444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 590 of 1444.\n",
      "Processing row 591 of 1444.\n",
      "Processing row 592 of 1444.\n",
      "Processing row 593 of 1444.\n",
      "Processing row 594 of 1444.\n",
      "Processing row 595 of 1444.\n",
      "Processing row 596 of 1444.\n",
      "Processing row 597 of 1444.\n",
      "Processing row 598 of 1444.\n",
      "Processing row 599 of 1444.\n",
      "Processing row 600 of 1444.\n",
      "Processing row 601 of 1444.\n",
      "Processing row 602 of 1444.\n",
      "Processing row 603 of 1444.\n",
      "Processing row 604 of 1444.\n",
      "Processing row 605 of 1444.\n",
      "Processing row 606 of 1444.\n",
      "Processing row 607 of 1444.\n",
      "Processing row 608 of 1444.\n",
      "Processing row 609 of 1444.\n",
      "Processing row 610 of 1444.\n",
      "Processing row 611 of 1444.\n",
      "Processing row 612 of 1444.\n",
      "Processing row 613 of 1444.\n",
      "Processing row 614 of 1444.\n",
      "Processing row 615 of 1444.\n",
      "Processing row 616 of 1444.\n",
      "Processing row 617 of 1444.\n",
      "Processing row 618 of 1444.\n",
      "Processing row 619 of 1444.\n",
      "Processing row 620 of 1444.\n",
      "Processing row 621 of 1444.\n",
      "Processing row 622 of 1444.\n",
      "Processing row 623 of 1444.\n",
      "Processing row 624 of 1444.\n",
      "Processing row 625 of 1444.\n",
      "Processing row 626 of 1444.\n",
      "Processing row 627 of 1444.\n",
      "Processing row 628 of 1444.\n",
      "Processing row 629 of 1444.\n",
      "Processing row 630 of 1444.\n",
      "Processing row 631 of 1444.\n",
      "Processing row 632 of 1444.\n",
      "Processing row 633 of 1444.\n",
      "Processing row 634 of 1444.\n",
      "Processing row 635 of 1444.\n",
      "Processing row 636 of 1444.\n",
      "Processing row 637 of 1444.\n",
      "Processing row 638 of 1444.\n",
      "Processing row 639 of 1444.\n",
      "Processing row 640 of 1444.\n",
      "Processing row 641 of 1444.\n",
      "Processing row 642 of 1444.\n",
      "Processing row 643 of 1444.\n",
      "Processing row 644 of 1444.\n",
      "Processing row 645 of 1444.\n",
      "Processing row 646 of 1444.\n",
      "Processing row 647 of 1444.\n",
      "Processing row 648 of 1444.\n",
      "Processing row 649 of 1444.\n",
      "Processing row 650 of 1444.\n",
      "Processing row 651 of 1444.\n",
      "Processing row 652 of 1444.\n",
      "Processing row 653 of 1444.\n",
      "Processing row 654 of 1444.\n",
      "Processing row 655 of 1444.\n",
      "Processing row 656 of 1444.\n",
      "Processing row 657 of 1444.\n",
      "Processing row 658 of 1444.\n",
      "Processing row 659 of 1444.\n",
      "Processing row 660 of 1444.\n",
      "Processing row 661 of 1444.\n",
      "Processing row 662 of 1444.\n",
      "Processing row 663 of 1444.\n",
      "Processing row 664 of 1444.\n",
      "Processing row 665 of 1444.\n",
      "Processing row 666 of 1444.\n",
      "Processing row 667 of 1444.\n",
      "Processing row 668 of 1444.\n",
      "Processing row 669 of 1444.\n",
      "Processing row 670 of 1444.\n",
      "Processing row 671 of 1444.\n",
      "Processing row 672 of 1444.\n",
      "Processing row 673 of 1444.\n",
      "Processing row 674 of 1444.\n",
      "Processing row 675 of 1444.\n",
      "Processing row 676 of 1444.\n",
      "Processing row 677 of 1444.\n",
      "Processing row 678 of 1444.\n",
      "Processing row 679 of 1444.\n",
      "Processing row 680 of 1444.\n",
      "Processing row 681 of 1444.\n",
      "Processing row 682 of 1444.\n",
      "Processing row 683 of 1444.\n",
      "Processing row 684 of 1444.\n",
      "Processing row 685 of 1444.\n",
      "Processing row 686 of 1444.\n",
      "Processing row 687 of 1444.\n",
      "Processing row 688 of 1444.\n",
      "Processing row 689 of 1444.\n",
      "Processing row 690 of 1444.\n",
      "Processing row 691 of 1444.\n",
      "Processing row 692 of 1444.\n",
      "Processing row 693 of 1444.\n",
      "Processing row 694 of 1444.\n",
      "Processing row 695 of 1444.\n",
      "Processing row 696 of 1444.\n",
      "Processing row 697 of 1444.\n",
      "Processing row 698 of 1444.\n",
      "Processing row 699 of 1444.\n",
      "Processing row 700 of 1444.\n",
      "Processing row 701 of 1444.\n",
      "Processing row 702 of 1444.\n",
      "Processing row 703 of 1444.\n",
      "Processing row 704 of 1444.\n",
      "Processing row 705 of 1444.\n",
      "Processing row 706 of 1444.\n",
      "Processing row 707 of 1444.\n",
      "Processing row 708 of 1444.\n",
      "Processing row 709 of 1444.\n",
      "Processing row 710 of 1444.\n",
      "Processing row 711 of 1444.\n",
      "Processing row 712 of 1444.\n",
      "Processing row 713 of 1444.\n",
      "Processing row 714 of 1444.\n",
      "Processing row 715 of 1444.\n",
      "Processing row 716 of 1444.\n",
      "Processing row 717 of 1444.\n",
      "Processing row 718 of 1444.\n",
      "Processing row 719 of 1444.\n",
      "Processing row 720 of 1444.\n",
      "Processing row 721 of 1444.\n",
      "Processing row 722 of 1444.\n",
      "Processing row 723 of 1444.\n",
      "Processing row 724 of 1444.\n",
      "Processing row 725 of 1444.\n",
      "Processing row 726 of 1444.\n",
      "Processing row 727 of 1444.\n",
      "Processing row 728 of 1444.\n",
      "Processing row 729 of 1444.\n",
      "Processing row 730 of 1444.\n",
      "Processing row 731 of 1444.\n",
      "Processing row 732 of 1444.\n",
      "Processing row 733 of 1444.\n",
      "Processing row 734 of 1444.\n",
      "Processing row 735 of 1444.\n",
      "Processing row 736 of 1444.\n",
      "Processing row 737 of 1444.\n",
      "Processing row 738 of 1444.\n",
      "Processing row 739 of 1444.\n",
      "Processing row 740 of 1444.\n",
      "Processing row 741 of 1444.\n",
      "Processing row 742 of 1444.\n",
      "Processing row 743 of 1444.\n",
      "Processing row 744 of 1444.\n",
      "Processing row 745 of 1444.\n",
      "Processing row 746 of 1444.\n",
      "Processing row 747 of 1444.\n",
      "Processing row 748 of 1444.\n",
      "Processing row 749 of 1444.\n",
      "Processing row 750 of 1444.\n",
      "Processing row 751 of 1444.\n",
      "Processing row 752 of 1444.\n",
      "Processing row 753 of 1444.\n",
      "Processing row 754 of 1444.\n",
      "Processing row 755 of 1444.\n",
      "Processing row 756 of 1444.\n",
      "Processing row 757 of 1444.\n",
      "Processing row 758 of 1444.\n",
      "Processing row 759 of 1444.\n",
      "Processing row 760 of 1444.\n",
      "Processing row 761 of 1444.\n",
      "Processing row 762 of 1444.\n",
      "Processing row 763 of 1444.\n",
      "Processing row 764 of 1444.\n",
      "Processing row 765 of 1444.\n",
      "Processing row 766 of 1444.\n",
      "Processing row 767 of 1444.\n",
      "Processing row 768 of 1444.\n",
      "Processing row 769 of 1444.\n",
      "Processing row 770 of 1444.\n",
      "Processing row 771 of 1444.\n",
      "Processing row 772 of 1444.\n",
      "Processing row 773 of 1444.\n",
      "Processing row 774 of 1444.\n",
      "Processing row 775 of 1444.\n",
      "Processing row 776 of 1444.\n",
      "Processing row 777 of 1444.\n",
      "Processing row 778 of 1444.\n",
      "Processing row 779 of 1444.\n",
      "Processing row 780 of 1444.\n",
      "Processing row 781 of 1444.\n",
      "Processing row 782 of 1444.\n",
      "Processing row 783 of 1444.\n",
      "Processing row 784 of 1444.\n",
      "Processing row 785 of 1444.\n",
      "Processing row 786 of 1444.\n",
      "Processing row 787 of 1444.\n",
      "Processing row 788 of 1444.\n",
      "Processing row 789 of 1444.\n",
      "Processing row 790 of 1444.\n",
      "Processing row 791 of 1444.\n",
      "Processing row 792 of 1444.\n",
      "Processing row 793 of 1444.\n",
      "Processing row 794 of 1444.\n",
      "Processing row 795 of 1444.\n",
      "Processing row 796 of 1444.\n",
      "Processing row 797 of 1444.\n",
      "Processing row 798 of 1444.\n",
      "Processing row 799 of 1444.\n",
      "Processing row 800 of 1444.\n",
      "Processing row 801 of 1444.\n",
      "Processing row 802 of 1444.\n",
      "Processing row 803 of 1444.\n",
      "Processing row 804 of 1444.\n",
      "Processing row 805 of 1444.\n",
      "Processing row 806 of 1444.\n",
      "Processing row 807 of 1444.\n",
      "Processing row 808 of 1444.\n",
      "Processing row 809 of 1444.\n",
      "Processing row 810 of 1444.\n",
      "Processing row 811 of 1444.\n",
      "Processing row 812 of 1444.\n",
      "Processing row 813 of 1444.\n",
      "Processing row 814 of 1444.\n",
      "Processing row 815 of 1444.\n",
      "Processing row 816 of 1444.\n",
      "Processing row 817 of 1444.\n",
      "Processing row 818 of 1444.\n",
      "Processing row 819 of 1444.\n",
      "Processing row 820 of 1444.\n",
      "Processing row 821 of 1444.\n",
      "Processing row 822 of 1444.\n",
      "Processing row 823 of 1444.\n",
      "Processing row 824 of 1444.\n",
      "Processing row 825 of 1444.\n",
      "Processing row 826 of 1444.\n",
      "Processing row 827 of 1444.\n",
      "Processing row 828 of 1444.\n",
      "Processing row 829 of 1444.\n",
      "Processing row 830 of 1444.\n",
      "Processing row 831 of 1444.\n",
      "Processing row 832 of 1444.\n",
      "Processing row 833 of 1444.\n",
      "Processing row 834 of 1444.\n",
      "Processing row 835 of 1444.\n",
      "Processing row 836 of 1444.\n",
      "Processing row 837 of 1444.\n",
      "Processing row 838 of 1444.\n",
      "Processing row 839 of 1444.\n",
      "Processing row 840 of 1444.\n",
      "Processing row 841 of 1444.\n",
      "Processing row 842 of 1444.\n",
      "Processing row 843 of 1444.\n",
      "Processing row 844 of 1444.\n",
      "Processing row 845 of 1444.\n",
      "Processing row 846 of 1444.\n",
      "Processing row 847 of 1444.\n",
      "Processing row 848 of 1444.\n",
      "Processing row 849 of 1444.\n",
      "Processing row 850 of 1444.\n",
      "Processing row 851 of 1444.\n",
      "Processing row 852 of 1444.\n",
      "Processing row 853 of 1444.\n",
      "Processing row 854 of 1444.\n",
      "Processing row 855 of 1444.\n",
      "Processing row 856 of 1444.\n",
      "Processing row 857 of 1444.\n",
      "Processing row 858 of 1444.\n",
      "Processing row 859 of 1444.\n",
      "Processing row 860 of 1444.\n",
      "Processing row 861 of 1444.\n",
      "Processing row 862 of 1444.\n",
      "Processing row 863 of 1444.\n",
      "Processing row 864 of 1444.\n",
      "Processing row 865 of 1444.\n",
      "Processing row 866 of 1444.\n",
      "Processing row 867 of 1444.\n",
      "Processing row 868 of 1444.\n",
      "Processing row 869 of 1444.\n",
      "Processing row 870 of 1444.\n",
      "Processing row 871 of 1444.\n",
      "Processing row 872 of 1444.\n",
      "Processing row 873 of 1444.\n",
      "Processing row 874 of 1444.\n",
      "Processing row 875 of 1444.\n",
      "Processing row 876 of 1444.\n",
      "Processing row 877 of 1444.\n",
      "Processing row 878 of 1444.\n",
      "Processing row 879 of 1444.\n",
      "Processing row 880 of 1444.\n",
      "Processing row 881 of 1444.\n",
      "Processing row 882 of 1444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 883 of 1444.\n",
      "Processing row 884 of 1444.\n",
      "Processing row 885 of 1444.\n",
      "Processing row 886 of 1444.\n",
      "Processing row 887 of 1444.\n",
      "Processing row 888 of 1444.\n",
      "Processing row 889 of 1444.\n",
      "Processing row 890 of 1444.\n",
      "Processing row 891 of 1444.\n",
      "Processing row 892 of 1444.\n",
      "Processing row 893 of 1444.\n",
      "Processing row 894 of 1444.\n",
      "Processing row 895 of 1444.\n",
      "Processing row 896 of 1444.\n",
      "Processing row 897 of 1444.\n",
      "Processing row 898 of 1444.\n",
      "Processing row 899 of 1444.\n",
      "Processing row 900 of 1444.\n",
      "Processing row 901 of 1444.\n",
      "Processing row 902 of 1444.\n",
      "Processing row 903 of 1444.\n",
      "Processing row 904 of 1444.\n",
      "Processing row 905 of 1444.\n",
      "Processing row 906 of 1444.\n",
      "Processing row 907 of 1444.\n",
      "Processing row 908 of 1444.\n",
      "Processing row 909 of 1444.\n",
      "Processing row 910 of 1444.\n",
      "Processing row 911 of 1444.\n",
      "Processing row 912 of 1444.\n",
      "Processing row 913 of 1444.\n",
      "Processing row 914 of 1444.\n",
      "Processing row 915 of 1444.\n",
      "Processing row 916 of 1444.\n",
      "Processing row 917 of 1444.\n",
      "Processing row 918 of 1444.\n",
      "Processing row 919 of 1444.\n",
      "Processing row 920 of 1444.\n",
      "Processing row 921 of 1444.\n",
      "Processing row 922 of 1444.\n",
      "Processing row 923 of 1444.\n",
      "Processing row 924 of 1444.\n",
      "Processing row 925 of 1444.\n",
      "Processing row 926 of 1444.\n",
      "Processing row 927 of 1444.\n",
      "Processing row 928 of 1444.\n",
      "Processing row 929 of 1444.\n",
      "Processing row 930 of 1444.\n",
      "Processing row 931 of 1444.\n",
      "Processing row 932 of 1444.\n",
      "Processing row 933 of 1444.\n",
      "Processing row 934 of 1444.\n",
      "Processing row 935 of 1444.\n",
      "Processing row 936 of 1444.\n",
      "Processing row 937 of 1444.\n",
      "Processing row 938 of 1444.\n",
      "Processing row 939 of 1444.\n",
      "Processing row 940 of 1444.\n",
      "Processing row 941 of 1444.\n",
      "Processing row 942 of 1444.\n",
      "Processing row 943 of 1444.\n",
      "Processing row 944 of 1444.\n",
      "Processing row 945 of 1444.\n",
      "Processing row 946 of 1444.\n",
      "Processing row 947 of 1444.\n",
      "Processing row 948 of 1444.\n",
      "Processing row 949 of 1444.\n",
      "Processing row 950 of 1444.\n",
      "Processing row 951 of 1444.\n",
      "Processing row 952 of 1444.\n",
      "Processing row 953 of 1444.\n",
      "Processing row 954 of 1444.\n",
      "Processing row 955 of 1444.\n",
      "Processing row 956 of 1444.\n",
      "Processing row 957 of 1444.\n",
      "Processing row 958 of 1444.\n",
      "Processing row 959 of 1444.\n",
      "Processing row 960 of 1444.\n",
      "Processing row 961 of 1444.\n",
      "Processing row 962 of 1444.\n",
      "Processing row 963 of 1444.\n",
      "Processing row 964 of 1444.\n",
      "Processing row 965 of 1444.\n",
      "Processing row 966 of 1444.\n",
      "Processing row 967 of 1444.\n",
      "Processing row 968 of 1444.\n",
      "Processing row 969 of 1444.\n",
      "Processing row 970 of 1444.\n",
      "Processing row 971 of 1444.\n",
      "Processing row 972 of 1444.\n",
      "Processing row 973 of 1444.\n",
      "Processing row 974 of 1444.\n",
      "Processing row 975 of 1444.\n",
      "Processing row 976 of 1444.\n",
      "Processing row 977 of 1444.\n",
      "Processing row 978 of 1444.\n",
      "Processing row 979 of 1444.\n",
      "Processing row 980 of 1444.\n",
      "Processing row 981 of 1444.\n",
      "Processing row 982 of 1444.\n",
      "Processing row 983 of 1444.\n",
      "Processing row 984 of 1444.\n",
      "Processing row 985 of 1444.\n",
      "Processing row 986 of 1444.\n",
      "Processing row 987 of 1444.\n",
      "Processing row 988 of 1444.\n",
      "Processing row 989 of 1444.\n",
      "Processing row 990 of 1444.\n",
      "Processing row 991 of 1444.\n",
      "Processing row 992 of 1444.\n",
      "Processing row 993 of 1444.\n",
      "Processing row 994 of 1444.\n",
      "Processing row 995 of 1444.\n",
      "Processing row 996 of 1444.\n",
      "Processing row 997 of 1444.\n",
      "Processing row 998 of 1444.\n",
      "Processing row 999 of 1444.\n",
      "Processing row 1000 of 1444.\n",
      "Processing row 1001 of 1444.\n",
      "Processing row 1002 of 1444.\n",
      "Processing row 1003 of 1444.\n",
      "Processing row 1004 of 1444.\n",
      "Processing row 1005 of 1444.\n",
      "Processing row 1006 of 1444.\n",
      "Processing row 1007 of 1444.\n",
      "Processing row 1008 of 1444.\n",
      "Processing row 1009 of 1444.\n",
      "Processing row 1010 of 1444.\n",
      "Processing row 1011 of 1444.\n",
      "Processing row 1012 of 1444.\n",
      "Processing row 1013 of 1444.\n",
      "Processing row 1014 of 1444.\n",
      "Processing row 1015 of 1444.\n",
      "Processing row 1016 of 1444.\n",
      "Processing row 1017 of 1444.\n",
      "Processing row 1018 of 1444.\n",
      "Processing row 1019 of 1444.\n",
      "Processing row 1020 of 1444.\n",
      "Processing row 1021 of 1444.\n",
      "Processing row 1022 of 1444.\n",
      "Processing row 1023 of 1444.\n",
      "Processing row 1024 of 1444.\n",
      "Processing row 1025 of 1444.\n",
      "Processing row 1026 of 1444.\n",
      "Processing row 1027 of 1444.\n",
      "Processing row 1028 of 1444.\n",
      "Processing row 1029 of 1444.\n",
      "Processing row 1030 of 1444.\n",
      "Processing row 1031 of 1444.\n",
      "Processing row 1032 of 1444.\n",
      "Processing row 1033 of 1444.\n",
      "Processing row 1034 of 1444.\n",
      "Processing row 1035 of 1444.\n",
      "Processing row 1036 of 1444.\n",
      "Processing row 1037 of 1444.\n",
      "Processing row 1038 of 1444.\n",
      "Processing row 1039 of 1444.\n",
      "Processing row 1040 of 1444.\n",
      "Processing row 1041 of 1444.\n",
      "Processing row 1042 of 1444.\n",
      "Processing row 1043 of 1444.\n",
      "Processing row 1044 of 1444.\n",
      "Processing row 1045 of 1444.\n",
      "Processing row 1046 of 1444.\n",
      "Processing row 1047 of 1444.\n",
      "Processing row 1048 of 1444.\n",
      "Processing row 1049 of 1444.\n",
      "Processing row 1050 of 1444.\n",
      "Processing row 1051 of 1444.\n",
      "Processing row 1052 of 1444.\n",
      "Processing row 1053 of 1444.\n",
      "Processing row 1054 of 1444.\n",
      "Processing row 1055 of 1444.\n",
      "Processing row 1056 of 1444.\n",
      "Processing row 1057 of 1444.\n",
      "Processing row 1058 of 1444.\n",
      "Processing row 1059 of 1444.\n",
      "Processing row 1060 of 1444.\n",
      "Processing row 1061 of 1444.\n",
      "Processing row 1062 of 1444.\n",
      "Processing row 1063 of 1444.\n",
      "Processing row 1064 of 1444.\n",
      "Processing row 1065 of 1444.\n",
      "Processing row 1066 of 1444.\n",
      "Processing row 1067 of 1444.\n",
      "Processing row 1068 of 1444.\n",
      "Processing row 1069 of 1444.\n",
      "Processing row 1070 of 1444.\n",
      "Processing row 1071 of 1444.\n",
      "Processing row 1072 of 1444.\n",
      "Processing row 1073 of 1444.\n",
      "Processing row 1074 of 1444.\n",
      "Processing row 1075 of 1444.\n",
      "Processing row 1077 of 1444.\n",
      "Processing row 1078 of 1444.\n",
      "Processing row 1079 of 1444.\n",
      "Processing row 1080 of 1444.\n",
      "Processing row 1083 of 1444.\n",
      "Processing row 1084 of 1444.\n",
      "Processing row 1085 of 1444.\n",
      "Processing row 1086 of 1444.\n",
      "Processing row 1089 of 1444.\n",
      "Processing row 1091 of 1444.\n",
      "Processing row 1092 of 1444.\n",
      "Processing row 1097 of 1444.\n",
      "Processing row 1101 of 1444.\n",
      "Processing row 1106 of 1444.\n",
      "Processing row 1108 of 1444.\n",
      "Processing row 1112 of 1444.\n",
      "Processing row 1113 of 1444.\n",
      "Processing row 1114 of 1444.\n",
      "Processing row 1116 of 1444.\n",
      "Processing row 1119 of 1444.\n",
      "Processing row 1120 of 1444.\n",
      "Processing row 1121 of 1444.\n",
      "Processing row 1122 of 1444.\n",
      "Processing row 1123 of 1444.\n",
      "Processing row 1124 of 1444.\n",
      "Processing row 1125 of 1444.\n",
      "Processing row 1127 of 1444.\n",
      "Processing row 1128 of 1444.\n",
      "Processing row 1130 of 1444.\n",
      "Processing row 1134 of 1444.\n",
      "Processing row 1135 of 1444.\n",
      "Processing row 1137 of 1444.\n",
      "Processing row 1138 of 1444.\n",
      "Processing row 1139 of 1444.\n",
      "Processing row 1140 of 1444.\n",
      "Processing row 1141 of 1444.\n",
      "Processing row 1142 of 1444.\n",
      "Processing row 1143 of 1444.\n",
      "Processing row 1144 of 1444.\n",
      "Processing row 1145 of 1444.\n",
      "Processing row 1146 of 1444.\n",
      "Processing row 1147 of 1444.\n",
      "Processing row 1148 of 1444.\n",
      "Processing row 1149 of 1444.\n",
      "Processing row 1150 of 1444.\n",
      "Processing row 1151 of 1444.\n",
      "Processing row 1152 of 1444.\n",
      "Processing row 1153 of 1444.\n",
      "Processing row 1154 of 1444.\n",
      "Processing row 1155 of 1444.\n",
      "Processing row 1156 of 1444.\n",
      "Processing row 1157 of 1444.\n",
      "Processing row 1158 of 1444.\n",
      "Processing row 1159 of 1444.\n",
      "Processing row 1160 of 1444.\n",
      "Processing row 1161 of 1444.\n",
      "Processing row 1162 of 1444.\n",
      "Processing row 1163 of 1444.\n",
      "Processing row 1164 of 1444.\n",
      "Processing row 1165 of 1444.\n",
      "Processing row 1166 of 1444.\n",
      "Processing row 1167 of 1444.\n",
      "Processing row 1168 of 1444.\n",
      "Processing row 1169 of 1444.\n",
      "Processing row 1170 of 1444.\n",
      "Processing row 1171 of 1444.\n",
      "Processing row 1172 of 1444.\n",
      "Processing row 1173 of 1444.\n",
      "Processing row 1174 of 1444.\n",
      "Processing row 1175 of 1444.\n",
      "Processing row 1176 of 1444.\n",
      "Processing row 1177 of 1444.\n",
      "Processing row 1178 of 1444.\n",
      "Processing row 1179 of 1444.\n",
      "Processing row 1180 of 1444.\n",
      "Processing row 1181 of 1444.\n",
      "Processing row 1183 of 1444.\n",
      "Processing row 1184 of 1444.\n",
      "Processing row 1185 of 1444.\n",
      "Processing row 1186 of 1444.\n",
      "Processing row 1187 of 1444.\n",
      "Processing row 1188 of 1444.\n",
      "Processing row 1189 of 1444.\n",
      "Processing row 1190 of 1444.\n",
      "Processing row 1191 of 1444.\n",
      "Processing row 1192 of 1444.\n",
      "Processing row 1195 of 1444.\n",
      "Processing row 1196 of 1444.\n",
      "Processing row 1197 of 1444.\n",
      "Processing row 1198 of 1444.\n",
      "Processing row 1199 of 1444.\n",
      "Processing row 1200 of 1444.\n",
      "Processing row 1202 of 1444.\n",
      "Processing row 1203 of 1444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 1206 of 1444.\n",
      "Processing row 1208 of 1444.\n",
      "Processing row 1210 of 1444.\n",
      "Processing row 1219 of 1444.\n",
      "Processing row 1221 of 1444.\n",
      "Processing row 1225 of 1444.\n",
      "Processing row 1226 of 1444.\n",
      "Processing row 1227 of 1444.\n",
      "Processing row 1228 of 1444.\n",
      "Processing row 1230 of 1444.\n",
      "Processing row 1232 of 1444.\n",
      "Processing row 1233 of 1444.\n",
      "Processing row 1234 of 1444.\n",
      "Processing row 1235 of 1444.\n",
      "Processing row 1237 of 1444.\n",
      "Processing row 1244 of 1444.\n",
      "Processing row 1247 of 1444.\n",
      "Processing row 1251 of 1444.\n",
      "Processing row 1252 of 1444.\n",
      "Processing row 1253 of 1444.\n",
      "Processing row 1254 of 1444.\n",
      "Processing row 1257 of 1444.\n",
      "Processing row 1260 of 1444.\n",
      "Processing row 1261 of 1444.\n",
      "Processing row 1263 of 1444.\n",
      "Processing row 1266 of 1444.\n",
      "Processing row 1267 of 1444.\n",
      "Processing row 1271 of 1444.\n",
      "Processing row 1273 of 1444.\n",
      "Processing row 1274 of 1444.\n",
      "Processing row 1276 of 1444.\n",
      "Processing row 1277 of 1444.\n",
      "Processing row 1278 of 1444.\n",
      "Processing row 1279 of 1444.\n",
      "Processing row 1280 of 1444.\n",
      "Processing row 1281 of 1444.\n",
      "Processing row 1282 of 1444.\n",
      "Processing row 1283 of 1444.\n",
      "Processing row 1285 of 1444.\n",
      "Processing row 1286 of 1444.\n",
      "Processing row 1289 of 1444.\n",
      "Processing row 1290 of 1444.\n",
      "Processing row 1291 of 1444.\n",
      "Processing row 1292 of 1444.\n",
      "Processing row 1293 of 1444.\n",
      "Processing row 1294 of 1444.\n",
      "Processing row 1295 of 1444.\n",
      "Processing row 1296 of 1444.\n",
      "Processing row 1297 of 1444.\n",
      "Processing row 1298 of 1444.\n",
      "Processing row 1299 of 1444.\n",
      "Processing row 1303 of 1444.\n",
      "Processing row 1304 of 1444.\n",
      "Processing row 1305 of 1444.\n",
      "Processing row 1306 of 1444.\n",
      "Processing row 1307 of 1444.\n",
      "Processing row 1308 of 1444.\n",
      "Processing row 1313 of 1444.\n",
      "Processing row 1314 of 1444.\n",
      "Processing row 1315 of 1444.\n",
      "Processing row 1317 of 1444.\n",
      "Processing row 1318 of 1444.\n",
      "Processing row 1319 of 1444.\n",
      "Processing row 1320 of 1444.\n",
      "Processing row 1321 of 1444.\n",
      "Processing row 1324 of 1444.\n",
      "Processing row 1326 of 1444.\n",
      "Processing row 1328 of 1444.\n",
      "Processing row 1329 of 1444.\n",
      "Processing row 1330 of 1444.\n",
      "Processing row 1331 of 1444.\n",
      "Processing row 1332 of 1444.\n",
      "Processing row 1333 of 1444.\n",
      "Processing row 1334 of 1444.\n",
      "Processing row 1335 of 1444.\n",
      "Processing row 1336 of 1444.\n",
      "Processing row 1337 of 1444.\n",
      "Processing row 1338 of 1444.\n",
      "Processing row 1340 of 1444.\n",
      "Processing row 1343 of 1444.\n",
      "Processing row 1344 of 1444.\n",
      "Processing row 1346 of 1444.\n",
      "Processing row 1348 of 1444.\n",
      "Processing row 1349 of 1444.\n",
      "Processing row 1350 of 1444.\n",
      "Processing row 1351 of 1444.\n",
      "Processing row 1354 of 1444.\n",
      "Processing row 1355 of 1444.\n",
      "Processing row 1356 of 1444.\n",
      "Processing row 1357 of 1444.\n",
      "Processing row 1358 of 1444.\n",
      "Processing row 1359 of 1444.\n",
      "Processing row 1360 of 1444.\n",
      "Processing row 1361 of 1444.\n",
      "Processing row 1362 of 1444.\n",
      "Processing row 1363 of 1444.\n",
      "Processing row 1364 of 1444.\n",
      "Processing row 1365 of 1444.\n",
      "Processing row 1366 of 1444.\n",
      "Processing row 1367 of 1444.\n",
      "Processing row 1368 of 1444.\n",
      "Processing row 1369 of 1444.\n",
      "Processing row 1371 of 1444.\n",
      "Processing row 1372 of 1444.\n",
      "Processing row 1377 of 1444.\n",
      "Processing row 1378 of 1444.\n",
      "Processing row 1386 of 1444.\n",
      "Processing row 1387 of 1444.\n",
      "Processing row 1390 of 1444.\n",
      "Processing row 1391 of 1444.\n",
      "Processing row 1393 of 1444.\n",
      "Processing row 1394 of 1444.\n",
      "Processing row 1395 of 1444.\n",
      "Processing row 1396 of 1444.\n",
      "Processing row 1397 of 1444.\n",
      "Processing row 1398 of 1444.\n",
      "Processing row 1400 of 1444.\n",
      "Processing row 1401 of 1444.\n",
      "Processing row 1402 of 1444.\n",
      "Processing row 1403 of 1444.\n",
      "Processing row 1404 of 1444.\n",
      "Processing row 1405 of 1444.\n",
      "Processing row 1406 of 1444.\n",
      "Processing row 1407 of 1444.\n",
      "Processing row 1408 of 1444.\n",
      "Processing row 1409 of 1444.\n",
      "Processing row 1410 of 1444.\n",
      "Processing row 1411 of 1444.\n",
      "Processing row 1412 of 1444.\n",
      "Processing row 1414 of 1444.\n",
      "Processing row 1415 of 1444.\n",
      "Processing row 1416 of 1444.\n",
      "Processing row 1418 of 1444.\n",
      "Processing row 1419 of 1444.\n",
      "Processing row 1420 of 1444.\n",
      "Processing row 1421 of 1444.\n",
      "Processing row 1422 of 1444.\n",
      "Processing row 1423 of 1444.\n",
      "Processing row 1424 of 1444.\n",
      "Processing row 1425 of 1444.\n",
      "Processing row 1426 of 1444.\n",
      "Processing row 1427 of 1444.\n",
      "Processing row 1428 of 1444.\n",
      "Processing row 1429 of 1444.\n",
      "Processing row 1430 of 1444.\n",
      "Processing row 1431 of 1444.\n",
      "Processing row 1432 of 1444.\n",
      "Processing row 1433 of 1444.\n",
      "Processing row 1434 of 1444.\n",
      "Processing row 1435 of 1444.\n",
      "Processing row 1436 of 1444.\n",
      "Processing row 1437 of 1444.\n",
      "Processing row 1439 of 1444.\n",
      "Processing row 1440 of 1444.\n",
      "Processing row 1441 of 1444.\n",
      "Processing row 1442 of 1444.\n",
      "Processing row 1443 of 1444.\n",
      "Processing row 1444 of 1444.\n",
      "\n",
      "Total Time Processed:\n",
      "3155.546615123749\n"
     ]
    }
   ],
   "source": [
    "# Iterations. Update the similar score values to title_title\n",
    "\n",
    "# ATTENTION: THE ITERATION TAKES time, better to track progress\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Number of articles in title_title\n",
    "len_articles = title_title.shape[0]\n",
    "\n",
    "\n",
    "# List of index of all articles\n",
    "idx_articles = list(title_title.index)\n",
    "\n",
    "# Last idx for tracking in print\n",
    "idx_last = title_title.tail(1).index[0]\n",
    "\n",
    "# Loop thru each article: for each row, loop each columns. \n",
    "for i in idx_articles:\n",
    "    print(f\"Processing row {i} of {idx_last}.\")\n",
    "    \n",
    "    for j in idx_articles:\n",
    "        title_title.loc[i, j] = compute_title_similarity(i, j)\n",
    "        \n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTotal Time Processed:\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1434</th>\n",
       "      <th>1435</th>\n",
       "      <th>1436</th>\n",
       "      <th>1437</th>\n",
       "      <th>1439</th>\n",
       "      <th>1440</th>\n",
       "      <th>1441</th>\n",
       "      <th>1442</th>\n",
       "      <th>1443</th>\n",
       "      <th>1444</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.201993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.115216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136276</td>\n",
       "      <td>0.260556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136276</td>\n",
       "      <td>0.069973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.115216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136276</td>\n",
       "      <td>0.450176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136276</td>\n",
       "      <td>0.069973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125367</td>\n",
       "      <td>0.105992</td>\n",
       "      <td>0.130494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125367</td>\n",
       "      <td>0.064371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136276</td>\n",
       "      <td>0.136276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125367</td>\n",
       "      <td>0.170776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201993</td>\n",
       "      <td>0.170776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069973</td>\n",
       "      <td>0.069973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064371</td>\n",
       "      <td>0.087687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103716</td>\n",
       "      <td>0.087687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063601</td>\n",
       "      <td>0.054473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054473</td>\n",
       "      <td>0.078745</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1328 rows Ã 1328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0         1         2     3         4         5     6     \\\n",
       "article_id                                                             \n",
       "0            1.0  0.000000  0.000000   0.0  0.000000  0.000000   0.0   \n",
       "1            0.0  1.000000  0.201993   0.0  0.084580  0.115216   0.0   \n",
       "2            0.0  0.201993  1.000000   0.0  0.084580  0.115216   0.0   \n",
       "3            0.0  0.000000  0.000000   1.0  0.000000  0.000000   0.0   \n",
       "4            0.0  0.084580  0.084580   0.0  1.000000  0.105992   0.0   \n",
       "...          ...       ...       ...   ...       ...       ...   ...   \n",
       "1440         0.0  0.136276  0.136276   0.0  0.125367  0.170776   0.0   \n",
       "1441         0.0  0.069973  0.069973   0.0  0.064371  0.087687   0.0   \n",
       "1442         0.0  0.000000  0.000000   0.0  0.000000  0.000000   0.0   \n",
       "1443         0.0  0.000000  0.000000   0.0  0.000000  0.000000   0.0   \n",
       "1444         0.0  0.000000  0.000000   0.0  0.000000  0.000000   0.0   \n",
       "\n",
       "article_id      7         8         9     ...  1434  1435  1436  1437  1439  \\\n",
       "article_id                                ...                                 \n",
       "0           0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "1           0.136276  0.260556  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "2           0.136276  0.450176  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "3           0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "4           0.125367  0.105992  0.130494  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "...              ...       ...       ...  ...   ...   ...   ...   ...   ...   \n",
       "1440        0.201993  0.170776  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "1441        0.103716  0.087687  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "1442        0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "1443        0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "1444        0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "article_id      1440      1441      1442      1443      1444  \n",
       "article_id                                                    \n",
       "0           0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1           0.136276  0.069973  0.000000  0.000000  0.000000  \n",
       "2           0.136276  0.069973  0.000000  0.000000  0.000000  \n",
       "3           0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4           0.125367  0.064371  0.000000  0.000000  0.000000  \n",
       "...              ...       ...       ...       ...       ...  \n",
       "1440        1.000000  0.390105  0.000000  0.000000  0.000000  \n",
       "1441        0.390105  1.000000  0.000000  0.000000  0.000000  \n",
       "1442        0.000000  0.000000  1.000000  0.063601  0.054473  \n",
       "1443        0.000000  0.000000  0.063601  1.000000  0.078745  \n",
       "1444        0.000000  0.000000  0.054473  0.078745  1.000000  \n",
       "\n",
       "[1328 rows x 1328 columns]"
      ]
     },
     "execution_count": 2202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to pickle\n",
    "\n",
    "title_title.to_pickle('title_title_similarity_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9999 in title_title.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look Up similar articles by article by title similarity (title-title)\n",
    "\n",
    "# Use a query method from pre-calculated \n",
    "# title_title df. Save time.\n",
    "\n",
    "def loopup_similar_title_articles(article_id, data=title_title, n=20):\n",
    "    \n",
    "    # input: n - number of top similar to return\n",
    "    \n",
    "    if article_id in title_title.index.values: \n",
    "    \n",
    "        ids = list(data.loc[article_id][data.loc[article_id].index != article_id].sort_values(ascending=False).head(20).index)\n",
    "        names = list(titles_df.loc[ids].title.values)\n",
    "        \n",
    "    else:\n",
    "        print(f\"[article_id] {article_id} not found in dataset.\")\n",
    "        ids = []\n",
    "        names = []\n",
    "    \n",
    "    \n",
    "    return ids, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[122,\n",
       " 313,\n",
       " 1427,\n",
       " 762,\n",
       " 254,\n",
       " 1390,\n",
       " 655,\n",
       " 805,\n",
       " 444,\n",
       " 800,\n",
       " 260,\n",
       " 1035,\n",
       " 384,\n",
       " 1001,\n",
       " 40,\n",
       " 500,\n",
       " 893,\n",
       " 1297,\n",
       " 390,\n",
       " 1185]"
      ]
     },
     "execution_count": 2223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar articles (with title similarity)\n",
    "\n",
    "rec_a = loopup_similar_title_articles(437)[0]\n",
    "rec_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[655,\n",
       " 511,\n",
       " 686,\n",
       " 555,\n",
       " 751,\n",
       " 762,\n",
       " 662,\n",
       " 122,\n",
       " 517,\n",
       " 653,\n",
       " 335,\n",
       " 805,\n",
       " 254,\n",
       " 417,\n",
       " 827,\n",
       " 112,\n",
       " 644,\n",
       " 82,\n",
       " 529,\n",
       " 384]"
      ]
     },
     "execution_count": 2225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar articles (with overall similarity score (title, body, desc))\n",
    "\n",
    "rec_b = loopup_similar_articles(437)[0]\n",
    "rec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two list. \n",
    "# List that from title similarity and list that from overall similarity. \n",
    "# Intersection plus each list's unique items.\n",
    "# the combined result list served as a union of two list. \n",
    "\n",
    "def union_list(list_a, list_b):\n",
    "    # input: two lists \n",
    "    # output: a union list.\n",
    "    \n",
    "    intersect = np.intersect1d(list_a, list_b)\n",
    "    unique_in_a = set(list_a) - set(intersect)\n",
    "    unique_in_b = set(list_b) - set(intersect)\n",
    "    \n",
    "    union = list(set.union(set.union(unique_in_a, unique_in_b), intersect))\n",
    "    \n",
    "    # shuffle the list\n",
    "    random.shuffle(union)\n",
    "\n",
    "    \n",
    "    return union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[653,\n",
       " 1297,\n",
       " 805,\n",
       " 644,\n",
       " 827,\n",
       " 500,\n",
       " 40,\n",
       " 762,\n",
       " 1427,\n",
       " 335,\n",
       " 529,\n",
       " 313,\n",
       " 122,\n",
       " 655,\n",
       " 112,\n",
       " 800,\n",
       " 893,\n",
       " 384,\n",
       " 686,\n",
       " 417,\n",
       " 1001,\n",
       " 555,\n",
       " 444,\n",
       " 1390,\n",
       " 254,\n",
       " 1035,\n",
       " 751,\n",
       " 1185,\n",
       " 517,\n",
       " 82,\n",
       " 260,\n",
       " 662,\n",
       " 511,\n",
       " 390]"
      ]
     },
     "execution_count": 2245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(union_list(rec_a, rec_b)))\n",
    "\n",
    "union_rec_list = union_list(rec_a, rec_b)\n",
    "\n",
    "union_rec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_titles(ids, data=titles_df):\n",
    "    # Input: a list of article ids\n",
    "    # data: the all article title dataframe\n",
    "    \n",
    "    titles = data[data.article_id.isin(ids)].title.apply(lambda x: x.title()).values\n",
    "    result = list(titles)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Putting All Content-Content together make rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting union results to make content recs\n",
    "\n",
    "def make_content_recs(article_id, data=titles_df):\n",
    "    '''\n",
    "    Strategy: get relevant content based on title score, and get relevant content based\n",
    "              on overall (title,desc,body) score. Combine them.\n",
    "              Also look futher another extra layer, to fetch relevant content of its relevant content. \n",
    "              think of 'relevant of relevant.', put these extra to union list. \n",
    "    '''\n",
    "    \n",
    "    # output: ids and titles tuple\n",
    "    \n",
    "    recs_based_on_title = loopup_similar_title_articles(article_id)[0]\n",
    "    recs_based_on_overall = loopup_similar_articles(article_id)[0]\n",
    "    \n",
    "    union_recs = union_list(recs_based_on_title, recs_based_on_overall)\n",
    "    \n",
    "    extra_recs = []\n",
    "    \n",
    "    \n",
    "    # If article_id not in df_content, find one from the list that is in df_content or article_article,\n",
    "    # so that it can use overall score similarity to load extra relevant content\n",
    "    if article_id not in article_article.index.values:\n",
    "        # print(f\"{article_id} not in article_article, pulling addinional relevant content.\")\n",
    "        \n",
    "        for i in union_recs:\n",
    "            if i in article_article.index.values:\n",
    "                # find similar base on overall score, and slice first 5 instance\n",
    "                extra = loopup_similar_articles(i)[0][:5]\n",
    "                extra_recs.append(extra)\n",
    "                \n",
    "    extra_recs = np.array(extra_recs).flatten()\n",
    "    extra_recs = list(np.unique(extra_recs))\n",
    "\n",
    "\n",
    "    for e in extra_recs:\n",
    "        if e not in union_recs:\n",
    "            union_recs.append(e)\n",
    "            \n",
    "    \n",
    "    titles = get_article_titles(union_recs)\n",
    "            \n",
    "    \n",
    "    return union_recs, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1420,\n",
       "  800,\n",
       "  655,\n",
       "  161,\n",
       "  313,\n",
       "  34,\n",
       "  1422,\n",
       "  1390,\n",
       "  893,\n",
       "  444,\n",
       "  122,\n",
       "  809,\n",
       "  762,\n",
       "  260,\n",
       "  805,\n",
       "  1035,\n",
       "  254,\n",
       "  124,\n",
       "  384,\n",
       "  437,\n",
       "  96,\n",
       "  112,\n",
       "  250,\n",
       "  455,\n",
       "  495,\n",
       "  511,\n",
       "  555,\n",
       "  592,\n",
       "  595,\n",
       "  600,\n",
       "  686,\n",
       "  721,\n",
       "  723,\n",
       "  751,\n",
       "  793,\n",
       "  806,\n",
       "  812,\n",
       "  907,\n",
       "  967,\n",
       "  972,\n",
       "  1006],\n",
       " ['Top 10 Machine Learning Use Cases: Part 1',\n",
       "  'Improving Quality Of Life With Spark-Empowered Machine Learning',\n",
       "  'Building Custom Machine Learning Algorithms With Apache Systemml',\n",
       "  'Watson Machine Learning For Developers',\n",
       "  'Python Machine Learning: Scikit-Learn Tutorial',\n",
       "  'Use The Machine Learning Library In Spark',\n",
       "  'Building Your First Machine Learning System ',\n",
       "  'Apple, Ibm Add Machine Learning To Partnership With Watson-Core Ml Coupling',\n",
       "  'The Machine Learning Database',\n",
       "  'What Is Machine Learning?',\n",
       "  'Continuous Learning On Watson',\n",
       "  'Ibm Watson Machine Learning: Get Started',\n",
       "  'Declarative Machine Learning',\n",
       "  'Make Machine Learning A Reality For Your Enterprise',\n",
       "  'Top 10 Machine Learning Algorithms For Beginners',\n",
       "  'Put A Human Face On Machine Learning With Wml & Dsx',\n",
       "  'Build A Naive-Bayes Model With Wml & Dsx',\n",
       "  'Apache Spark Analytics',\n",
       "  'Load Dashdb Data With Apache Spark',\n",
       "  'Access Ibm Analytics For Apache Spark From Rstudio',\n",
       "  'Create A Project For Watson Machine Learning In Dsx',\n",
       "  'Score A Predictive Model Built With Ibm Spss Modeler, Wml & Dsx',\n",
       "  'The Power Of Machine Learning In Spark',\n",
       "  '10 Essential Algorithms For Machine Learning Engineers',\n",
       "  'Build A Predictive Analytic Model',\n",
       "  'From Machine Learning To Learning Machine (Dinesh Nirmal)',\n",
       "  '10 Powerful Features On Watson Data Platform, No Coding Necessary',\n",
       "  'Machine Learning For The Enterprise',\n",
       "  'Machine Learning For Everyone',\n",
       "  'Collaborate On Projects In Dsx',\n",
       "  'Use The Machine Learning Library',\n",
       "  'Machine Learning Exercises In Python, Part 1',\n",
       "  'Use The Machine Learning Library In Ibm Analytics For Apache Spark',\n",
       "  'Build Spark Sql Queries',\n",
       "  'Ml Algorithm != Learning Machine',\n",
       "  'Create A Project In Dsx',\n",
       "  'Essentials Of Machine Learning Algorithms (With Python And R Codes)',\n",
       "  'Machine Learning For The Enterprise.',\n",
       "  'Style Transfer Experiments With Watson Machine Learning',\n",
       "  'Use Apache Systemml And Spark For Machine Learning',\n",
       "  'Use R Dataframes & Ibm Watson Natural Language Understanding'])"
      ]
     },
     "execution_count": 2348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make recs based on content article_id 1427. \n",
    "# Revealing that they are 'machine learning' related\n",
    "\n",
    "make_content_recs(1427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Extracting topics from popular content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_bigrams_by(id, data=titles_df):\n",
    "    title = data.loc[id].title\n",
    "    \n",
    "    # a list of tokens\n",
    "    tokened_text = tokenize(title)\n",
    "    \n",
    "    # ngrams 2\n",
    "    grams = [f\"{x[0]} {x[1]}\" for x in list(bigrams(tokened_text))]\n",
    "    \n",
    "    # sort with frequency descending\n",
    "    sorted_grams = [x[0] for x in Counter(grams).most_common()]\n",
    "    \n",
    "    return sorted_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics_by_title_bigrams(article_id, m=2):\n",
    "    \n",
    "    # Input: m - threadhold about top M to slice from the frequent term counter\n",
    "    \n",
    "    # Get relavant articles ids. (also append self)\n",
    "    ids = [article_id] + make_content_recs(article_id)[0]\n",
    "    \n",
    "    # Get ngrams for each title, and assemble to documents list. \n",
    "    documents = [get_title_bigrams_by(i) for i in ids]\n",
    "    \n",
    "    \n",
    "    # Find most common bigrams. Highly possible be the topic\n",
    "    documents_flatten = [item for sublist in documents for item in sublist]\n",
    "    \n",
    "    # most counted terms. Select first m instances. \n",
    "    topics_counter = Counter(documents_flatten).most_common()[:m]\n",
    "    \n",
    "    topics = [x[0].title() for x in topics_counter]\n",
    "    \n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine Learning', 'Deep Learning']\n",
      "['Apache Spark', 'Cloud Data']\n",
      "['Data Science', 'Ibm Data']\n",
      "['Ibm Watson', 'Watson Data']\n"
     ]
    }
   ],
   "source": [
    "# Test topics extracting from given a article id\n",
    "\n",
    "print(extract_topics_by_title_bigrams(500))\n",
    "\n",
    "print(extract_topics_by_title_bigrams(55))\n",
    "\n",
    "print(extract_topics_by_title_bigrams(1400))\n",
    "\n",
    "print(extract_topics_by_title_bigrams(937))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Let's show what are the top topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0, 1162.0, 1304.0, 1436.0, 1271.0, 1398.0, 43.0, 1351.0, 1393.0, 1185.0, 1160.0, 1354.0, 1368.0]\n",
      "\n",
      "\n",
      "[1429] is about: ['Deep Learning', 'Machine Learning']\n",
      "[1330] is about: ['Cloudant Query', 'Apache Spark']\n",
      "[1431] is about: ['Data Science', 'Watson Data']\n",
      "[1427] is about: ['Machine Learning', 'Watson Machine']\n",
      "[1364] is about: ['Machine Learning', 'Data Science']\n",
      "[1314] is about: ['Apache Spark', 'Jupyter Notebook']\n",
      "[1293] is about: ['Cloudant Query', 'Data Science']\n",
      "[1170] is about: ['Apache Spark', 'Spark Sql']\n",
      "[1162] is about: ['Machine Learning', 'Jupyter Notebook']\n",
      "[1304] is about: ['Machine Learning', 'Logistic Regression']\n",
      "[1436] is about: ['Data Science', 'Ibm Watson']\n",
      "[1271] is about: ['Making Compose', 'Compose Customer']\n",
      "[1398] is about: ['Country Statistic', 'Country Population']\n",
      "[43] is about: ['Deep Learning', 'Data Science']\n",
      "[1351] is about: ['Data Science', 'Watson Data']\n",
      "[1393] is about: ['Data Science', 'Web Pick']\n",
      "[1185] is about: ['Machine Learning', 'Learning Algorithm']\n",
      "[1160] is about: ['Apache Spark', 'Spark 2']\n",
      "[1354] is about: ['Machine Learning', 'Apache Spark']\n",
      "[1368] is about: ['Machine Learning', 'Wml Dsx']\n",
      "\n",
      "\n",
      "Based on most viewed articles (top 20 instance), the top 10  popular topics are:\n",
      "\n",
      "1 Machine Learning\n",
      "2 Data Science\n",
      "3 Apache Spark\n",
      "4 Deep Learning\n",
      "5 Cloudant Query\n",
      "6 Watson Data\n",
      "7 Jupyter Notebook\n",
      "8 Watson Machine\n",
      "9 Spark Sql\n",
      "10 Logistic Regression\n",
      "\n",
      "This is valuable because based on such demand, we can feed more related content in future. \n",
      "Also to serve as knowleadge-based recommendation for brand new users\n"
     ]
    }
   ],
   "source": [
    "# Let's show what are the top topics\n",
    "\n",
    "# Checked the top viewed content\n",
    "print(most_popular_list)\n",
    "print('\\n')\n",
    "\n",
    "# A list to hold each hot/popular content's topic\n",
    "popular_topics_overall = []\n",
    "\n",
    "# Loop thru each popular article, extract its topic and combine them.\n",
    "for article_id in most_popular_list:\n",
    "    article_id = int(article_id)\n",
    "    topics = extract_topics_by_title_bigrams(article_id)\n",
    "    print(f\"[{article_id}] is about: {topics}\")\n",
    "    popular_topics_overall.append(topics)\n",
    "    \n",
    "\n",
    "# Sort with most frequent to represent most popular topics\n",
    "print(f\"\\n\\nBased on most viewed articles (top 20 instance), the top 10  popular topics are:\\n\")\n",
    "most_popular_topics = [x[0] for x in Counter(np.array(popular_topics_overall).flatten()).most_common()[0:10]]\n",
    "\n",
    "for k,v in enumerate(most_popular_topics):\n",
    "    print(k+1, v)\n",
    "    \n",
    "    \n",
    "\n",
    "print(f\"\\nThis is valuable because based on such demand, we can feed more related content in future. \\nAlso to serve as knowleadge-based recommendation for brand new users\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
