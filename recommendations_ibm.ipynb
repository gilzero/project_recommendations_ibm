{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/2322/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "2      1429.0         use deep learning for image classification   \n",
       "3      1338.0          ml optimization using cognitive assistant   \n",
       "4      1276.0          deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50% of individuals interact with ____ number of articles or fewer.\n",
    "\n",
    "median_val = df.groupby('email').size().sort_values(ascending=True).median()\n",
    "\n",
    "median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The maximum number of user-article interactions by any 1 user is ______. ?\n",
    "\n",
    "max_views_by_user = df.groupby('email').size().sort_values(ascending=False).head(1).values[0]\n",
    "\n",
    "max_views_by_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content['article_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>Community Detection at Scale</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>During the seven-week Insight Data Engineering...</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>* United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...</td>\n",
       "      <td>When used to make sense of huge amounts of con...</td>\n",
       "      <td>How smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>One of the earliest documented catalogs was co...</td>\n",
       "      <td>How smart catalogs can turn the big data flood...</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>Today’s world of data science leverages data f...</td>\n",
       "      <td>Using Apache Spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>Today’s world of data science leverages data f...</td>\n",
       "      <td>Using Apache Spark as a parallel processing fr...</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_body  \\\n",
       "50   Follow Sign in / Sign up Home About Insight Da...   \n",
       "365  Follow Sign in / Sign up Home About Insight Da...   \n",
       "221  * United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...   \n",
       "692  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "232  Homepage Follow Sign in Get started Homepage *...   \n",
       "971  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "399  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "761  Homepage Follow Sign in Get started Homepage *...   \n",
       "578  This video shows you how to construct queries ...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "\n",
       "                                       doc_description  \\\n",
       "50                        Community Detection at Scale   \n",
       "365  During the seven-week Insight Data Engineering...   \n",
       "221  When used to make sense of huge amounts of con...   \n",
       "692  One of the earliest documented catalogs was co...   \n",
       "232  If you are like most data scientists, you are ...   \n",
       "971  If you are like most data scientists, you are ...   \n",
       "399  Today’s world of data science leverages data f...   \n",
       "761  Today’s world of data science leverages data f...   \n",
       "578  This video shows you how to construct queries ...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "\n",
       "                                         doc_full_name doc_status  article_id  \n",
       "50                        Graph-based machine learning       Live          50  \n",
       "365                       Graph-based machine learning       Live          50  \n",
       "221  How smart catalogs can turn the big data flood...       Live         221  \n",
       "692  How smart catalogs can turn the big data flood...       Live         221  \n",
       "232  Self-service data preparation with IBM Data Re...       Live         232  \n",
       "971  Self-service data preparation with IBM Data Re...       Live         232  \n",
       "399  Using Apache Spark as a parallel processing fr...       Live         398  \n",
       "761  Using Apache Spark as a parallel processing fr...       Live         398  \n",
       "578                              Use the Primary Index       Live         577  \n",
       "970                              Use the Primary Index       Live         577  "
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "\n",
    "dups = df_content.groupby('article_id').size().sort_values(ascending=False) # duplicates of article id\n",
    "dups_idx = dups[dups > 1].index # duplicates index\n",
    "\n",
    "df_content[df_content['article_id'].isin(dups_idx)].sort_values(by='article_id') # retrieve duplicated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "\n",
    "df_content = df_content.drop_duplicates('article_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_articles = df.article_id.nunique() # The number of unique articles that have at least one interaction\n",
    "unique_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_articles = df_content.article_id.nunique() # The number of unique articles on the IBM platform\n",
    "total_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5148"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users = df.email.nunique() # The number of unique users\n",
    "unique_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45993"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_article_interactions = df.shape[0] # The number of user-article interactions\n",
    "\n",
    "user_article_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_ds = df.groupby('article_id').size().sort_values(ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1429.0'"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most viewed article in the dataset as a string with one value following the decimal \n",
    "most_viewed_article_id = str(most_viewed_ds.index[0])\n",
    "most_viewed_article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most viewed article in the dataset was viewed how many times?\n",
    "\n",
    "max_views = most_viewed_ds.values[0]\n",
    "max_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1      1314.0       healthcare python streaming application demo        2\n",
       "2      1429.0         use deep learning for image classification        3\n",
       "3      1338.0          ml optimization using cognitive assistant        4\n",
       "4      1276.0          deploy your python model as a restful api        5"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you have everything right here! Nice job!\n"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val, \n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions, \n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views, \n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id, \n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles, \n",
    "    '`The number of unique users in the dataset is ______`': unique_users, \n",
    "    '`The number of unique articles on the IBM platform`': total_articles \n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids_ds = df.groupby('article_id').size().sort_values(ascending=False).head(n)\n",
    "    ids = article_ids_ds.index\n",
    "    \n",
    "    top_articles = []\n",
    "    for i in ids:\n",
    "        top_articles.append(df[df['article_id'] == i].head(1).title.values[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return top_articles # Return the top article titles from df (not df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles_ids - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids_ds = df.groupby('article_id').size().sort_values(ascending=False).head(n)\n",
    "    ids = article_ids_ds.index\n",
    "    top_articles_ids = list(ids)\n",
    " \n",
    "    return top_articles_ids # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use deep learning for image classification', 'insights from new york car accident reports', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'healthcare python streaming application demo', 'finding optimal locations of new store using decision optimization', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model']\n",
      "[1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0, 1162.0, 1304.0]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top_5 looks like the solution list! Nice job.\n",
      "Your top_10 looks like the solution list! Nice job.\n",
      "Your top_20 looks like the solution list! Nice job.\n"
     ]
    }
   ],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    df['title'] = 1\n",
    "    df = df[['user_id', 'article_id', 'title']]\n",
    "    \n",
    "    user_item = pd.pivot_table(df, index='user_id', columns='article_id', values='title', fill_value=0)\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>12.0</th>\n",
       "      <th>14.0</th>\n",
       "      <th>15.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>18.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1434.0</th>\n",
       "      <th>1435.0</th>\n",
       "      <th>1436.0</th>\n",
       "      <th>1437.0</th>\n",
       "      <th>1439.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1441.0</th>\n",
       "      <th>1442.0</th>\n",
       "      <th>1443.0</th>\n",
       "      <th>1444.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5149 rows × 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0.0     2.0     4.0     8.0     9.0     12.0    14.0    15.0    \\\n",
       "user_id                                                                      \n",
       "1                0       0       0       0       0       0       0       0   \n",
       "2                0       0       0       0       0       0       0       0   \n",
       "3                0       0       0       0       0       1       0       0   \n",
       "4                0       0       0       0       0       0       0       0   \n",
       "5                0       0       0       0       0       0       0       0   \n",
       "...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "5145             0       0       0       0       0       0       0       0   \n",
       "5146             0       0       0       0       0       0       0       0   \n",
       "5147             0       0       0       0       0       0       0       0   \n",
       "5148             0       0       0       0       0       0       0       0   \n",
       "5149             0       0       0       0       0       0       0       0   \n",
       "\n",
       "article_id  16.0    18.0    ...  1434.0  1435.0  1436.0  1437.0  1439.0  \\\n",
       "user_id                     ...                                           \n",
       "1                0       0  ...       0       0       1       0       1   \n",
       "2                0       0  ...       0       0       0       0       0   \n",
       "3                0       0  ...       0       0       1       0       0   \n",
       "4                0       0  ...       0       0       0       0       0   \n",
       "5                0       0  ...       0       0       0       0       0   \n",
       "...            ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "5145             0       0  ...       0       0       0       0       0   \n",
       "5146             0       0  ...       0       0       0       0       0   \n",
       "5147             0       0  ...       0       0       0       0       0   \n",
       "5148             0       0  ...       0       0       0       0       0   \n",
       "5149             1       0  ...       0       0       0       0       0   \n",
       "\n",
       "article_id  1440.0  1441.0  1442.0  1443.0  1444.0  \n",
       "user_id                                             \n",
       "1                0       0       0       0       0  \n",
       "2                0       0       0       0       0  \n",
       "3                0       0       0       0       0  \n",
       "4                0       0       0       0       0  \n",
       "5                0       0       0       0       0  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "5145             0       0       0       0       0  \n",
       "5146             0       0       0       0       0  \n",
       "5147             0       0       0       0       0  \n",
       "5148             0       0       0       0       0  \n",
       "5149             0       0       0       0       0  \n",
       "\n",
       "[5149 rows x 714 columns]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar). The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users.\n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.119027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119027</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id         1         2\n",
       "user_id                    \n",
       "1        1.000000  0.119027\n",
       "2        0.119027  1.000000"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment with pandas inbuilt corr\n",
    "\n",
    "user_item.loc[(1,2), :].transpose().corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar). The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. **Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users.**\n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    user_ids_all = user_item.index\n",
    "    \n",
    "    # a dictionary holds [user id : similarity] value pairs\n",
    "    dot_product_dict = {}\n",
    "\n",
    "    # compute similarity of each user to the provided user\n",
    "    for i in user_ids_all:\n",
    "        dot_product = np.dot(user_item.loc[user_id:user_id,:], user_item.loc[i:i,:].transpose())\n",
    "        dot_product_dict[i] = dot_product[0][0]\n",
    "        \n",
    "    # sort by highest (most similar), also drop against self    \n",
    "    ds = pd.Series(dot_product_dict.values(), index=dot_product_dict.keys()).drop(index=user_id).sort_values(ascending=False)\n",
    "    \n",
    "    # extract only index as list of ids. \n",
    "    most_similar_users = list(ds.index)\n",
    "       \n",
    "    return most_similar_users # return a list of the users in order from most to least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: [3933, 23, 3782, 203, 4459, 3870, 131, 46, 4201, 395]\n",
      "The 5 most similar users to user 3933 are: [1, 23, 3782, 4459, 203]\n",
      "The 3 most similar users to user 46 are: [4201, 23, 3782]\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['using pixiedust for fast, flexible, and easier data analysis and experimentation',\n",
       " 'healthcare python streaming application demo',\n",
       " 'deploy your python model as a restful api',\n",
       " 'bayesian nonparametric models – stats and bots']"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    article_names = []\n",
    "    \n",
    "    for id in article_ids:\n",
    "\n",
    "        name = df[df.article_id == id].title.head(1).values[0]\n",
    "\n",
    "        article_names.append(name)\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "a_ids = [1430.0, 1314.0, 1276.0, 233.0]\n",
    "get_article_names(a_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0],\n",
       " ['using deep learning to reconstruct high-resolution audio',\n",
       "  'build a python app on the streaming analytics service',\n",
       "  'gosales transactions for naive bayes model',\n",
       "  'healthcare python streaming application demo',\n",
       "  'use r dataframes & ibm watson natural language understanding',\n",
       "  'use xgboost, scikit-learn & ibm watson machine learning apis'])"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    article_ids = list(user_item.loc[user_id][user_item.loc[user_id] > 0].index)\n",
    "    article_names = get_article_names(article_ids)\n",
    "    \n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "get_user_articles(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # get all similar user ids for the targeted user\n",
    "    similar_uids = find_similar_users(user_id)\n",
    "\n",
    "\n",
    "    # get all article ids of the targeted user\n",
    "    article_ids_target_user = get_user_articles(user_id)[0]\n",
    "    # print(f\"[article_ids_target_user]:\\n {article_ids_target_user} \\n\")\n",
    "\n",
    "    \n",
    "    # a list contain unseen articles to recommend\n",
    "    recs = []\n",
    "\n",
    "    for uid in similar_uids:\n",
    "\n",
    "        # print(f\"\\n\\n\\n[number of recs]: {len(recs)}\\n\")\n",
    "        if len(recs) == m:\n",
    "            # print(f\"Number of recs reaches threadhold. Enough. Stop\")\n",
    "            break\n",
    "\n",
    "\n",
    "        #print(f\"[similar user id]: {uid}\")\n",
    "\n",
    "        # get this uid's article ids, and arbitrarily shuffle\n",
    "        article_ids_similar_user = get_user_articles(uid)[0]\n",
    "\n",
    "        # compute the differences of articles seen between the this user and targeted user\n",
    "        # subtraction's order matters\n",
    "        set_diff = list(set(article_ids_similar_user) - set(article_ids_target_user))\n",
    "        # print(f\"[set_diff]:\\n {set_diff} \\n\")\n",
    "        \n",
    "        # make a shuffle for arbitraily chocies from the diff set\n",
    "        random.shuffle(set_diff)\n",
    "\n",
    "        # add the differences of article ids to recs [], append only unique (no duplicate)\n",
    "        for i in set_diff:\n",
    "            if i not in recs and len(recs) < m:\n",
    "                recs.append(i)\n",
    "                # print(f\"[id] {i} appended\")\n",
    "\n",
    "    return recs # return your recommendations for this user_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data tidying in data science experience',\n",
       " 'this week in data science (april 18, 2017)',\n",
       " 'shaping data with ibm data refinery',\n",
       " 'data science platforms are on the rise and ibm is leading the way',\n",
       " 'timeseries data analysis of iot events by using jupyter notebook',\n",
       " 'got zip code data? prep it for analytics. – ibm watson data lab – medium',\n",
       " 'higher-order logistic regression for large datasets',\n",
       " 'using machine learning to predict parking difficulty',\n",
       " 'a tensorflow regression model to predict house values',\n",
       " 'deep forest: towards an alternative to deep neural networks']"
      ]
     },
     "execution_count": 1146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 1168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUT THE ANSWER WANT ARTICLE IDS TO BE STRING RATHER THAN NUMBER. However according to dataset, \n",
    "# they are all float. In the case, I just use its default astype for argument. \n",
    "\n",
    "df.article_id.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here\n",
    "assert set(get_article_names([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names([1320.0, 232.0, 844.0])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set([1320.0, 232.0, 844.0])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set([1024.0, 1176.0, 1305.0, 1314.0, 1422.0, 1427.0])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    # get all neighbors ids\n",
    "    nbh_ids = find_similar_users(user_id)\n",
    "    \n",
    "    \n",
    "    # assemble a data matrix\n",
    "    data_matrix = np.array([\n",
    "    [\n",
    "        x, # neighbor id\n",
    "        np.dot(user_item.loc[user_id:user_id,:], user_item.loc[x:x,:].transpose())[0][0], # similarity score\n",
    "        df[df.user_id == x].shape[0] # number of content interaction\n",
    "    ] for x in nbh_ids])\n",
    "    \n",
    "    # make a dataframe\n",
    "    neighbors_df = pd.DataFrame(data=data_matrix, \n",
    "                                columns=['neighbor_id', 'similarity', 'num_interactions'], \n",
    "                                index=data_matrix[:,0]).sort_values(by=['similarity', 'num_interactions'],\n",
    "                                                                    ascending=[False, False])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # get all similar user ids for the targeted user\n",
    "    # fetch with the 'neighbors_df'\n",
    "    similar_uids = list(get_top_sorted_users(user_id).index)\n",
    "    #print(f\"[similar_uids]: \\n{similar_uids}\")\n",
    "\n",
    "\n",
    "    # get all article ids of the targeted user\n",
    "    article_ids_target_user = get_user_articles(user_id)[0]\n",
    "    # print(f\"[article_ids_target_user]:\\n {article_ids_target_user} \\n\")\n",
    "\n",
    "    \n",
    "    # a list contain unseen articles to recommend\n",
    "    recs = []\n",
    "\n",
    "    for uid in similar_uids:\n",
    "\n",
    "        # print(f\"\\n\\n\\n[number of recs]: {len(recs)}\\n\")\n",
    "        if len(recs) == m:\n",
    "            #print(f\"Number of recs reaches threadhold {m}. Enough. Stop\")\n",
    "            break\n",
    "\n",
    "\n",
    "        #print(f\"[similar user id]: {uid}\")\n",
    "\n",
    "        # get this uid's article ids, and arbitrarily shuffle\n",
    "        article_ids_similar_user = get_user_articles(uid)[0]\n",
    "\n",
    "        # compute the differences of articles seen between the this user and targeted user\n",
    "        # subtraction's order matters\n",
    "        set_diff = list(set(article_ids_similar_user) - set(article_ids_target_user))\n",
    "        #print(f\"[set_diff before sort]:\\n {set_diff} \\n\")\n",
    "        \n",
    "        # Sort the set. Determine with highest total interactions metric \n",
    "        set_diff = list(df[df.article_id.isin(set_diff)]['article_id'].value_counts().index)\n",
    "        #print(f\"[set_diff after sort]:\\n {set_diff} \\n\")\n",
    "\n",
    "        # add the differences of article ids to recs [], append only unique (no duplicate)\n",
    "        for i in set_diff:\n",
    "            if i not in recs and len(recs) < m:\n",
    "                recs.append(i)\n",
    "                #print(f\"[id] {i} appended\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    rec_names = get_article_names(recs)\n",
    "    \n",
    "    return recs, rec_names\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "[1330.0, 1427.0, 1364.0, 1170.0, 1162.0, 1304.0, 1351.0, 1160.0, 1354.0, 1368.0]\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['insights from new york car accident reports', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model', 'model bike sharing data with spss', 'analyze accident reports on amazon emr spark', 'movie recommender system with spark machine learning', 'putting a human face on machine learning']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>num_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>3933</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      neighbor_id  similarity  num_interactions\n",
       "3933         3933          35                45"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_sorted_users(1).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>num_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>3870</td>\n",
       "      <td>74</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>3782</td>\n",
       "      <td>39</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>203</td>\n",
       "      <td>33</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>4459</td>\n",
       "      <td>33</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>29</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>3764</td>\n",
       "      <td>29</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>3697</td>\n",
       "      <td>29</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>242</td>\n",
       "      <td>25</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      neighbor_id  similarity  num_interactions\n",
       "3870         3870          74               144\n",
       "3782         3782          39               363\n",
       "23             23          38               364\n",
       "203           203          33               160\n",
       "4459         4459          33               158\n",
       "98             98          29               170\n",
       "3764         3764          29               169\n",
       "49             49          29               147\n",
       "3697         3697          29               145\n",
       "242           242          25               148"
      ]
     },
     "execution_count": 1316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_sorted_users(131).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = 3933 # Find the user that is most similar to user 1 \n",
    "user131_10th_sim = 242 # Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This all looks good!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. If we were given a new user, which of the above functions would you be able to use to make recommendations? Explain. Can you think of a better way we might make recommendations? Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: For new user, code start problem, we can use knowledge base approach, pulling most-interacted (viewed) content and/or trending content. Since the dataset has no timestamp attribute, we might just pull most-interacted content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below. You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1429.0,\n",
       " 1330.0,\n",
       " 1431.0,\n",
       " 1427.0,\n",
       " 1364.0,\n",
       " 1314.0,\n",
       " 1293.0,\n",
       " 1170.0,\n",
       " 1162.0,\n",
       " 1304.0]"
      ]
     },
     "execution_count": 1321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user = 0.0\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "\n",
    "\n",
    "new_user_recs = get_top_article_ids(10, df) # Your recommendations here\n",
    "\n",
    "new_user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "assert set(new_user_recs) == set([1314.0,1429.0,1293.0,1427.0,1162.0,1364.0,1304.0,1170.0,1431.0,1330.0]), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations (EXTRA - NOT REQUIRED)</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term.  You might consider content to be the **doc_body**, **doc_description**, or **doc_full_name**.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` Use the function body below to create a content based recommender.  Since there isn't one right answer for this recommendation tactic, no test functions are provided.  Feel free to change the function inputs if you decide you want to try a method that requires more input values.  The input values are currently set with one idea in mind that you may use to make content based recommendations.  One additional idea is that you might want to choose the most popular recommendations that meet your 'content criteria', but again, there is a lot of flexibility in how you might make these recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gilzero/project_disaster_responses/blob/main/train_classifier.py\n",
    "\n",
    "https://towardsdatascience.com/the-best-document-similarity-algorithm-in-2020-a-beginners-guide-a01b9ef8cf05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# constants and reusable objects\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Compose is all about immediacy. You want a new...</td>\n",
       "      <td>Using Compose's PostgreSQL data browser.</td>\n",
       "      <td>Browsing PostgreSQL Data with Compose</td>\n",
       "      <td>Live</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UPGRADING YOUR POSTGRESQL TO 9.5Share on Twitt...</td>\n",
       "      <td>Upgrading your PostgreSQL deployment to versio...</td>\n",
       "      <td>Upgrading your PostgreSQL to 9.5</td>\n",
       "      <td>Live</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Follow Sign in / Sign up 135 8 * Share\\r\\n * 1...</td>\n",
       "      <td>For a company like Slack that strives to be as...</td>\n",
       "      <td>Data Wrangling at Slack</td>\n",
       "      <td>Live</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>* Host\\r\\n * Competitions\\r\\n * Datasets\\r\\n *...</td>\n",
       "      <td>Kaggle is your home for data science. Learn ne...</td>\n",
       "      <td>Data Science Bowl 2017</td>\n",
       "      <td>Live</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>THE GRADIENT FLOW\\r\\nDATA / TECHNOLOGY / CULTU...</td>\n",
       "      <td>[A version of this post appears on the O’Reill...</td>\n",
       "      <td>Using Apache Spark to predict attack vectors a...</td>\n",
       "      <td>Live</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OFFLINE-FIRST IOS APPS WITH SWIFT &amp; PART 1: TH...</td>\n",
       "      <td>Apple's sample app, Food Tracker, taught you i...</td>\n",
       "      <td>Offline-First iOS Apps with Swift &amp; Cloudant S...</td>\n",
       "      <td>Live</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Warehousing data from Cloudant to dashDB great...</td>\n",
       "      <td>Replicating data to a relational dashDB databa...</td>\n",
       "      <td>Warehousing GeoJSON documents</td>\n",
       "      <td>Live</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>This recipe showcases how one can analyze the ...</td>\n",
       "      <td>Timeseries Data Analysis of IoT events by usin...</td>\n",
       "      <td>Live</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Maureen McElaney Blocked Unblock Follow Follow...</td>\n",
       "      <td>There’s a reason you’ve been hearing a lot abo...</td>\n",
       "      <td>Bridging the Gap Between Python and Scala Jupy...</td>\n",
       "      <td>Live</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Raj Singh Blocked Unblock Follow Following Dev...</td>\n",
       "      <td>Who are those people lurking behind the statis...</td>\n",
       "      <td>Got zip code data? Prep it for analytics. – IB...</td>\n",
       "      <td>Live</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Early methods to integrate machine learning us...</td>\n",
       "      <td>Apache Spark™ 2.0: Extend Structured Streaming...</td>\n",
       "      <td>Live</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>* Home\\r\\n * Research\\r\\n * Partnerships and C...</td>\n",
       "      <td>The performance of supervised predictive model...</td>\n",
       "      <td>Higher-order Logistic Regression for Large Dat...</td>\n",
       "      <td>Live</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Enterprise Pricing Articles Sign in Free 30-Da...</td>\n",
       "      <td>We've always considered MySQL as a potential C...</td>\n",
       "      <td>Compose for MySQL now for you</td>\n",
       "      <td>Live</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Homepage Follow Sign in / Sign up * Home\\r\\n *...</td>\n",
       "      <td>It has never been easier to build AI or machin...</td>\n",
       "      <td>The Greatest Public Datasets for AI – Startup ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>METRICS MAVEN: MODE D'EMPLOI - FINDING THE MOD...</td>\n",
       "      <td>In our Metrics Maven series, Compose's data sc...</td>\n",
       "      <td>Finding the Mode in PostgreSQL</td>\n",
       "      <td>Live</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>It is often useful to use RStudio for one piec...</td>\n",
       "      <td>Working interactively with RStudio and noteboo...</td>\n",
       "      <td>Live</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Raj Singh Blocked Unblock Follow Following Dev...</td>\n",
       "      <td>You’re doing your data a disservice if you don...</td>\n",
       "      <td>Mapping for Data Science with PixieDust and Ma...</td>\n",
       "      <td>Live</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IMPORTING JSON DOCUMENTS WITH NOSQLIMPORT\\r\\nG...</td>\n",
       "      <td>Introducing nosqlimport, an npm module to help...</td>\n",
       "      <td>Move CSVs into different JSON doc stores</td>\n",
       "      <td>Live</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>This video shows you how to build and query a ...</td>\n",
       "      <td>This video shows you how to build and query a ...</td>\n",
       "      <td>Tutorial: How to build and query a Cloudant ge...</td>\n",
       "      <td>Live</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>THE CONVERSATIONAL INTERFACE IS THE NEW PARADI...</td>\n",
       "      <td>Botkit provides a simple framework to handle t...</td>\n",
       "      <td>The Conversational Interface is the New Paradigm</td>\n",
       "      <td>Live</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>Want to learn more about how we created the Da...</td>\n",
       "      <td>Creating the Data Science Experience</td>\n",
       "      <td>Live</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GOOGLE RESEARCH BLOG The latest news from Rese...</td>\n",
       "      <td>Much of driving is spent either stuck in traff...</td>\n",
       "      <td>Using Machine Learning to predict parking diff...</td>\n",
       "      <td>Live</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>This talk assumes you have a basic understandi...</td>\n",
       "      <td>Getting The Best Performance With PySpark</td>\n",
       "      <td>Live</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ACCESS DENIED\\r\\nSadly, your client does not s...</td>\n",
       "      <td>In this paper, we propose gcForest, a decision...</td>\n",
       "      <td>Deep Forest: Towards An Alternative to Deep Ne...</td>\n",
       "      <td>Live</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>I’m very happy and proud to announce that IBM ...</td>\n",
       "      <td>Experience IoT with Coursera</td>\n",
       "      <td>Live</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KDNUGGETS\\r\\nData Mining, Analytics, Big Data,...</td>\n",
       "      <td>An open API is available on the internet for f...</td>\n",
       "      <td>How open API economy accelerates the growth of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video shows you how to sign up for a free...</td>\n",
       "      <td>Sign up for a free trial in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>Stacking is a model ensembling technique used ...</td>\n",
       "      <td>A Kaggler's Guide to Model Stacking in Practice</td>\n",
       "      <td>Live</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Working Vis * \\r\\n * \\r\\n\\r\\n * Home\\r\\n * Abo...</td>\n",
       "      <td>Analytics and visualization often go hand-in-h...</td>\n",
       "      <td>Using Brunel in IPython/Jupyter Notebooks</td>\n",
       "      <td>Live</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Steve Moore ...</td>\n",
       "      <td>Machine learning has already extended into so ...</td>\n",
       "      <td>Top 10 Machine Learning Use Cases: Part 1</td>\n",
       "      <td>Live</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nick Kasten Blocked Unblock Follow Following C...</td>\n",
       "      <td>In this article, I’ll describe an app I built ...</td>\n",
       "      <td>Gaze Into My Reddit Crystal Ball – IBM Watson ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...</td>\n",
       "      <td>Here’s a quick and handy guide to creating dat...</td>\n",
       "      <td>Data visualization playbook: The right level o...</td>\n",
       "      <td>Live</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Homepage IBM Watson Data Lab Follow Sign in / ...</td>\n",
       "      <td>When you customise your Cloudant domain with C...</td>\n",
       "      <td>Create a Custom Domain for Cloudant Using Clou...</td>\n",
       "      <td>Live</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The primary index is the fastest way to retrie...</td>\n",
       "      <td>A guide to using Cloudant's _all_docs endpoint...</td>\n",
       "      <td>For Developers: Querying the Cloudant Primary ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>* R Views\\r\\n * About this Blog\\r\\n * Contribu...</td>\n",
       "      <td>Our app will be simple in that it displays pri...</td>\n",
       "      <td>Pulling and Displaying ETF Data</td>\n",
       "      <td>Live</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Stats and Bots Follow Sign in / Sign up * Home...</td>\n",
       "      <td>Ensemble learning helps improve machine learni...</td>\n",
       "      <td>Ensemble Learning to Improve Machine Learning ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TL;DR: It's easy to customise the Mongo shell'...</td>\n",
       "      <td>It's easy to customize the Mongo shell's promp...</td>\n",
       "      <td>Customizing MongoDB’s Shell with Compact Prompts</td>\n",
       "      <td>Live</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>GETTING STARTED WITH COMPOSE'S SCYLLADB\\r\\nSha...</td>\n",
       "      <td>Getting started with ScyllaDB is easy since it...</td>\n",
       "      <td>Getting Started with Compose's ScyllaDB</td>\n",
       "      <td>Live</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>This free Deep Learning with TensorFlow course...</td>\n",
       "      <td>Deep Learning With Tensorflow Course by Big Da...</td>\n",
       "      <td>Live</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Learn how to use IBM Bluemix and the Simple Da...</td>\n",
       "      <td>Uncover Product Insights Hidden in Stack Overflow</td>\n",
       "      <td>Live</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Build a custom library for Apache® Spark™ and ...</td>\n",
       "      <td>Build a custom library for Apache® Spark™ and ...</td>\n",
       "      <td>Start Developing with Spark and Notebooks</td>\n",
       "      <td>Live</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>REAL-TIME Q&amp;A APP WITH RETHINKDB\\r\\nMatt Colli...</td>\n",
       "      <td>RethinkDB's push updates makes it great for re...</td>\n",
       "      <td>Q&amp;A Voting App with RethinkDB</td>\n",
       "      <td>Live</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Watch how to download and install Database Con...</td>\n",
       "      <td>Install IBM Database Conversion Workbench</td>\n",
       "      <td>Live</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Data Science Experience Datasci X * Data Scien...</td>\n",
       "      <td>Learn to use IBM Data Science Experience.</td>\n",
       "      <td>Data Science Experience Documentation</td>\n",
       "      <td>Live</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Compose The Compose logo Articles Sign in Free...</td>\n",
       "      <td>We'll also look at using PostGIS to filter our...</td>\n",
       "      <td>GeoFile: Using OpenStreetMap Data in Compose P...</td>\n",
       "      <td>Live</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>Community Detection at Scale</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>* Free 7-Day Crash Course\\r\\n * Blog\\r\\n * Mas...</td>\n",
       "      <td>Get to know the ML landscape through this prac...</td>\n",
       "      <td>Modern Machine Learning Algorithms</td>\n",
       "      <td>Live</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>* United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...</td>\n",
       "      <td>Watch how to build a storefront web app with I...</td>\n",
       "      <td>Build an app using IBM Graph</td>\n",
       "      <td>Live</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>Starting today, users will be able to access S...</td>\n",
       "      <td>Introducing Streams Designer</td>\n",
       "      <td>Live</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...</td>\n",
       "      <td>Discover eight ways that Apache Spark’s machin...</td>\n",
       "      <td>8 ways to turn data into value with Apache Spa...</td>\n",
       "      <td>Live</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>PREDICT FLIGHT DELAYS WITH APACHE SPARK MLLIB,...</td>\n",
       "      <td>Build a Machine Learning model with Apache Spa...</td>\n",
       "      <td>Predict Flight Delays with Apache Spark MLLib,...</td>\n",
       "      <td>Live</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>INTRODUCING THE SIMPLE AUTOCOMPLETE SERVICE\\r\\...</td>\n",
       "      <td>Easily add autocomplete to your web form field...</td>\n",
       "      <td>Introducing the Simple Autocomplete Service</td>\n",
       "      <td>Live</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>WILL WOLF\\r\\nDATA SCIENCE THINGS AND THOUGHTS ...</td>\n",
       "      <td>In this work, we explore improving a vanilla r...</td>\n",
       "      <td>Transfer Learning for Flight Delay Prediction ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>01. Holden Karau, IBM, Visits #theCUBE!. (00:2...</td>\n",
       "      <td>Advancements in the Spark Community</td>\n",
       "      <td>Live</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Homepage PUBLISHED IN AUTONOMOUS AGENTS — #AI ...</td>\n",
       "      <td>Let’s say you have the gift of flight (or you ...</td>\n",
       "      <td>How to tame the valley — Hessian-free hacks fo...</td>\n",
       "      <td>Live</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             doc_body  \\\n",
       "0   Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1   No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2   ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3   DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4   Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "5   Compose is all about immediacy. You want a new...   \n",
       "6   UPGRADING YOUR POSTGRESQL TO 9.5Share on Twitt...   \n",
       "7   Follow Sign in / Sign up 135 8 * Share\\r\\n * 1...   \n",
       "8   * Host\\r\\n * Competitions\\r\\n * Datasets\\r\\n *...   \n",
       "9   THE GRADIENT FLOW\\r\\nDATA / TECHNOLOGY / CULTU...   \n",
       "10  OFFLINE-FIRST IOS APPS WITH SWIFT & PART 1: TH...   \n",
       "11  Warehousing data from Cloudant to dashDB great...   \n",
       "12  Skip to main content IBM developerWorks / Deve...   \n",
       "13  Maureen McElaney Blocked Unblock Follow Follow...   \n",
       "14  Raj Singh Blocked Unblock Follow Following Dev...   \n",
       "15  * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "16  * Home\\r\\n * Research\\r\\n * Partnerships and C...   \n",
       "17  Enterprise Pricing Articles Sign in Free 30-Da...   \n",
       "18  Homepage Follow Sign in / Sign up * Home\\r\\n *...   \n",
       "19  METRICS MAVEN: MODE D'EMPLOI - FINDING THE MOD...   \n",
       "20  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "21  Raj Singh Blocked Unblock Follow Following Dev...   \n",
       "22  IMPORTING JSON DOCUMENTS WITH NOSQLIMPORT\\r\\nG...   \n",
       "23  This video shows you how to build and query a ...   \n",
       "24  THE CONVERSATIONAL INTERFACE IS THE NEW PARADI...   \n",
       "25  Skip navigation Upload Sign in SearchLoading.....   \n",
       "26  GOOGLE RESEARCH BLOG The latest news from Rese...   \n",
       "27  Skip navigation Upload Sign in SearchLoading.....   \n",
       "28  ACCESS DENIED\\r\\nSadly, your client does not s...   \n",
       "29  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "30  KDNUGGETS\\r\\nData Mining, Analytics, Big Data,...   \n",
       "31  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "32  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "33  Working Vis * \\r\\n * \\r\\n\\r\\n * Home\\r\\n * Abo...   \n",
       "34  Homepage Follow Sign in / Sign up Steve Moore ...   \n",
       "35  Nick Kasten Blocked Unblock Follow Following C...   \n",
       "36  Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...   \n",
       "37  Homepage IBM Watson Data Lab Follow Sign in / ...   \n",
       "38  The primary index is the fastest way to retrie...   \n",
       "39  * R Views\\r\\n * About this Blog\\r\\n * Contribu...   \n",
       "40  Stats and Bots Follow Sign in / Sign up * Home...   \n",
       "41  TL;DR: It's easy to customise the Mongo shell'...   \n",
       "42  GETTING STARTED WITH COMPOSE'S SCYLLADB\\r\\nSha...   \n",
       "43  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "44  Skip to main content IBM developerWorks / Deve...   \n",
       "45  Build a custom library for Apache® Spark™ and ...   \n",
       "46  REAL-TIME Q&A APP WITH RETHINKDB\\r\\nMatt Colli...   \n",
       "47  Skip to main content IBM developerWorks / Deve...   \n",
       "48  Data Science Experience Datasci X * Data Scien...   \n",
       "49  Compose The Compose logo Articles Sign in Free...   \n",
       "50  Follow Sign in / Sign up Home About Insight Da...   \n",
       "51  * Free 7-Day Crash Course\\r\\n * Blog\\r\\n * Mas...   \n",
       "52  * United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\n...   \n",
       "53  Homepage Follow Sign in Get started Homepage *...   \n",
       "54  Jump to navigation\\r\\n\\r\\n * Twitter\\r\\n * Lin...   \n",
       "55  PREDICT FLIGHT DELAYS WITH APACHE SPARK MLLIB,...   \n",
       "56  INTRODUCING THE SIMPLE AUTOCOMPLETE SERVICE\\r\\...   \n",
       "57  WILL WOLF\\r\\nDATA SCIENCE THINGS AND THOUGHTS ...   \n",
       "58  Skip navigation Upload Sign in SearchLoading.....   \n",
       "59  Homepage PUBLISHED IN AUTONOMOUS AGENTS — #AI ...   \n",
       "\n",
       "                                      doc_description  \\\n",
       "0   Detect bad readings in real time using Python ...   \n",
       "1   See the forest, see the trees. Here lies the c...   \n",
       "2   Here’s this week’s news in Data Science and Bi...   \n",
       "3   Learn how distributed DBs solve the problem of...   \n",
       "4   This video demonstrates the power of IBM DataS...   \n",
       "5            Using Compose's PostgreSQL data browser.   \n",
       "6   Upgrading your PostgreSQL deployment to versio...   \n",
       "7   For a company like Slack that strives to be as...   \n",
       "8   Kaggle is your home for data science. Learn ne...   \n",
       "9   [A version of this post appears on the O’Reill...   \n",
       "10  Apple's sample app, Food Tracker, taught you i...   \n",
       "11  Replicating data to a relational dashDB databa...   \n",
       "12  This recipe showcases how one can analyze the ...   \n",
       "13  There’s a reason you’ve been hearing a lot abo...   \n",
       "14  Who are those people lurking behind the statis...   \n",
       "15  Early methods to integrate machine learning us...   \n",
       "16  The performance of supervised predictive model...   \n",
       "17  We've always considered MySQL as a potential C...   \n",
       "18  It has never been easier to build AI or machin...   \n",
       "19  In our Metrics Maven series, Compose's data sc...   \n",
       "20  It is often useful to use RStudio for one piec...   \n",
       "21  You’re doing your data a disservice if you don...   \n",
       "22  Introducing nosqlimport, an npm module to help...   \n",
       "23  This video shows you how to build and query a ...   \n",
       "24  Botkit provides a simple framework to handle t...   \n",
       "25  Want to learn more about how we created the Da...   \n",
       "26  Much of driving is spent either stuck in traff...   \n",
       "27  This talk assumes you have a basic understandi...   \n",
       "28  In this paper, we propose gcForest, a decision...   \n",
       "29  I’m very happy and proud to announce that IBM ...   \n",
       "30  An open API is available on the internet for f...   \n",
       "31  This video shows you how to sign up for a free...   \n",
       "32  Stacking is a model ensembling technique used ...   \n",
       "33  Analytics and visualization often go hand-in-h...   \n",
       "34  Machine learning has already extended into so ...   \n",
       "35  In this article, I’ll describe an app I built ...   \n",
       "36  Here’s a quick and handy guide to creating dat...   \n",
       "37  When you customise your Cloudant domain with C...   \n",
       "38  A guide to using Cloudant's _all_docs endpoint...   \n",
       "39  Our app will be simple in that it displays pri...   \n",
       "40  Ensemble learning helps improve machine learni...   \n",
       "41  It's easy to customize the Mongo shell's promp...   \n",
       "42  Getting started with ScyllaDB is easy since it...   \n",
       "43  This free Deep Learning with TensorFlow course...   \n",
       "44  Learn how to use IBM Bluemix and the Simple Da...   \n",
       "45  Build a custom library for Apache® Spark™ and ...   \n",
       "46  RethinkDB's push updates makes it great for re...   \n",
       "47  Watch how to download and install Database Con...   \n",
       "48          Learn to use IBM Data Science Experience.   \n",
       "49  We'll also look at using PostGIS to filter our...   \n",
       "50                       Community Detection at Scale   \n",
       "51  Get to know the ML landscape through this prac...   \n",
       "52  Watch how to build a storefront web app with I...   \n",
       "53  Starting today, users will be able to access S...   \n",
       "54  Discover eight ways that Apache Spark’s machin...   \n",
       "55  Build a Machine Learning model with Apache Spa...   \n",
       "56  Easily add autocomplete to your web form field...   \n",
       "57  In this work, we explore improving a vanilla r...   \n",
       "58  01. Holden Karau, IBM, Visits #theCUBE!. (00:2...   \n",
       "59  Let’s say you have the gift of flight (or you ...   \n",
       "\n",
       "                                        doc_full_name doc_status  article_id  \n",
       "0   Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1   Communicating data science: A guide to present...       Live           1  \n",
       "2          This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3   DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4       Analyze NY Restaurant data using Spark in DSX       Live           4  \n",
       "5               Browsing PostgreSQL Data with Compose       Live           5  \n",
       "6                    Upgrading your PostgreSQL to 9.5       Live           6  \n",
       "7                             Data Wrangling at Slack       Live           7  \n",
       "8                              Data Science Bowl 2017       Live           8  \n",
       "9   Using Apache Spark to predict attack vectors a...       Live           9  \n",
       "10  Offline-First iOS Apps with Swift & Cloudant S...       Live          10  \n",
       "11                      Warehousing GeoJSON documents       Live          11  \n",
       "12  Timeseries Data Analysis of IoT events by usin...       Live          12  \n",
       "13  Bridging the Gap Between Python and Scala Jupy...       Live          13  \n",
       "14  Got zip code data? Prep it for analytics. – IB...       Live          14  \n",
       "15  Apache Spark™ 2.0: Extend Structured Streaming...       Live          15  \n",
       "16  Higher-order Logistic Regression for Large Dat...       Live          16  \n",
       "17                      Compose for MySQL now for you       Live          17  \n",
       "18  The Greatest Public Datasets for AI – Startup ...       Live          18  \n",
       "19                     Finding the Mode in PostgreSQL       Live          19  \n",
       "20  Working interactively with RStudio and noteboo...       Live          20  \n",
       "21  Mapping for Data Science with PixieDust and Ma...       Live          21  \n",
       "22           Move CSVs into different JSON doc stores       Live          22  \n",
       "23  Tutorial: How to build and query a Cloudant ge...       Live          23  \n",
       "24   The Conversational Interface is the New Paradigm       Live          24  \n",
       "25               Creating the Data Science Experience       Live          25  \n",
       "26  Using Machine Learning to predict parking diff...       Live          26  \n",
       "27          Getting The Best Performance With PySpark       Live          27  \n",
       "28  Deep Forest: Towards An Alternative to Deep Ne...       Live          28  \n",
       "29                       Experience IoT with Coursera       Live          29  \n",
       "30  How open API economy accelerates the growth of...       Live          30  \n",
       "31                    Sign up for a free trial in DSX       Live          31  \n",
       "32    A Kaggler's Guide to Model Stacking in Practice       Live          32  \n",
       "33          Using Brunel in IPython/Jupyter Notebooks       Live          33  \n",
       "34          Top 10 Machine Learning Use Cases: Part 1       Live          34  \n",
       "35  Gaze Into My Reddit Crystal Ball – IBM Watson ...       Live          35  \n",
       "36  Data visualization playbook: The right level o...       Live          36  \n",
       "37  Create a Custom Domain for Cloudant Using Clou...       Live          37  \n",
       "38  For Developers: Querying the Cloudant Primary ...       Live          38  \n",
       "39                    Pulling and Displaying ETF Data       Live          39  \n",
       "40  Ensemble Learning to Improve Machine Learning ...       Live          40  \n",
       "41   Customizing MongoDB’s Shell with Compact Prompts       Live          41  \n",
       "42            Getting Started with Compose's ScyllaDB       Live          42  \n",
       "43  Deep Learning With Tensorflow Course by Big Da...       Live          43  \n",
       "44  Uncover Product Insights Hidden in Stack Overflow       Live          44  \n",
       "45          Start Developing with Spark and Notebooks       Live          45  \n",
       "46                      Q&A Voting App with RethinkDB       Live          46  \n",
       "47          Install IBM Database Conversion Workbench       Live          47  \n",
       "48              Data Science Experience Documentation       Live          48  \n",
       "49  GeoFile: Using OpenStreetMap Data in Compose P...       Live          49  \n",
       "50                       Graph-based machine learning       Live          50  \n",
       "51                 Modern Machine Learning Algorithms       Live          51  \n",
       "52                       Build an app using IBM Graph       Live          52  \n",
       "53                       Introducing Streams Designer       Live          53  \n",
       "54  8 ways to turn data into value with Apache Spa...       Live          54  \n",
       "55  Predict Flight Delays with Apache Spark MLLib,...       Live          55  \n",
       "56        Introducing the Simple Autocomplete Service       Live          56  \n",
       "57  Transfer Learning for Flight Delay Prediction ...       Live          57  \n",
       "58                Advancements in the Spark Community       Live          58  \n",
       "59  How to tame the valley — Hessian-free hacks fo...       Live          59  "
      ]
     },
     "execution_count": 1461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp = df_content.copy()\n",
    "df_nlp.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    private tokenizer to transform each text.\n",
    "    As a NLP helper function including following tasks:\n",
    "    - Replace URLs\n",
    "    - Normalize text\n",
    "    - Remove punctuation\n",
    "    - Tokenize words\n",
    "    - Remove stop words\n",
    "    - Legmmatize words\n",
    "    :param text: A message text.\n",
    "    :return: cleaned tokens extracted from original message text.\n",
    "    '''\n",
    "\n",
    "    # print(f\"original text: \\n {text}\")\n",
    "\n",
    "    # replace urls\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word).strip() for word in tokens if word not in stop_words]\n",
    "\n",
    "    # in case after normalize/lemmatize, if there is no words, make a dummy element. otherwise StartingVerb breaks\n",
    "    if len(tokens) < 1:\n",
    "        tokens = ['none']\n",
    "\n",
    "    # print(f\"tokens: \\n {tokens} \\n\\n\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skip navigation sign searchloading close yeah keep undo closethis video unavailable watch queue queue watch queue queue remove disconnect next video starting stop 1 loading watch queue queue count total find closedemo detect malfunctioning iot sensor streaming analytics ibm analyticsloading unsubscribe ibm analytics cancel unsubscribeworking subscribe subscribed unsubscribe 26kloading loading working add towant watch later sign add video playlist sign share reportneed report video sign report inappropriate content sign transcript statistic add translation 175 view 6like video sign make opinion count sign 7 0don like video sign make opinion count sign 1loading loading transcript interactive transcript could loaded loading loading rating available video rented feature available right please try later published nov 6 2017this video demonstrates streaming analytics application written python running ibm data science experience result analysis displayed map using plotly notebook demonstrated video available try urlplaceholder visit streamsdev article tip stream urlplaceholder python api developer guide urlplaceholder streaming analytics python course urlplaceholder category science technology license standard youtube license show show lessloading autoplay autoplay enabled suggested video automatically play next next python ecosystem data science guided tour christian staudt duration 25 41 pydata 1 411 view 25 41 ibm streaming analytics python duration 1 00 51 john neill 105 view 1 00 51 customer using ibm data science experience expected case expected one duration 18 29 databricks 327 view 18 29 giovanni lanzani applied data science duration 35 14 pydata 2 728 view 35 14 detecting fraud real time azure stream analytics duration 32 16 philip howard 71 view 32 16 step step guide build real time anomaly detection system using apache spark streaming duration 16 11 mariusz jacyno 4 591 view 16 11 real time analytics azure stream analytics duration 54 47 pas business analytics virtual group 940 view 54 47 real time machine learning analytics using structured streaming kinesis firehose duration 31 25 databricks 660 view 31 25 data science duration 25 05 manish telang 3 view 25 05 real time log analytics using amazon kinesis amazon elasticsearch service duration 28 32 amazon web service webinar channel 1 072 view 28 32 ibm data science experience machine learning use case healthcare duration 26 53 idea 157 view 26 53 streaming analytics comparison open source framework product cloud service duration 47 06 kai w hner 1 761 view 47 06 overview ibm streaming analytics bluemix duration 44 12 ibm analytics 1 311 view 44 12 predicting stock price learn python data science 4 duration 7 39 siraj raval 274 452 view 7 39 rest api concept example duration 8 53 webconcepts 1 687 034 view 8 53 streaming data analytics apache spark streaming duration 1 01 19 data guru 300 view 1 01 19 orchestrate ibm data science experience analytics workflow using node red duration 10 16 balaji kadambi 109 view 10 16 delight client data science ibm integrated analytics system duration 15 05 ibm analytics 1 581 view 15 05 devops simple english duration 7 07 rackspace 657 396 view 7 07 introduction learn python data science 1 duration 6 55 siraj raval 206 552 view 6 55 loading suggestion show language english location united state restricted mode history helploading loading loading press copyright creator advertise developer youtube term privacy policy safety send feedback test new feature loading working sign add watch lateradd loading playlist'"
      ]
     },
     "execution_count": 1396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tokenized result. \n",
    "\n",
    "df_nlp.iloc[0].doc_body\n",
    "\n",
    "token_doc = tokenize(df_nlp.iloc[0].doc_body)\n",
    "\n",
    "token_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found index and article id mismatched. Update index with article to make it consistant\n",
    "\n",
    "df_nlp.index = df_nlp.article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1046    False\n",
       "1047    False\n",
       "1048    False\n",
       "1049    False\n",
       "1050    False\n",
       "Name: doc_body, Length: 1051, dtype: bool"
      ]
     },
     "execution_count": 1637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Data. For empty content in body\n",
    "# There are some rows' doc_body value is NaN\n",
    "df_nlp.doc_body.isnull()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_body, doc_description, doc_full_name, doc_status, article_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 1634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the empty body with 'empty' placeholder\n",
    "df_nlp.loc[df_nlp[df_nlp.doc_body.isnull()].index, 'doc_body'] = 'empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the empty desc with 'empty' placeholder\n",
    "# There are some rows' doc_description value is NaN\n",
    "\n",
    "df_nlp.loc[df_nlp[df_nlp.doc_description.isnull()].index, 'doc_description'] = 'empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_article_body_similarity(article_id_1, article_id_2):\n",
    "    # Compute the consine similarity based on tfidf of body content\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(df_nlp.loc[article_id_1].doc_body))\n",
    "    doc_b = ' '.join(tokenize(df_nlp.loc[article_id_2].doc_body))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_article_title_similarity(article_id_1, article_id_2):\n",
    "    # Compute the consine similarity based on tfidf of title (doc_full_name)\n",
    "    # think of title tag for seo pagerank\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(df_nlp.loc[article_id_1].doc_full_name))\n",
    "    doc_b = ' '.join(tokenize(df_nlp.loc[article_id_2].doc_full_name))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_article_desc_similarity(article_id_1, article_id_2):\n",
    "    # Compute the consine similarity based on tfidf of desc content (doc_description)\n",
    "    # think of desc tag for seo pagerank\n",
    "    \n",
    "\n",
    "    doc_a = ' '.join(tokenize(df_nlp.loc[article_id_1].doc_description))\n",
    "    doc_b = ' '.join(tokenize(df_nlp.loc[article_id_2].doc_description))\n",
    "    \n",
    "    \n",
    "\n",
    "    # combine to a list of documents\n",
    "    documents = [doc_a, doc_b]\n",
    "\n",
    "    \n",
    "    # instanciate a scikitlearn tfidf vecorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # fit transform to get a sparse matrix\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    # # UnComment out belows to examing details (term features, tfidf values)\n",
    "    # ====================================\n",
    "    # term_features =  vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # convert to readable array \n",
    "    # matrix_array = matrix.toarray()\n",
    "\n",
    "    # # assemble to a dataframe for explor \n",
    "    # tfidf_dataframe = pd.DataFrame(data=matrix_array, columns=term_features)\n",
    "    # ====================================\n",
    "\n",
    "\n",
    "    return cosine_similarity(matrix[:1], matrix[1:])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_article_similarity(article_id_1, article_id_2):\n",
    "    # calculate similary for body, title, desc, then combine a single value.\n",
    "    # think about how google weight. Title tag is very heavy. SEO-wised\n",
    "    # so having 3 consine similarity values, then do a normailized one. \n",
    "    # what's the formular? think of a course, assignments weight x, final exam weight y,\n",
    "    # then what's total grade.\n",
    "    # https://www.indeed.com/career-advice/career-development/how-to-calculate-weighted-average\n",
    "    \n",
    "    similarity_title = _compute_article_title_similarity(article_id_1,article_id_2)\n",
    "    similarity_body = _compute_article_body_similarity(article_id_1,article_id_2)\n",
    "    similarity_desc = _compute_article_desc_similarity(article_id_1,article_id_2)\n",
    "    \n",
    "    # a weighted sum caluculation for final score\n",
    "    \n",
    "    overall = similarity_title * 0.5 + similarity_body * 0.4 + similarity_desc * 0.1\n",
    "    \n",
    "    return overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07122043497708203"
      ]
     },
     "execution_count": 1576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_article_similarity(55,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1794,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar articles for a given article\n",
    "def find_similar_articles(article_id, data=df_nlp):\n",
    "    \n",
    "    article_ids_all = data.index\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for i in article_ids_all:\n",
    "        # print(f\"\\n[i]: {i}\")\n",
    "        \n",
    "        if i == article_id:\n",
    "            continue\n",
    "\n",
    "        similarity_score = compute_article_similarity(article_id,i)\n",
    "        similarity_dict[i] = similarity_score\n",
    "        # print(f\"[similarity_score]: {similarity_score}\")\n",
    "    \n",
    "    \n",
    "    similarity_ds = pd.Series(data=similarity_dict.values(), \n",
    "                           index=similarity_dict.keys()).sort_values(ascending=False).index\n",
    "    \n",
    "    return list(similarity_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1795,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[389,\n",
       " 993,\n",
       " 949,\n",
       " 592,\n",
       " 714,\n",
       " 117,\n",
       " 678,\n",
       " 942,\n",
       " 231,\n",
       " 925,\n",
       " 15,\n",
       " 463,\n",
       " 353,\n",
       " 835,\n",
       " 907,\n",
       " 284,\n",
       " 977,\n",
       " 600,\n",
       " 595,\n",
       " 997,\n",
       " 239,\n",
       " 240,\n",
       " 411,\n",
       " 45,\n",
       " 119,\n",
       " 473,\n",
       " 375,\n",
       " 934,\n",
       " 560,\n",
       " 264,\n",
       " 893,\n",
       " 515,\n",
       " 54,\n",
       " 404,\n",
       " 849,\n",
       " 815,\n",
       " 380,\n",
       " 486,\n",
       " 632,\n",
       " 674,\n",
       " 424,\n",
       " 443,\n",
       " 55,\n",
       " 9,\n",
       " 647,\n",
       " 398,\n",
       " 161,\n",
       " 707,\n",
       " 193,\n",
       " 948,\n",
       " 844,\n",
       " 721,\n",
       " 681,\n",
       " 223,\n",
       " 58,\n",
       " 961,\n",
       " 403,\n",
       " 77,\n",
       " 874,\n",
       " 1005,\n",
       " 762,\n",
       " 96,\n",
       " 143,\n",
       " 112,\n",
       " 1049,\n",
       " 938,\n",
       " 121,\n",
       " 249,\n",
       " 235,\n",
       " 4,\n",
       " 20,\n",
       " 341,\n",
       " 689,\n",
       " 842,\n",
       " 809,\n",
       " 489,\n",
       " 744,\n",
       " 141,\n",
       " 1042,\n",
       " 872,\n",
       " 941,\n",
       " 400,\n",
       " 594,\n",
       " 624,\n",
       " 302,\n",
       " 318,\n",
       " 342,\n",
       " 468,\n",
       " 272,\n",
       " 760,\n",
       " 706,\n",
       " 280,\n",
       " 643,\n",
       " 562,\n",
       " 47,\n",
       " 27,\n",
       " 956,\n",
       " 499,\n",
       " 469,\n",
       " 385,\n",
       " 7,\n",
       " 84,\n",
       " 110,\n",
       " 326,\n",
       " 241,\n",
       " 801,\n",
       " 728,\n",
       " 228,\n",
       " 378,\n",
       " 683,\n",
       " 198,\n",
       " 362,\n",
       " 138,\n",
       " 147,\n",
       " 369,\n",
       " 593,\n",
       " 154,\n",
       " 569,\n",
       " 847,\n",
       " 104,\n",
       " 772,\n",
       " 447,\n",
       " 787,\n",
       " 1028,\n",
       " 226,\n",
       " 176,\n",
       " 578,\n",
       " 936,\n",
       " 958,\n",
       " 13,\n",
       " 65,\n",
       " 48,\n",
       " 334,\n",
       " 267,\n",
       " 432,\n",
       " 634,\n",
       " 146,\n",
       " 776,\n",
       " 293,\n",
       " 828,\n",
       " 313,\n",
       " 44,\n",
       " 720,\n",
       " 717,\n",
       " 846,\n",
       " 97,\n",
       " 523,\n",
       " 120,\n",
       " 869,\n",
       " 575,\n",
       " 691,\n",
       " 800,\n",
       " 269,\n",
       " 1021,\n",
       " 430,\n",
       " 262,\n",
       " 351,\n",
       " 11,\n",
       " 12,\n",
       " 651,\n",
       " 864,\n",
       " 62,\n",
       " 644,\n",
       " 703,\n",
       " 729,\n",
       " 136,\n",
       " 230,\n",
       " 773,\n",
       " 856,\n",
       " 935,\n",
       " 73,\n",
       " 444,\n",
       " 1001,\n",
       " 87,\n",
       " 974,\n",
       " 865,\n",
       " 736,\n",
       " 416,\n",
       " 510,\n",
       " 245,\n",
       " 101,\n",
       " 325,\n",
       " 476,\n",
       " 5,\n",
       " 491,\n",
       " 1017,\n",
       " 829,\n",
       " 672,\n",
       " 232,\n",
       " 304,\n",
       " 1013,\n",
       " 698,\n",
       " 807,\n",
       " 374,\n",
       " 126,\n",
       " 306,\n",
       " 474,\n",
       " 319,\n",
       " 631,\n",
       " 1034,\n",
       " 183,\n",
       " 440,\n",
       " 780,\n",
       " 682,\n",
       " 461,\n",
       " 395,\n",
       " 710,\n",
       " 210,\n",
       " 962,\n",
       " 410,\n",
       " 557,\n",
       " 861,\n",
       " 78,\n",
       " 965,\n",
       " 836,\n",
       " 1035,\n",
       " 793,\n",
       " 712,\n",
       " 687,\n",
       " 95,\n",
       " 103,\n",
       " 108,\n",
       " 36,\n",
       " 251,\n",
       " 529,\n",
       " 75,\n",
       " 363,\n",
       " 502,\n",
       " 990,\n",
       " 966,\n",
       " 409,\n",
       " 732,\n",
       " 617,\n",
       " 584,\n",
       " 820,\n",
       " 448,\n",
       " 739,\n",
       " 427,\n",
       " 859,\n",
       " 564,\n",
       " 172,\n",
       " 191,\n",
       " 100,\n",
       " 621,\n",
       " 213,\n",
       " 653,\n",
       " 428,\n",
       " 663,\n",
       " 76,\n",
       " 711,\n",
       " 725,\n",
       " 194,\n",
       " 827,\n",
       " 604,\n",
       " 216,\n",
       " 758,\n",
       " 441,\n",
       " 540,\n",
       " 953,\n",
       " 192,\n",
       " 888,\n",
       " 417,\n",
       " 521,\n",
       " 524,\n",
       " 122,\n",
       " 1022,\n",
       " 144,\n",
       " 583,\n",
       " 1025,\n",
       " 29,\n",
       " 850,\n",
       " 753,\n",
       " 517,\n",
       " 202,\n",
       " 640,\n",
       " 766,\n",
       " 892,\n",
       " 364,\n",
       " 1039,\n",
       " 999,\n",
       " 217,\n",
       " 808,\n",
       " 559,\n",
       " 708,\n",
       " 52,\n",
       " 464,\n",
       " 344,\n",
       " 479,\n",
       " 764,\n",
       " 733,\n",
       " 665,\n",
       " 543,\n",
       " 134,\n",
       " 743,\n",
       " 685,\n",
       " 25,\n",
       " 1000,\n",
       " 556,\n",
       " 960,\n",
       " 348,\n",
       " 106,\n",
       " 980,\n",
       " 701,\n",
       " 260,\n",
       " 153,\n",
       " 881,\n",
       " 668,\n",
       " 763,\n",
       " 799,\n",
       " 113,\n",
       " 426,\n",
       " 382,\n",
       " 288,\n",
       " 244,\n",
       " 771,\n",
       " 21,\n",
       " 493,\n",
       " 53,\n",
       " 855,\n",
       " 3,\n",
       " 88,\n",
       " 534,\n",
       " 174,\n",
       " 1043,\n",
       " 102,\n",
       " 2,\n",
       " 988,\n",
       " 796,\n",
       " 549,\n",
       " 221,\n",
       " 419,\n",
       " 676,\n",
       " 459,\n",
       " 626,\n",
       " 450,\n",
       " 740,\n",
       " 929,\n",
       " 623,\n",
       " 694,\n",
       " 910,\n",
       " 114,\n",
       " 335,\n",
       " 518,\n",
       " 208,\n",
       " 139,\n",
       " 406,\n",
       " 769,\n",
       " 123,\n",
       " 716,\n",
       " 125,\n",
       " 964,\n",
       " 177,\n",
       " 49,\n",
       " 806,\n",
       " 973,\n",
       " 42,\n",
       " 298,\n",
       " 320,\n",
       " 933,\n",
       " 636,\n",
       " 512,\n",
       " 446,\n",
       " 646,\n",
       " 212,\n",
       " 972,\n",
       " 975,\n",
       " 184,\n",
       " 915,\n",
       " 688,\n",
       " 775,\n",
       " 456,\n",
       " 56,\n",
       " 278,\n",
       " 115,\n",
       " 211,\n",
       " 203,\n",
       " 111,\n",
       " 669,\n",
       " 567,\n",
       " 14,\n",
       " 453,\n",
       " 897,\n",
       " 22,\n",
       " 601,\n",
       " 848,\n",
       " 902,\n",
       " 1019,\n",
       " 274,\n",
       " 995,\n",
       " 950,\n",
       " 625,\n",
       " 791,\n",
       " 250,\n",
       " 969,\n",
       " 883,\n",
       " 655,\n",
       " 768,\n",
       " 66,\n",
       " 957,\n",
       " 778,\n",
       " 797,\n",
       " 613,\n",
       " 246,\n",
       " 323,\n",
       " 531,\n",
       " 186,\n",
       " 418,\n",
       " 330,\n",
       " 10,\n",
       " 587,\n",
       " 148,\n",
       " 1033,\n",
       " 1037,\n",
       " 1026,\n",
       " 159,\n",
       " 533,\n",
       " 501,\n",
       " 454,\n",
       " 661,\n",
       " 98,\n",
       " 487,\n",
       " 899,\n",
       " 642,\n",
       " 412,\n",
       " 852,\n",
       " 151,\n",
       " 229,\n",
       " 607,\n",
       " 281,\n",
       " 679,\n",
       " 649,\n",
       " 8,\n",
       " 35,\n",
       " 415,\n",
       " 72,\n",
       " 145,\n",
       " 349,\n",
       " 421,\n",
       " 582,\n",
       " 639,\n",
       " 436,\n",
       " 37,\n",
       " 275,\n",
       " 373,\n",
       " 658,\n",
       " 586,\n",
       " 985,\n",
       " 394,\n",
       " 129,\n",
       " 350,\n",
       " 573,\n",
       " 994,\n",
       " 285,\n",
       " 445,\n",
       " 522,\n",
       " 504,\n",
       " 519,\n",
       " 92,\n",
       " 305,\n",
       " 352,\n",
       " 885,\n",
       " 581,\n",
       " 438,\n",
       " 686,\n",
       " 383,\n",
       " 779,\n",
       " 978,\n",
       " 816,\n",
       " 31,\n",
       " 135,\n",
       " 439,\n",
       " 903,\n",
       " 822,\n",
       " 552,\n",
       " 188,\n",
       " 347,\n",
       " 452,\n",
       " 393,\n",
       " 268,\n",
       " 149,\n",
       " 568,\n",
       " 242,\n",
       " 550,\n",
       " 824,\n",
       " 170,\n",
       " 505,\n",
       " 219,\n",
       " 931,\n",
       " 690,\n",
       " 508,\n",
       " 1041,\n",
       " 105,\n",
       " 821,\n",
       " 329,\n",
       " 798,\n",
       " 63,\n",
       " 940,\n",
       " 628,\n",
       " 301,\n",
       " 741,\n",
       " 546,\n",
       " 737,\n",
       " 197,\n",
       " 1050,\n",
       " 390,\n",
       " 913,\n",
       " 619,\n",
       " 930,\n",
       " 548,\n",
       " 659,\n",
       " 675,\n",
       " 127,\n",
       " 513,\n",
       " 858,\n",
       " 86,\n",
       " 943,\n",
       " 719,\n",
       " 322,\n",
       " 503,\n",
       " 908,\n",
       " 831,\n",
       " 46,\n",
       " 805,\n",
       " 745,\n",
       " 91,\n",
       " 425,\n",
       " 182,\n",
       " 128,\n",
       " 782,\n",
       " 884,\n",
       " 530,\n",
       " 317,\n",
       " 734,\n",
       " 392,\n",
       " 490,\n",
       " 811,\n",
       " 673,\n",
       " 71,\n",
       " 455,\n",
       " 160,\n",
       " 277,\n",
       " 558,\n",
       " 0,\n",
       " 810,\n",
       " 696,\n",
       " 576,\n",
       " 413,\n",
       " 397,\n",
       " 986,\n",
       " 162,\n",
       " 606,\n",
       " 693,\n",
       " 866,\n",
       " 64,\n",
       " 616,\n",
       " 1038,\n",
       " 283,\n",
       " 38,\n",
       " 588,\n",
       " 379,\n",
       " 509,\n",
       " 402,\n",
       " 93,\n",
       " 654,\n",
       " 615,\n",
       " 50,\n",
       " 263,\n",
       " 74,\n",
       " 794,\n",
       " 596,\n",
       " 467,\n",
       " 218,\n",
       " 882,\n",
       " 368,\n",
       " 124,\n",
       " 1040,\n",
       " 266,\n",
       " 705,\n",
       " 921,\n",
       " 355,\n",
       " 166,\n",
       " 803,\n",
       " 585,\n",
       " 371,\n",
       " 442,\n",
       " 377,\n",
       " 920,\n",
       " 589,\n",
       " 321,\n",
       " 912,\n",
       " 1002,\n",
       " 1018,\n",
       " 259,\n",
       " 247,\n",
       " 834,\n",
       " 370,\n",
       " 252,\n",
       " 597,\n",
       " 33,\n",
       " 214,\n",
       " 751,\n",
       " 635,\n",
       " 492,\n",
       " 458,\n",
       " 754,\n",
       " 328,\n",
       " 832,\n",
       " 237,\n",
       " 257,\n",
       " 650,\n",
       " 603,\n",
       " 630,\n",
       " 457,\n",
       " 222,\n",
       " 327,\n",
       " 622,\n",
       " 795,\n",
       " 551,\n",
       " 99,\n",
       " 906,\n",
       " 360,\n",
       " 1023,\n",
       " 339,\n",
       " 179,\n",
       " 173,\n",
       " 670,\n",
       " 752,\n",
       " 367,\n",
       " 536,\n",
       " 923,\n",
       " 662,\n",
       " 372,\n",
       " 116,\n",
       " 1,\n",
       " 199,\n",
       " 387,\n",
       " 554,\n",
       " 475,\n",
       " 132,\n",
       " 547,\n",
       " 384,\n",
       " 19,\n",
       " 784,\n",
       " 1015,\n",
       " 167,\n",
       " 431,\n",
       " 854,\n",
       " 570,\n",
       " 695,\n",
       " 664,\n",
       " 843,\n",
       " 555,\n",
       " 171,\n",
       " 482,\n",
       " 612,\n",
       " 652,\n",
       " 983,\n",
       " 895,\n",
       " 39,\n",
       " 478,\n",
       " 528,\n",
       " 316,\n",
       " 83,\n",
       " 757,\n",
       " 300,\n",
       " 1045,\n",
       " 131,\n",
       " 1029,\n",
       " 748,\n",
       " 423,\n",
       " 175,\n",
       " 60,\n",
       " 566,\n",
       " 157,\n",
       " 702,\n",
       " 460,\n",
       " 954,\n",
       " 742,\n",
       " 618,\n",
       " 919,\n",
       " 496,\n",
       " 340,\n",
       " 853,\n",
       " 273,\n",
       " 1030,\n",
       " 253,\n",
       " 207,\n",
       " 168,\n",
       " 178,\n",
       " 574,\n",
       " 968,\n",
       " 945,\n",
       " 435,\n",
       " 833,\n",
       " 434,\n",
       " 258,\n",
       " 433,\n",
       " 786,\n",
       " 814,\n",
       " 361,\n",
       " 788,\n",
       " 150,\n",
       " 867,\n",
       " 718,\n",
       " 656,\n",
       " 946,\n",
       " 620,\n",
       " 727,\n",
       " 488,\n",
       " 818,\n",
       " 610,\n",
       " 516,\n",
       " 470,\n",
       " 477,\n",
       " 255,\n",
       " 224,\n",
       " 817,\n",
       " 887,\n",
       " 916,\n",
       " 1020,\n",
       " 984,\n",
       " 331,\n",
       " 1014,\n",
       " 466,\n",
       " 580,\n",
       " 605,\n",
       " 345,\n",
       " 1006,\n",
       " 645,\n",
       " 591,\n",
       " 785,\n",
       " 880,\n",
       " 254,\n",
       " 67,\n",
       " 40,\n",
       " 967,\n",
       " 599,\n",
       " 998,\n",
       " 781,\n",
       " 579,\n",
       " 709,\n",
       " 391,\n",
       " 449,\n",
       " 287,\n",
       " 43,\n",
       " 215,\n",
       " 756,\n",
       " 366,\n",
       " 133,\n",
       " 24,\n",
       " 878,\n",
       " 236,\n",
       " 495,\n",
       " 225,\n",
       " 697,\n",
       " 947,\n",
       " 271,\n",
       " 158,\n",
       " 1003,\n",
       " 614,\n",
       " 541,\n",
       " 437,\n",
       " 180,\n",
       " 876,\n",
       " 89,\n",
       " 959,\n",
       " 414,\n",
       " 918,\n",
       " 248,\n",
       " 804,\n",
       " 289,\n",
       " 527,\n",
       " 17,\n",
       " 109,\n",
       " 730,\n",
       " 765,\n",
       " 671,\n",
       " 94,\n",
       " 602,\n",
       " 917,\n",
       " 1011,\n",
       " 937,\n",
       " 724,\n",
       " 462,\n",
       " 971,\n",
       " 169,\n",
       " 494,\n",
       " 873,\n",
       " 715,\n",
       " 970,\n",
       " 34,\n",
       " 932,\n",
       " 471,\n",
       " 851,\n",
       " 1008,\n",
       " 726,\n",
       " 537,\n",
       " 26,\n",
       " 890,\n",
       " 359,\n",
       " 343,\n",
       " 525,\n",
       " 358,\n",
       " 611,\n",
       " 922,\n",
       " 979,\n",
       " 201,\n",
       " 877,\n",
       " 511,\n",
       " 914,\n",
       " 429,\n",
       " 722,\n",
       " 747,\n",
       " 51,\n",
       " 238,\n",
       " 862,\n",
       " 196,\n",
       " 265,\n",
       " 939,\n",
       " 870,\n",
       " 951,\n",
       " 497,\n",
       " 838,\n",
       " 189,\n",
       " 871,\n",
       " 571,\n",
       " 152,\n",
       " 332,\n",
       " 904,\n",
       " 989,\n",
       " 61,\n",
       " 381,\n",
       " 767,\n",
       " 981,\n",
       " 826,\n",
       " 308,\n",
       " 130,\n",
       " 205,\n",
       " 1044,\n",
       " 608,\n",
       " 1012,\n",
       " 812,\n",
       " 365,\n",
       " 770,\n",
       " 388,\n",
       " 30,\n",
       " 684,\n",
       " 307,\n",
       " 572,\n",
       " 713,\n",
       " 282,\n",
       " 627,\n",
       " 792,\n",
       " 987,\n",
       " 894,\n",
       " 542,\n",
       " 830,\n",
       " 609,\n",
       " 164,\n",
       " 845,\n",
       " 401,\n",
       " 1046,\n",
       " 909,\n",
       " 407,\n",
       " 1027,\n",
       " 553,\n",
       " 405,\n",
       " 819,\n",
       " 860,\n",
       " 590,\n",
       " 68,\n",
       " 837,\n",
       " 1016,\n",
       " 598,\n",
       " 16,\n",
       " 290,\n",
       " 28,\n",
       " 790,\n",
       " 310,\n",
       " 243,\n",
       " 735,\n",
       " 311,\n",
       " 761,\n",
       " 276,\n",
       " 641,\n",
       " 900,\n",
       " 336,\n",
       " 286,\n",
       " 484,\n",
       " 209,\n",
       " 545,\n",
       " 376,\n",
       " 996,\n",
       " 18,\n",
       " 142,\n",
       " 875,\n",
       " 465,\n",
       " 723,\n",
       " 982,\n",
       " 315,\n",
       " 185,\n",
       " 155,\n",
       " 514,\n",
       " 396,\n",
       " 955,\n",
       " 80,\n",
       " 783,\n",
       " 991,\n",
       " 6,\n",
       " 746,\n",
       " 911,\n",
       " 234,\n",
       " 220,\n",
       " 952,\n",
       " 165,\n",
       " 57,\n",
       " 738,\n",
       " 731,\n",
       " 386,\n",
       " 755,\n",
       " 422,\n",
       " 657,\n",
       " 506,\n",
       " 118,\n",
       " 140,\n",
       " 928,\n",
       " 565,\n",
       " 314,\n",
       " 200,\n",
       " 648,\n",
       " 324,\n",
       " 749,\n",
       " 233,\n",
       " 813,\n",
       " 680,\n",
       " 156,\n",
       " 357,\n",
       " 500,\n",
       " 1024,\n",
       " 137,\n",
       " 777,\n",
       " 802,\n",
       " 312,\n",
       " 85,\n",
       " 905,\n",
       " 69,\n",
       " 498,\n",
       " 337,\n",
       " 408,\n",
       " 81,\n",
       " 924,\n",
       " 399,\n",
       " 107,\n",
       " 82,\n",
       " 901,\n",
       " 520,\n",
       " 774,\n",
       " 863,\n",
       " 354,\n",
       " 309,\n",
       " 356,\n",
       " 338,\n",
       " 291,\n",
       " 1004,\n",
       " 1048,\n",
       " 296,\n",
       " 32,\n",
       " 699,\n",
       " 195,\n",
       " 70,\n",
       " 279,\n",
       " 181,\n",
       " 976,\n",
       " 898,\n",
       " 692,\n",
       " 297,\n",
       " 868,\n",
       " 667,\n",
       " 187,\n",
       " 1047,\n",
       " 227,\n",
       " 879,\n",
       " 190,\n",
       " 823,\n",
       " 577,\n",
       " 638,\n",
       " 41,\n",
       " 637,\n",
       " 789,\n",
       " 963,\n",
       " 700,\n",
       " 1007,\n",
       " 256,\n",
       " 666,\n",
       " 23,\n",
       " 90,\n",
       " 294,\n",
       " 507,\n",
       " 927,\n",
       " 535,\n",
       " ...]"
      ]
     },
     "execution_count": 1795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate in real time, it might take some seconds\n",
    "# (since it loop to all articles against the targeted article)\n",
    "\n",
    "relavant_articles = find_similar_articles(420)\n",
    "\n",
    "relavant_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>What a difference a version number makes! With...</td>\n",
       "      <td>Apache Spark™ 2.0: Impressive Improvements to ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>The Apache Spark website documents the propert...</td>\n",
       "      <td>Configuring the Apache Spark SQL Context</td>\n",
       "      <td>Live</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>The Apache Spark SQL component has several sub...</td>\n",
       "      <td>Apache Spark SQL Analyzer Resolves Order-by Co...</td>\n",
       "      <td>Live</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>APACHE SPARK ANALYTICSCombine Apache® Spark™ w...</td>\n",
       "      <td>Combine Apache® Spark™ with other cloud servic...</td>\n",
       "      <td>Apache Spark Analytics</td>\n",
       "      <td>Live</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>From the big crop of books about Apache Spark™...</td>\n",
       "      <td>A Survey of Books about Apache Spark™</td>\n",
       "      <td>Live</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>This post provides a brief summary of sample c...</td>\n",
       "      <td>Apache Spark™ 2.0: Migrating Applications</td>\n",
       "      <td>Live</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>{ spark .tc } * Community\\r\\n * Projects\\r\\n *...</td>\n",
       "      <td>Spark SQL Version 1.6 runs queries faster! Tha...</td>\n",
       "      <td>Spark SQL - Rapid Performance Evolution</td>\n",
       "      <td>Live</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Sean looks back on his first encounter with Sp...</td>\n",
       "      <td>Interview with Sean Li, New Apache Spark™ Comm...</td>\n",
       "      <td>Live</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Get faster queries and write less code too. Le...</td>\n",
       "      <td>Speed your SQL Queries with Spark SQL</td>\n",
       "      <td>Live</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video shows you how to use the Spark SQL ...</td>\n",
       "      <td>Build SQL queries with Apache Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Early methods to integrate machine learning us...</td>\n",
       "      <td>Apache Spark™ 2.0: Extend Structured Streaming...</td>\n",
       "      <td>Live</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Skip navigation Upload Sign in SearchLoading.....</td>\n",
       "      <td>Learn Why and How To Use Spark for large amoun...</td>\n",
       "      <td>What is Spark?</td>\n",
       "      <td>Live</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>RStudio Blog * Home\\r\\n\\r\\n * Subscribe to fee...</td>\n",
       "      <td>We’re excited today to announce sparklyr, a ne...</td>\n",
       "      <td>sparklyr — R interface for Apache Spark</td>\n",
       "      <td>Live</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>How to build SQL Queries in a Scala notebook u...</td>\n",
       "      <td>Build SQL Queries in a Scala notebook using Ap...</td>\n",
       "      <td>Live</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>How to build SQL Queries in a Scala notebook u...</td>\n",
       "      <td>Build Spark SQL Queries</td>\n",
       "      <td>Live</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>Now that the dust has settled on Apache Spark ...</td>\n",
       "      <td>Apache Spark 2.0: Machine Learning. Under the ...</td>\n",
       "      <td>Live</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>* Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...</td>\n",
       "      <td>A handful of talks at the recent Spark Summit ...</td>\n",
       "      <td>Apache Spark as the New Engine of Genomics</td>\n",
       "      <td>Live</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>In this post I will show you how to use the IB...</td>\n",
       "      <td>Access IBM Analytics for Apache Spark from RSt...</td>\n",
       "      <td>Live</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>Learn how to create a connection to dashDB dat...</td>\n",
       "      <td>Load dashDB Data with Apache Spark</td>\n",
       "      <td>Live</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Skip to main content IBM developerWorks / Deve...</td>\n",
       "      <td>One of the best things about Apache Spark is t...</td>\n",
       "      <td>Apache Spark: Upgrade and speed-up your analytics</td>\n",
       "      <td>Live</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     doc_body  \\\n",
       "article_id                                                      \n",
       "389         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "993         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "949         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "592         APACHE SPARK ANALYTICSCombine Apache® Spark™ w...   \n",
       "714         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "117         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "678         { spark .tc } * Community\\r\\n * Projects\\r\\n *...   \n",
       "942         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "231         Skip to main content IBM developerWorks / Deve...   \n",
       "925         Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "15          * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "463         Skip navigation Upload Sign in SearchLoading.....   \n",
       "353         RStudio Blog * Home\\r\\n\\r\\n * Subscribe to fee...   \n",
       "835         Skip to main content IBM developerWorks / Deve...   \n",
       "907         Skip to main content IBM developerWorks / Deve...   \n",
       "284         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "977         * Home\\r\\n * Community\\r\\n * Projects\\r\\n * Bl...   \n",
       "600         Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "595         Skip to main content IBM developerWorks / Deve...   \n",
       "997         Skip to main content IBM developerWorks / Deve...   \n",
       "\n",
       "                                              doc_description  \\\n",
       "article_id                                                      \n",
       "389         What a difference a version number makes! With...   \n",
       "993         The Apache Spark website documents the propert...   \n",
       "949         The Apache Spark SQL component has several sub...   \n",
       "592         Combine Apache® Spark™ with other cloud servic...   \n",
       "714         From the big crop of books about Apache Spark™...   \n",
       "117         This post provides a brief summary of sample c...   \n",
       "678         Spark SQL Version 1.6 runs queries faster! Tha...   \n",
       "942         Sean looks back on his first encounter with Sp...   \n",
       "231         Get faster queries and write less code too. Le...   \n",
       "925         This video shows you how to use the Spark SQL ...   \n",
       "15          Early methods to integrate machine learning us...   \n",
       "463         Learn Why and How To Use Spark for large amoun...   \n",
       "353         We’re excited today to announce sparklyr, a ne...   \n",
       "835         How to build SQL Queries in a Scala notebook u...   \n",
       "907         How to build SQL Queries in a Scala notebook u...   \n",
       "284         Now that the dust has settled on Apache Spark ...   \n",
       "977         A handful of talks at the recent Spark Summit ...   \n",
       "600         In this post I will show you how to use the IB...   \n",
       "595         Learn how to create a connection to dashDB dat...   \n",
       "997         One of the best things about Apache Spark is t...   \n",
       "\n",
       "                                                doc_full_name doc_status  \\\n",
       "article_id                                                                 \n",
       "389         Apache Spark™ 2.0: Impressive Improvements to ...       Live   \n",
       "993                  Configuring the Apache Spark SQL Context       Live   \n",
       "949         Apache Spark SQL Analyzer Resolves Order-by Co...       Live   \n",
       "592                                    Apache Spark Analytics       Live   \n",
       "714                     A Survey of Books about Apache Spark™       Live   \n",
       "117                 Apache Spark™ 2.0: Migrating Applications       Live   \n",
       "678                   Spark SQL - Rapid Performance Evolution       Live   \n",
       "942         Interview with Sean Li, New Apache Spark™ Comm...       Live   \n",
       "231                     Speed your SQL Queries with Spark SQL       Live   \n",
       "925                Build SQL queries with Apache Spark in DSX       Live   \n",
       "15          Apache Spark™ 2.0: Extend Structured Streaming...       Live   \n",
       "463                                            What is Spark?       Live   \n",
       "353                   sparklyr — R interface for Apache Spark       Live   \n",
       "835         Build SQL Queries in a Scala notebook using Ap...       Live   \n",
       "907                                   Build Spark SQL Queries       Live   \n",
       "284         Apache Spark 2.0: Machine Learning. Under the ...       Live   \n",
       "977                Apache Spark as the New Engine of Genomics       Live   \n",
       "600         Access IBM Analytics for Apache Spark from RSt...       Live   \n",
       "595                        Load dashDB Data with Apache Spark       Live   \n",
       "997         Apache Spark: Upgrade and speed-up your analytics       Live   \n",
       "\n",
       "            article_id  \n",
       "article_id              \n",
       "389                389  \n",
       "993                993  \n",
       "949                949  \n",
       "592                592  \n",
       "714                714  \n",
       "117                117  \n",
       "678                678  \n",
       "942                942  \n",
       "231                231  \n",
       "925                925  \n",
       "15                  15  \n",
       "463                463  \n",
       "353                353  \n",
       "835                835  \n",
       "907                907  \n",
       "284                284  \n",
       "977                977  \n",
       "600                600  \n",
       "595                595  \n",
       "997                997  "
      ]
     },
     "execution_count": 1663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking relavancy. They are order by relavancy. \n",
    "\n",
    "df_nlp.iloc[relavant_articles].head(20)\n",
    "\n",
    "# Looks like related to 'Apache Spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above find_similar_articles work well, however it is calculating on the fly, could take time to load, for user experience, real time calculating too expensive. So make a article_article_similary dataframe for look up. (the content-content rec). This can be act as, people view X article also might be interested in Y article based on relavancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "Make a dataframe, store article-article-similarity.\n",
    "(Based on the cleaned df_content version dataset: df_nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1041</th>\n",
       "      <th>1042</th>\n",
       "      <th>1043</th>\n",
       "      <th>1044</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows × 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "article_id                                                              ...   \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "1046         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1047         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1048         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1049         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1050         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "article_id  1041  1042  1043  1044  1045  1046  1047  1048  1049  1050  \n",
       "article_id                                                              \n",
       "0            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "1046         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1047         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1048         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1049         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1050         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[1051 rows x 1051 columns]"
      ]
     },
     "execution_count": 1673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a user-user dummy dataframe based on shape of user_article_dict_trimmed\n",
    "article_article = pd.DataFrame(\n",
    "                data=np.zeros((len(df_nlp.index),len(df_nlp.index))), \n",
    "                index=df_nlp.index, \n",
    "                columns=df_nlp.index\n",
    "            )\n",
    "\n",
    "article_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0 of 1050.\n",
      "Processing row 1 of 1050.\n",
      "Processing row 2 of 1050.\n",
      "Processing row 3 of 1050.\n",
      "Processing row 4 of 1050.\n",
      "Processing row 5 of 1050.\n",
      "Processing row 6 of 1050.\n",
      "Processing row 7 of 1050.\n",
      "Processing row 8 of 1050.\n",
      "Processing row 9 of 1050.\n",
      "Processing row 10 of 1050.\n",
      "Processing row 11 of 1050.\n",
      "Processing row 12 of 1050.\n",
      "Processing row 13 of 1050.\n",
      "Processing row 14 of 1050.\n",
      "Processing row 15 of 1050.\n",
      "Processing row 16 of 1050.\n",
      "Processing row 17 of 1050.\n",
      "Processing row 18 of 1050.\n",
      "Processing row 19 of 1050.\n",
      "Processing row 20 of 1050.\n",
      "Processing row 21 of 1050.\n",
      "Processing row 22 of 1050.\n",
      "Processing row 23 of 1050.\n",
      "Processing row 24 of 1050.\n",
      "Processing row 25 of 1050.\n",
      "Processing row 26 of 1050.\n",
      "Processing row 27 of 1050.\n",
      "Processing row 28 of 1050.\n",
      "Processing row 29 of 1050.\n",
      "Processing row 30 of 1050.\n",
      "Processing row 31 of 1050.\n",
      "Processing row 32 of 1050.\n",
      "Processing row 33 of 1050.\n",
      "Processing row 34 of 1050.\n",
      "Processing row 35 of 1050.\n",
      "Processing row 36 of 1050.\n",
      "Processing row 37 of 1050.\n",
      "Processing row 38 of 1050.\n",
      "Processing row 39 of 1050.\n",
      "Processing row 40 of 1050.\n",
      "Processing row 41 of 1050.\n",
      "Processing row 42 of 1050.\n",
      "Processing row 43 of 1050.\n",
      "Processing row 44 of 1050.\n",
      "Processing row 45 of 1050.\n",
      "Processing row 46 of 1050.\n",
      "Processing row 47 of 1050.\n",
      "Processing row 48 of 1050.\n",
      "Processing row 49 of 1050.\n",
      "Processing row 50 of 1050.\n",
      "Processing row 51 of 1050.\n",
      "Processing row 52 of 1050.\n",
      "Processing row 53 of 1050.\n",
      "Processing row 54 of 1050.\n",
      "Processing row 55 of 1050.\n",
      "Processing row 56 of 1050.\n",
      "Processing row 57 of 1050.\n",
      "Processing row 58 of 1050.\n",
      "Processing row 59 of 1050.\n",
      "Processing row 60 of 1050.\n",
      "Processing row 61 of 1050.\n",
      "Processing row 62 of 1050.\n",
      "Processing row 63 of 1050.\n",
      "Processing row 64 of 1050.\n",
      "Processing row 65 of 1050.\n",
      "Processing row 66 of 1050.\n",
      "Processing row 67 of 1050.\n",
      "Processing row 68 of 1050.\n",
      "Processing row 69 of 1050.\n",
      "Processing row 70 of 1050.\n",
      "Processing row 71 of 1050.\n",
      "Processing row 72 of 1050.\n",
      "Processing row 73 of 1050.\n",
      "Processing row 74 of 1050.\n",
      "Processing row 75 of 1050.\n",
      "Processing row 76 of 1050.\n",
      "Processing row 77 of 1050.\n",
      "Processing row 78 of 1050.\n",
      "Processing row 79 of 1050.\n",
      "Processing row 80 of 1050.\n",
      "Processing row 81 of 1050.\n",
      "Processing row 82 of 1050.\n",
      "Processing row 83 of 1050.\n",
      "Processing row 84 of 1050.\n",
      "Processing row 85 of 1050.\n",
      "Processing row 86 of 1050.\n",
      "Processing row 87 of 1050.\n",
      "Processing row 88 of 1050.\n",
      "Processing row 89 of 1050.\n",
      "Processing row 90 of 1050.\n",
      "Processing row 91 of 1050.\n",
      "Processing row 92 of 1050.\n",
      "Processing row 93 of 1050.\n",
      "Processing row 94 of 1050.\n",
      "Processing row 95 of 1050.\n",
      "Processing row 96 of 1050.\n",
      "Processing row 97 of 1050.\n",
      "Processing row 98 of 1050.\n",
      "Processing row 99 of 1050.\n",
      "Processing row 100 of 1050.\n",
      "Processing row 101 of 1050.\n",
      "Processing row 102 of 1050.\n",
      "Processing row 103 of 1050.\n",
      "Processing row 104 of 1050.\n",
      "Processing row 105 of 1050.\n",
      "Processing row 106 of 1050.\n",
      "Processing row 107 of 1050.\n",
      "Processing row 108 of 1050.\n",
      "Processing row 109 of 1050.\n",
      "Processing row 110 of 1050.\n",
      "Processing row 111 of 1050.\n",
      "Processing row 112 of 1050.\n",
      "Processing row 113 of 1050.\n",
      "Processing row 114 of 1050.\n",
      "Processing row 115 of 1050.\n",
      "Processing row 116 of 1050.\n",
      "Processing row 117 of 1050.\n",
      "Processing row 118 of 1050.\n",
      "Processing row 119 of 1050.\n",
      "Processing row 120 of 1050.\n",
      "Processing row 121 of 1050.\n",
      "Processing row 122 of 1050.\n",
      "Processing row 123 of 1050.\n",
      "Processing row 124 of 1050.\n",
      "Processing row 125 of 1050.\n",
      "Processing row 126 of 1050.\n",
      "Processing row 127 of 1050.\n",
      "Processing row 128 of 1050.\n",
      "Processing row 129 of 1050.\n",
      "Processing row 130 of 1050.\n",
      "Processing row 131 of 1050.\n",
      "Processing row 132 of 1050.\n",
      "Processing row 133 of 1050.\n",
      "Processing row 134 of 1050.\n",
      "Processing row 135 of 1050.\n",
      "Processing row 136 of 1050.\n",
      "Processing row 137 of 1050.\n",
      "Processing row 138 of 1050.\n",
      "Processing row 139 of 1050.\n",
      "Processing row 140 of 1050.\n",
      "Processing row 141 of 1050.\n",
      "Processing row 142 of 1050.\n",
      "Processing row 143 of 1050.\n",
      "Processing row 144 of 1050.\n",
      "Processing row 145 of 1050.\n",
      "Processing row 146 of 1050.\n",
      "Processing row 147 of 1050.\n",
      "Processing row 148 of 1050.\n",
      "Processing row 149 of 1050.\n",
      "Processing row 150 of 1050.\n",
      "Processing row 151 of 1050.\n",
      "Processing row 152 of 1050.\n",
      "Processing row 153 of 1050.\n",
      "Processing row 154 of 1050.\n",
      "Processing row 155 of 1050.\n",
      "Processing row 156 of 1050.\n",
      "Processing row 157 of 1050.\n",
      "Processing row 158 of 1050.\n",
      "Processing row 159 of 1050.\n",
      "Processing row 160 of 1050.\n",
      "Processing row 161 of 1050.\n",
      "Processing row 162 of 1050.\n",
      "Processing row 163 of 1050.\n",
      "Processing row 164 of 1050.\n",
      "Processing row 165 of 1050.\n",
      "Processing row 166 of 1050.\n",
      "Processing row 167 of 1050.\n",
      "Processing row 168 of 1050.\n",
      "Processing row 169 of 1050.\n",
      "Processing row 170 of 1050.\n",
      "Processing row 171 of 1050.\n",
      "Processing row 172 of 1050.\n",
      "Processing row 173 of 1050.\n",
      "Processing row 174 of 1050.\n",
      "Processing row 175 of 1050.\n",
      "Processing row 176 of 1050.\n",
      "Processing row 177 of 1050.\n",
      "Processing row 178 of 1050.\n",
      "Processing row 179 of 1050.\n",
      "Processing row 180 of 1050.\n",
      "Processing row 181 of 1050.\n",
      "Processing row 182 of 1050.\n",
      "Processing row 183 of 1050.\n",
      "Processing row 184 of 1050.\n",
      "Processing row 185 of 1050.\n",
      "Processing row 186 of 1050.\n",
      "Processing row 187 of 1050.\n",
      "Processing row 188 of 1050.\n",
      "Processing row 189 of 1050.\n",
      "Processing row 190 of 1050.\n",
      "Processing row 191 of 1050.\n",
      "Processing row 192 of 1050.\n",
      "Processing row 193 of 1050.\n",
      "Processing row 194 of 1050.\n",
      "Processing row 195 of 1050.\n",
      "Processing row 196 of 1050.\n",
      "Processing row 197 of 1050.\n",
      "Processing row 198 of 1050.\n",
      "Processing row 199 of 1050.\n",
      "Processing row 200 of 1050.\n",
      "Processing row 201 of 1050.\n",
      "Processing row 202 of 1050.\n",
      "Processing row 203 of 1050.\n",
      "Processing row 204 of 1050.\n",
      "Processing row 205 of 1050.\n",
      "Processing row 206 of 1050.\n",
      "Processing row 207 of 1050.\n",
      "Processing row 208 of 1050.\n",
      "Processing row 209 of 1050.\n",
      "Processing row 210 of 1050.\n",
      "Processing row 211 of 1050.\n",
      "Processing row 212 of 1050.\n",
      "Processing row 213 of 1050.\n",
      "Processing row 214 of 1050.\n",
      "Processing row 215 of 1050.\n",
      "Processing row 216 of 1050.\n",
      "Processing row 217 of 1050.\n",
      "Processing row 218 of 1050.\n",
      "Processing row 219 of 1050.\n",
      "Processing row 220 of 1050.\n",
      "Processing row 221 of 1050.\n",
      "Processing row 222 of 1050.\n",
      "Processing row 223 of 1050.\n",
      "Processing row 224 of 1050.\n",
      "Processing row 225 of 1050.\n",
      "Processing row 226 of 1050.\n",
      "Processing row 227 of 1050.\n",
      "Processing row 228 of 1050.\n",
      "Processing row 229 of 1050.\n",
      "Processing row 230 of 1050.\n",
      "Processing row 231 of 1050.\n",
      "Processing row 232 of 1050.\n",
      "Processing row 233 of 1050.\n",
      "Processing row 234 of 1050.\n",
      "Processing row 235 of 1050.\n",
      "Processing row 236 of 1050.\n",
      "Processing row 237 of 1050.\n",
      "Processing row 238 of 1050.\n",
      "Processing row 239 of 1050.\n",
      "Processing row 240 of 1050.\n",
      "Processing row 241 of 1050.\n",
      "Processing row 242 of 1050.\n",
      "Processing row 243 of 1050.\n",
      "Processing row 244 of 1050.\n",
      "Processing row 245 of 1050.\n",
      "Processing row 246 of 1050.\n",
      "Processing row 247 of 1050.\n",
      "Processing row 248 of 1050.\n",
      "Processing row 249 of 1050.\n",
      "Processing row 250 of 1050.\n",
      "Processing row 251 of 1050.\n",
      "Processing row 252 of 1050.\n",
      "Processing row 253 of 1050.\n",
      "Processing row 254 of 1050.\n",
      "Processing row 255 of 1050.\n",
      "Processing row 256 of 1050.\n",
      "Processing row 257 of 1050.\n",
      "Processing row 258 of 1050.\n",
      "Processing row 259 of 1050.\n",
      "Processing row 260 of 1050.\n",
      "Processing row 261 of 1050.\n",
      "Processing row 262 of 1050.\n",
      "Processing row 263 of 1050.\n",
      "Processing row 264 of 1050.\n",
      "Processing row 265 of 1050.\n",
      "Processing row 266 of 1050.\n",
      "Processing row 267 of 1050.\n",
      "Processing row 268 of 1050.\n",
      "Processing row 269 of 1050.\n",
      "Processing row 270 of 1050.\n",
      "Processing row 271 of 1050.\n",
      "Processing row 272 of 1050.\n",
      "Processing row 273 of 1050.\n",
      "Processing row 274 of 1050.\n",
      "Processing row 275 of 1050.\n",
      "Processing row 276 of 1050.\n",
      "Processing row 277 of 1050.\n",
      "Processing row 278 of 1050.\n",
      "Processing row 279 of 1050.\n",
      "Processing row 280 of 1050.\n",
      "Processing row 281 of 1050.\n",
      "Processing row 282 of 1050.\n",
      "Processing row 283 of 1050.\n",
      "Processing row 284 of 1050.\n",
      "Processing row 285 of 1050.\n",
      "Processing row 286 of 1050.\n",
      "Processing row 287 of 1050.\n",
      "Processing row 288 of 1050.\n",
      "Processing row 289 of 1050.\n",
      "Processing row 290 of 1050.\n",
      "Processing row 291 of 1050.\n",
      "Processing row 292 of 1050.\n",
      "Processing row 293 of 1050.\n",
      "Processing row 294 of 1050.\n",
      "Processing row 295 of 1050.\n",
      "Processing row 296 of 1050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 297 of 1050.\n",
      "Processing row 298 of 1050.\n",
      "Processing row 299 of 1050.\n",
      "Processing row 300 of 1050.\n",
      "Processing row 301 of 1050.\n",
      "Processing row 302 of 1050.\n",
      "Processing row 303 of 1050.\n",
      "Processing row 304 of 1050.\n",
      "Processing row 305 of 1050.\n",
      "Processing row 306 of 1050.\n",
      "Processing row 307 of 1050.\n",
      "Processing row 308 of 1050.\n",
      "Processing row 309 of 1050.\n",
      "Processing row 310 of 1050.\n",
      "Processing row 311 of 1050.\n",
      "Processing row 312 of 1050.\n",
      "Processing row 313 of 1050.\n",
      "Processing row 314 of 1050.\n",
      "Processing row 315 of 1050.\n",
      "Processing row 316 of 1050.\n",
      "Processing row 317 of 1050.\n",
      "Processing row 318 of 1050.\n",
      "Processing row 319 of 1050.\n",
      "Processing row 320 of 1050.\n",
      "Processing row 321 of 1050.\n",
      "Processing row 322 of 1050.\n",
      "Processing row 323 of 1050.\n",
      "Processing row 324 of 1050.\n",
      "Processing row 325 of 1050.\n",
      "Processing row 326 of 1050.\n",
      "Processing row 327 of 1050.\n",
      "Processing row 328 of 1050.\n",
      "Processing row 329 of 1050.\n",
      "Processing row 330 of 1050.\n",
      "Processing row 331 of 1050.\n",
      "Processing row 332 of 1050.\n",
      "Processing row 333 of 1050.\n",
      "Processing row 334 of 1050.\n",
      "Processing row 335 of 1050.\n",
      "Processing row 336 of 1050.\n",
      "Processing row 337 of 1050.\n",
      "Processing row 338 of 1050.\n",
      "Processing row 339 of 1050.\n",
      "Processing row 340 of 1050.\n",
      "Processing row 341 of 1050.\n",
      "Processing row 342 of 1050.\n",
      "Processing row 343 of 1050.\n",
      "Processing row 344 of 1050.\n",
      "Processing row 345 of 1050.\n",
      "Processing row 346 of 1050.\n",
      "Processing row 347 of 1050.\n",
      "Processing row 348 of 1050.\n",
      "Processing row 349 of 1050.\n",
      "Processing row 350 of 1050.\n",
      "Processing row 351 of 1050.\n",
      "Processing row 352 of 1050.\n",
      "Processing row 353 of 1050.\n",
      "Processing row 354 of 1050.\n",
      "Processing row 355 of 1050.\n",
      "Processing row 356 of 1050.\n",
      "Processing row 357 of 1050.\n",
      "Processing row 358 of 1050.\n",
      "Processing row 359 of 1050.\n",
      "Processing row 360 of 1050.\n",
      "Processing row 361 of 1050.\n",
      "Processing row 362 of 1050.\n",
      "Processing row 363 of 1050.\n",
      "Processing row 364 of 1050.\n",
      "Processing row 365 of 1050.\n",
      "Processing row 366 of 1050.\n",
      "Processing row 367 of 1050.\n",
      "Processing row 368 of 1050.\n",
      "Processing row 369 of 1050.\n",
      "Processing row 370 of 1050.\n",
      "Processing row 371 of 1050.\n",
      "Processing row 372 of 1050.\n",
      "Processing row 373 of 1050.\n",
      "Processing row 374 of 1050.\n",
      "Processing row 375 of 1050.\n",
      "Processing row 376 of 1050.\n",
      "Processing row 377 of 1050.\n",
      "Processing row 378 of 1050.\n",
      "Processing row 379 of 1050.\n",
      "Processing row 380 of 1050.\n",
      "Processing row 381 of 1050.\n",
      "Processing row 382 of 1050.\n",
      "Processing row 383 of 1050.\n",
      "Processing row 384 of 1050.\n",
      "Processing row 385 of 1050.\n",
      "Processing row 386 of 1050.\n",
      "Processing row 387 of 1050.\n",
      "Processing row 388 of 1050.\n",
      "Processing row 389 of 1050.\n",
      "Processing row 390 of 1050.\n",
      "Processing row 391 of 1050.\n",
      "Processing row 392 of 1050.\n",
      "Processing row 393 of 1050.\n",
      "Processing row 394 of 1050.\n",
      "Processing row 395 of 1050.\n",
      "Processing row 396 of 1050.\n",
      "Processing row 397 of 1050.\n",
      "Processing row 398 of 1050.\n",
      "Processing row 399 of 1050.\n",
      "Processing row 400 of 1050.\n",
      "Processing row 401 of 1050.\n",
      "Processing row 402 of 1050.\n",
      "Processing row 403 of 1050.\n",
      "Processing row 404 of 1050.\n",
      "Processing row 405 of 1050.\n",
      "Processing row 406 of 1050.\n",
      "Processing row 407 of 1050.\n",
      "Processing row 408 of 1050.\n",
      "Processing row 409 of 1050.\n",
      "Processing row 410 of 1050.\n",
      "Processing row 411 of 1050.\n",
      "Processing row 412 of 1050.\n",
      "Processing row 413 of 1050.\n",
      "Processing row 414 of 1050.\n",
      "Processing row 415 of 1050.\n",
      "Processing row 416 of 1050.\n",
      "Processing row 417 of 1050.\n",
      "Processing row 418 of 1050.\n",
      "Processing row 419 of 1050.\n",
      "Processing row 420 of 1050.\n",
      "Processing row 421 of 1050.\n",
      "Processing row 422 of 1050.\n",
      "Processing row 423 of 1050.\n",
      "Processing row 424 of 1050.\n",
      "Processing row 425 of 1050.\n",
      "Processing row 426 of 1050.\n",
      "Processing row 427 of 1050.\n",
      "Processing row 428 of 1050.\n",
      "Processing row 429 of 1050.\n",
      "Processing row 430 of 1050.\n",
      "Processing row 431 of 1050.\n",
      "Processing row 432 of 1050.\n",
      "Processing row 433 of 1050.\n",
      "Processing row 434 of 1050.\n",
      "Processing row 435 of 1050.\n",
      "Processing row 436 of 1050.\n",
      "Processing row 437 of 1050.\n",
      "Processing row 438 of 1050.\n",
      "Processing row 439 of 1050.\n",
      "Processing row 440 of 1050.\n",
      "Processing row 441 of 1050.\n",
      "Processing row 442 of 1050.\n",
      "Processing row 443 of 1050.\n",
      "Processing row 444 of 1050.\n",
      "Processing row 445 of 1050.\n",
      "Processing row 446 of 1050.\n",
      "Processing row 447 of 1050.\n",
      "Processing row 448 of 1050.\n",
      "Processing row 449 of 1050.\n",
      "Processing row 450 of 1050.\n",
      "Processing row 451 of 1050.\n",
      "Processing row 452 of 1050.\n",
      "Processing row 453 of 1050.\n",
      "Processing row 454 of 1050.\n",
      "Processing row 455 of 1050.\n",
      "Processing row 456 of 1050.\n",
      "Processing row 457 of 1050.\n",
      "Processing row 458 of 1050.\n",
      "Processing row 459 of 1050.\n",
      "Processing row 460 of 1050.\n",
      "Processing row 461 of 1050.\n",
      "Processing row 462 of 1050.\n",
      "Processing row 463 of 1050.\n",
      "Processing row 464 of 1050.\n",
      "Processing row 465 of 1050.\n",
      "Processing row 466 of 1050.\n",
      "Processing row 467 of 1050.\n",
      "Processing row 468 of 1050.\n",
      "Processing row 469 of 1050.\n",
      "Processing row 470 of 1050.\n",
      "Processing row 471 of 1050.\n",
      "Processing row 472 of 1050.\n",
      "Processing row 473 of 1050.\n",
      "Processing row 474 of 1050.\n",
      "Processing row 475 of 1050.\n",
      "Processing row 476 of 1050.\n",
      "Processing row 477 of 1050.\n",
      "Processing row 478 of 1050.\n",
      "Processing row 479 of 1050.\n",
      "Processing row 480 of 1050.\n",
      "Processing row 481 of 1050.\n",
      "Processing row 482 of 1050.\n",
      "Processing row 483 of 1050.\n",
      "Processing row 484 of 1050.\n",
      "Processing row 485 of 1050.\n",
      "Processing row 486 of 1050.\n",
      "Processing row 487 of 1050.\n",
      "Processing row 488 of 1050.\n",
      "Processing row 489 of 1050.\n",
      "Processing row 490 of 1050.\n",
      "Processing row 491 of 1050.\n",
      "Processing row 492 of 1050.\n",
      "Processing row 493 of 1050.\n",
      "Processing row 494 of 1050.\n",
      "Processing row 495 of 1050.\n",
      "Processing row 496 of 1050.\n",
      "Processing row 497 of 1050.\n",
      "Processing row 498 of 1050.\n",
      "Processing row 499 of 1050.\n",
      "Processing row 500 of 1050.\n",
      "Processing row 501 of 1050.\n",
      "Processing row 502 of 1050.\n",
      "Processing row 503 of 1050.\n",
      "Processing row 504 of 1050.\n",
      "Processing row 505 of 1050.\n",
      "Processing row 506 of 1050.\n",
      "Processing row 507 of 1050.\n",
      "Processing row 508 of 1050.\n",
      "Processing row 509 of 1050.\n",
      "Processing row 510 of 1050.\n",
      "Processing row 511 of 1050.\n",
      "Processing row 512 of 1050.\n",
      "Processing row 513 of 1050.\n",
      "Processing row 514 of 1050.\n",
      "Processing row 515 of 1050.\n",
      "Processing row 516 of 1050.\n",
      "Processing row 517 of 1050.\n",
      "Processing row 518 of 1050.\n",
      "Processing row 519 of 1050.\n",
      "Processing row 520 of 1050.\n",
      "Processing row 521 of 1050.\n",
      "Processing row 522 of 1050.\n",
      "Processing row 523 of 1050.\n",
      "Processing row 524 of 1050.\n",
      "Processing row 525 of 1050.\n",
      "Processing row 526 of 1050.\n",
      "Processing row 527 of 1050.\n",
      "Processing row 528 of 1050.\n",
      "Processing row 529 of 1050.\n",
      "Processing row 530 of 1050.\n",
      "Processing row 531 of 1050.\n",
      "Processing row 532 of 1050.\n",
      "Processing row 533 of 1050.\n",
      "Processing row 534 of 1050.\n",
      "Processing row 535 of 1050.\n",
      "Processing row 536 of 1050.\n",
      "Processing row 537 of 1050.\n",
      "Processing row 538 of 1050.\n",
      "Processing row 539 of 1050.\n",
      "Processing row 540 of 1050.\n",
      "Processing row 541 of 1050.\n",
      "Processing row 542 of 1050.\n",
      "Processing row 543 of 1050.\n",
      "Processing row 544 of 1050.\n",
      "Processing row 545 of 1050.\n",
      "Processing row 546 of 1050.\n",
      "Processing row 547 of 1050.\n",
      "Processing row 548 of 1050.\n",
      "Processing row 549 of 1050.\n",
      "Processing row 550 of 1050.\n",
      "Processing row 551 of 1050.\n",
      "Processing row 552 of 1050.\n",
      "Processing row 553 of 1050.\n",
      "Processing row 554 of 1050.\n",
      "Processing row 555 of 1050.\n",
      "Processing row 556 of 1050.\n",
      "Processing row 557 of 1050.\n",
      "Processing row 558 of 1050.\n",
      "Processing row 559 of 1050.\n",
      "Processing row 560 of 1050.\n",
      "Processing row 561 of 1050.\n",
      "Processing row 562 of 1050.\n",
      "Processing row 563 of 1050.\n",
      "Processing row 564 of 1050.\n",
      "Processing row 565 of 1050.\n",
      "Processing row 566 of 1050.\n",
      "Processing row 567 of 1050.\n",
      "Processing row 568 of 1050.\n",
      "Processing row 569 of 1050.\n",
      "Processing row 570 of 1050.\n",
      "Processing row 571 of 1050.\n",
      "Processing row 572 of 1050.\n",
      "Processing row 573 of 1050.\n",
      "Processing row 574 of 1050.\n",
      "Processing row 575 of 1050.\n",
      "Processing row 576 of 1050.\n",
      "Processing row 577 of 1050.\n",
      "Processing row 578 of 1050.\n",
      "Processing row 579 of 1050.\n",
      "Processing row 580 of 1050.\n",
      "Processing row 581 of 1050.\n",
      "Processing row 582 of 1050.\n",
      "Processing row 583 of 1050.\n",
      "Processing row 584 of 1050.\n",
      "Processing row 585 of 1050.\n",
      "Processing row 586 of 1050.\n",
      "Processing row 587 of 1050.\n",
      "Processing row 588 of 1050.\n",
      "Processing row 589 of 1050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 590 of 1050.\n",
      "Processing row 591 of 1050.\n",
      "Processing row 592 of 1050.\n",
      "Processing row 593 of 1050.\n",
      "Processing row 594 of 1050.\n",
      "Processing row 595 of 1050.\n",
      "Processing row 596 of 1050.\n",
      "Processing row 597 of 1050.\n",
      "Processing row 598 of 1050.\n",
      "Processing row 599 of 1050.\n",
      "Processing row 600 of 1050.\n",
      "Processing row 601 of 1050.\n",
      "Processing row 602 of 1050.\n",
      "Processing row 603 of 1050.\n",
      "Processing row 604 of 1050.\n",
      "Processing row 605 of 1050.\n",
      "Processing row 606 of 1050.\n",
      "Processing row 607 of 1050.\n",
      "Processing row 608 of 1050.\n",
      "Processing row 609 of 1050.\n",
      "Processing row 610 of 1050.\n",
      "Processing row 611 of 1050.\n",
      "Processing row 612 of 1050.\n",
      "Processing row 613 of 1050.\n",
      "Processing row 614 of 1050.\n",
      "Processing row 615 of 1050.\n",
      "Processing row 616 of 1050.\n",
      "Processing row 617 of 1050.\n",
      "Processing row 618 of 1050.\n",
      "Processing row 619 of 1050.\n",
      "Processing row 620 of 1050.\n",
      "Processing row 621 of 1050.\n",
      "Processing row 622 of 1050.\n",
      "Processing row 623 of 1050.\n",
      "Processing row 624 of 1050.\n",
      "Processing row 625 of 1050.\n",
      "Processing row 626 of 1050.\n",
      "Processing row 627 of 1050.\n",
      "Processing row 628 of 1050.\n",
      "Processing row 629 of 1050.\n",
      "Processing row 630 of 1050.\n",
      "Processing row 631 of 1050.\n",
      "Processing row 632 of 1050.\n",
      "Processing row 633 of 1050.\n",
      "Processing row 634 of 1050.\n",
      "Processing row 635 of 1050.\n",
      "Processing row 636 of 1050.\n",
      "Processing row 637 of 1050.\n",
      "Processing row 638 of 1050.\n",
      "Processing row 639 of 1050.\n",
      "Processing row 640 of 1050.\n",
      "Processing row 641 of 1050.\n",
      "Processing row 642 of 1050.\n",
      "Processing row 643 of 1050.\n",
      "Processing row 644 of 1050.\n",
      "Processing row 645 of 1050.\n",
      "Processing row 646 of 1050.\n",
      "Processing row 647 of 1050.\n",
      "Processing row 648 of 1050.\n",
      "Processing row 649 of 1050.\n",
      "Processing row 650 of 1050.\n",
      "Processing row 651 of 1050.\n",
      "Processing row 652 of 1050.\n",
      "Processing row 653 of 1050.\n",
      "Processing row 654 of 1050.\n",
      "Processing row 655 of 1050.\n",
      "Processing row 656 of 1050.\n",
      "Processing row 657 of 1050.\n",
      "Processing row 658 of 1050.\n",
      "Processing row 659 of 1050.\n",
      "Processing row 660 of 1050.\n",
      "Processing row 661 of 1050.\n",
      "Processing row 662 of 1050.\n",
      "Processing row 663 of 1050.\n",
      "Processing row 664 of 1050.\n",
      "Processing row 665 of 1050.\n",
      "Processing row 666 of 1050.\n",
      "Processing row 667 of 1050.\n",
      "Processing row 668 of 1050.\n",
      "Processing row 669 of 1050.\n",
      "Processing row 670 of 1050.\n",
      "Processing row 671 of 1050.\n",
      "Processing row 672 of 1050.\n",
      "Processing row 673 of 1050.\n",
      "Processing row 674 of 1050.\n",
      "Processing row 675 of 1050.\n",
      "Processing row 676 of 1050.\n",
      "Processing row 677 of 1050.\n",
      "Processing row 678 of 1050.\n",
      "Processing row 679 of 1050.\n",
      "Processing row 680 of 1050.\n",
      "Processing row 681 of 1050.\n",
      "Processing row 682 of 1050.\n",
      "Processing row 683 of 1050.\n",
      "Processing row 684 of 1050.\n",
      "Processing row 685 of 1050.\n",
      "Processing row 686 of 1050.\n",
      "Processing row 687 of 1050.\n",
      "Processing row 688 of 1050.\n",
      "Processing row 689 of 1050.\n",
      "Processing row 690 of 1050.\n",
      "Processing row 691 of 1050.\n",
      "Processing row 692 of 1050.\n",
      "Processing row 693 of 1050.\n",
      "Processing row 694 of 1050.\n",
      "Processing row 695 of 1050.\n",
      "Processing row 696 of 1050.\n",
      "Processing row 697 of 1050.\n",
      "Processing row 698 of 1050.\n",
      "Processing row 699 of 1050.\n",
      "Processing row 700 of 1050.\n",
      "Processing row 701 of 1050.\n",
      "Processing row 702 of 1050.\n",
      "Processing row 703 of 1050.\n",
      "Processing row 704 of 1050.\n",
      "Processing row 705 of 1050.\n",
      "Processing row 706 of 1050.\n",
      "Processing row 707 of 1050.\n",
      "Processing row 708 of 1050.\n",
      "Processing row 709 of 1050.\n",
      "Processing row 710 of 1050.\n",
      "Processing row 711 of 1050.\n",
      "Processing row 712 of 1050.\n",
      "Processing row 713 of 1050.\n",
      "Processing row 714 of 1050.\n",
      "Processing row 715 of 1050.\n",
      "Processing row 716 of 1050.\n",
      "Processing row 717 of 1050.\n",
      "Processing row 718 of 1050.\n",
      "Processing row 719 of 1050.\n",
      "Processing row 720 of 1050.\n",
      "Processing row 721 of 1050.\n",
      "Processing row 722 of 1050.\n",
      "Processing row 723 of 1050.\n",
      "Processing row 724 of 1050.\n",
      "Processing row 725 of 1050.\n",
      "Processing row 726 of 1050.\n",
      "Processing row 727 of 1050.\n",
      "Processing row 728 of 1050.\n",
      "Processing row 729 of 1050.\n",
      "Processing row 730 of 1050.\n",
      "Processing row 731 of 1050.\n",
      "Processing row 732 of 1050.\n",
      "Processing row 733 of 1050.\n",
      "Processing row 734 of 1050.\n",
      "Processing row 735 of 1050.\n",
      "Processing row 736 of 1050.\n",
      "Processing row 737 of 1050.\n",
      "Processing row 738 of 1050.\n",
      "Processing row 739 of 1050.\n",
      "Processing row 740 of 1050.\n",
      "Processing row 741 of 1050.\n",
      "Processing row 742 of 1050.\n",
      "Processing row 743 of 1050.\n",
      "Processing row 744 of 1050.\n",
      "Processing row 745 of 1050.\n",
      "Processing row 746 of 1050.\n",
      "Processing row 747 of 1050.\n",
      "Processing row 748 of 1050.\n",
      "Processing row 749 of 1050.\n",
      "Processing row 750 of 1050.\n",
      "Processing row 751 of 1050.\n",
      "Processing row 752 of 1050.\n",
      "Processing row 753 of 1050.\n",
      "Processing row 754 of 1050.\n",
      "Processing row 755 of 1050.\n",
      "Processing row 756 of 1050.\n",
      "Processing row 757 of 1050.\n",
      "Processing row 758 of 1050.\n",
      "Processing row 759 of 1050.\n",
      "Processing row 760 of 1050.\n",
      "Processing row 761 of 1050.\n",
      "Processing row 762 of 1050.\n",
      "Processing row 763 of 1050.\n",
      "Processing row 764 of 1050.\n",
      "Processing row 765 of 1050.\n",
      "Processing row 766 of 1050.\n",
      "Processing row 767 of 1050.\n",
      "Processing row 768 of 1050.\n",
      "Processing row 769 of 1050.\n",
      "Processing row 770 of 1050.\n",
      "Processing row 771 of 1050.\n",
      "Processing row 772 of 1050.\n",
      "Processing row 773 of 1050.\n",
      "Processing row 774 of 1050.\n",
      "Processing row 775 of 1050.\n",
      "Processing row 776 of 1050.\n",
      "Processing row 777 of 1050.\n",
      "Processing row 778 of 1050.\n",
      "Processing row 779 of 1050.\n",
      "Processing row 780 of 1050.\n",
      "Processing row 781 of 1050.\n",
      "Processing row 782 of 1050.\n",
      "Processing row 783 of 1050.\n",
      "Processing row 784 of 1050.\n",
      "Processing row 785 of 1050.\n",
      "Processing row 786 of 1050.\n",
      "Processing row 787 of 1050.\n",
      "Processing row 788 of 1050.\n",
      "Processing row 789 of 1050.\n",
      "Processing row 790 of 1050.\n",
      "Processing row 791 of 1050.\n",
      "Processing row 792 of 1050.\n",
      "Processing row 793 of 1050.\n",
      "Processing row 794 of 1050.\n",
      "Processing row 795 of 1050.\n",
      "Processing row 796 of 1050.\n",
      "Processing row 797 of 1050.\n",
      "Processing row 798 of 1050.\n",
      "Processing row 799 of 1050.\n",
      "Processing row 800 of 1050.\n",
      "Processing row 801 of 1050.\n",
      "Processing row 802 of 1050.\n",
      "Processing row 803 of 1050.\n",
      "Processing row 804 of 1050.\n",
      "Processing row 805 of 1050.\n",
      "Processing row 806 of 1050.\n",
      "Processing row 807 of 1050.\n",
      "Processing row 808 of 1050.\n",
      "Processing row 809 of 1050.\n",
      "Processing row 810 of 1050.\n",
      "Processing row 811 of 1050.\n",
      "Processing row 812 of 1050.\n",
      "Processing row 813 of 1050.\n",
      "Processing row 814 of 1050.\n",
      "Processing row 815 of 1050.\n",
      "Processing row 816 of 1050.\n",
      "Processing row 817 of 1050.\n",
      "Processing row 818 of 1050.\n",
      "Processing row 819 of 1050.\n",
      "Processing row 820 of 1050.\n",
      "Processing row 821 of 1050.\n",
      "Processing row 822 of 1050.\n",
      "Processing row 823 of 1050.\n",
      "Processing row 824 of 1050.\n",
      "Processing row 825 of 1050.\n",
      "Processing row 826 of 1050.\n",
      "Processing row 827 of 1050.\n",
      "Processing row 828 of 1050.\n",
      "Processing row 829 of 1050.\n",
      "Processing row 830 of 1050.\n",
      "Processing row 831 of 1050.\n",
      "Processing row 832 of 1050.\n",
      "Processing row 833 of 1050.\n",
      "Processing row 834 of 1050.\n",
      "Processing row 835 of 1050.\n",
      "Processing row 836 of 1050.\n",
      "Processing row 837 of 1050.\n",
      "Processing row 838 of 1050.\n",
      "Processing row 839 of 1050.\n",
      "Processing row 840 of 1050.\n",
      "Processing row 841 of 1050.\n",
      "Processing row 842 of 1050.\n",
      "Processing row 843 of 1050.\n",
      "Processing row 844 of 1050.\n",
      "Processing row 845 of 1050.\n",
      "Processing row 846 of 1050.\n",
      "Processing row 847 of 1050.\n",
      "Processing row 848 of 1050.\n",
      "Processing row 849 of 1050.\n",
      "Processing row 850 of 1050.\n",
      "Processing row 851 of 1050.\n",
      "Processing row 852 of 1050.\n",
      "Processing row 853 of 1050.\n",
      "Processing row 854 of 1050.\n",
      "Processing row 855 of 1050.\n",
      "Processing row 856 of 1050.\n",
      "Processing row 857 of 1050.\n",
      "Processing row 858 of 1050.\n",
      "Processing row 859 of 1050.\n",
      "Processing row 860 of 1050.\n",
      "Processing row 861 of 1050.\n",
      "Processing row 862 of 1050.\n",
      "Processing row 863 of 1050.\n",
      "Processing row 864 of 1050.\n",
      "Processing row 865 of 1050.\n",
      "Processing row 866 of 1050.\n",
      "Processing row 867 of 1050.\n",
      "Processing row 868 of 1050.\n",
      "Processing row 869 of 1050.\n",
      "Processing row 870 of 1050.\n",
      "Processing row 871 of 1050.\n",
      "Processing row 872 of 1050.\n",
      "Processing row 873 of 1050.\n",
      "Processing row 874 of 1050.\n",
      "Processing row 875 of 1050.\n",
      "Processing row 876 of 1050.\n",
      "Processing row 877 of 1050.\n",
      "Processing row 878 of 1050.\n",
      "Processing row 879 of 1050.\n",
      "Processing row 880 of 1050.\n",
      "Processing row 881 of 1050.\n",
      "Processing row 882 of 1050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 883 of 1050.\n",
      "Processing row 884 of 1050.\n",
      "Processing row 885 of 1050.\n",
      "Processing row 886 of 1050.\n",
      "Processing row 887 of 1050.\n",
      "Processing row 888 of 1050.\n",
      "Processing row 889 of 1050.\n",
      "Processing row 890 of 1050.\n",
      "Processing row 891 of 1050.\n",
      "Processing row 892 of 1050.\n",
      "Processing row 893 of 1050.\n",
      "Processing row 894 of 1050.\n",
      "Processing row 895 of 1050.\n",
      "Processing row 896 of 1050.\n",
      "Processing row 897 of 1050.\n",
      "Processing row 898 of 1050.\n",
      "Processing row 899 of 1050.\n",
      "Processing row 900 of 1050.\n",
      "Processing row 901 of 1050.\n",
      "Processing row 902 of 1050.\n",
      "Processing row 903 of 1050.\n",
      "Processing row 904 of 1050.\n",
      "Processing row 905 of 1050.\n",
      "Processing row 906 of 1050.\n",
      "Processing row 907 of 1050.\n",
      "Processing row 908 of 1050.\n",
      "Processing row 909 of 1050.\n",
      "Processing row 910 of 1050.\n",
      "Processing row 911 of 1050.\n",
      "Processing row 912 of 1050.\n",
      "Processing row 913 of 1050.\n",
      "Processing row 914 of 1050.\n",
      "Processing row 915 of 1050.\n",
      "Processing row 916 of 1050.\n",
      "Processing row 917 of 1050.\n",
      "Processing row 918 of 1050.\n",
      "Processing row 919 of 1050.\n",
      "Processing row 920 of 1050.\n",
      "Processing row 921 of 1050.\n",
      "Processing row 922 of 1050.\n",
      "Processing row 923 of 1050.\n",
      "Processing row 924 of 1050.\n",
      "Processing row 925 of 1050.\n",
      "Processing row 926 of 1050.\n",
      "Processing row 927 of 1050.\n",
      "Processing row 928 of 1050.\n",
      "Processing row 929 of 1050.\n",
      "Processing row 930 of 1050.\n",
      "Processing row 931 of 1050.\n",
      "Processing row 932 of 1050.\n",
      "Processing row 933 of 1050.\n",
      "Processing row 934 of 1050.\n",
      "Processing row 935 of 1050.\n",
      "Processing row 936 of 1050.\n",
      "Processing row 937 of 1050.\n",
      "Processing row 938 of 1050.\n",
      "Processing row 939 of 1050.\n",
      "Processing row 940 of 1050.\n",
      "Processing row 941 of 1050.\n",
      "Processing row 942 of 1050.\n",
      "Processing row 943 of 1050.\n",
      "Processing row 944 of 1050.\n",
      "Processing row 945 of 1050.\n",
      "Processing row 946 of 1050.\n",
      "Processing row 947 of 1050.\n",
      "Processing row 948 of 1050.\n",
      "Processing row 949 of 1050.\n",
      "Processing row 950 of 1050.\n",
      "Processing row 951 of 1050.\n",
      "Processing row 952 of 1050.\n",
      "Processing row 953 of 1050.\n",
      "Processing row 954 of 1050.\n",
      "Processing row 955 of 1050.\n",
      "Processing row 956 of 1050.\n",
      "Processing row 957 of 1050.\n",
      "Processing row 958 of 1050.\n",
      "Processing row 959 of 1050.\n",
      "Processing row 960 of 1050.\n",
      "Processing row 961 of 1050.\n",
      "Processing row 962 of 1050.\n",
      "Processing row 963 of 1050.\n",
      "Processing row 964 of 1050.\n",
      "Processing row 965 of 1050.\n",
      "Processing row 966 of 1050.\n",
      "Processing row 967 of 1050.\n",
      "Processing row 968 of 1050.\n",
      "Processing row 969 of 1050.\n",
      "Processing row 970 of 1050.\n",
      "Processing row 971 of 1050.\n",
      "Processing row 972 of 1050.\n",
      "Processing row 973 of 1050.\n",
      "Processing row 974 of 1050.\n",
      "Processing row 975 of 1050.\n",
      "Processing row 976 of 1050.\n",
      "Processing row 977 of 1050.\n",
      "Processing row 978 of 1050.\n",
      "Processing row 979 of 1050.\n",
      "Processing row 980 of 1050.\n",
      "Processing row 981 of 1050.\n",
      "Processing row 982 of 1050.\n",
      "Processing row 983 of 1050.\n",
      "Processing row 984 of 1050.\n",
      "Processing row 985 of 1050.\n",
      "Processing row 986 of 1050.\n",
      "Processing row 987 of 1050.\n",
      "Processing row 988 of 1050.\n",
      "Processing row 989 of 1050.\n",
      "Processing row 990 of 1050.\n",
      "Processing row 991 of 1050.\n",
      "Processing row 992 of 1050.\n",
      "Processing row 993 of 1050.\n",
      "Processing row 994 of 1050.\n",
      "Processing row 995 of 1050.\n",
      "Processing row 996 of 1050.\n",
      "Processing row 997 of 1050.\n",
      "Processing row 998 of 1050.\n",
      "Processing row 999 of 1050.\n",
      "Processing row 1000 of 1050.\n",
      "Processing row 1001 of 1050.\n",
      "Processing row 1002 of 1050.\n",
      "Processing row 1003 of 1050.\n",
      "Processing row 1004 of 1050.\n",
      "Processing row 1005 of 1050.\n",
      "Processing row 1006 of 1050.\n",
      "Processing row 1007 of 1050.\n",
      "Processing row 1008 of 1050.\n",
      "Processing row 1009 of 1050.\n",
      "Processing row 1010 of 1050.\n",
      "Processing row 1011 of 1050.\n",
      "Processing row 1012 of 1050.\n",
      "Processing row 1013 of 1050.\n",
      "Processing row 1014 of 1050.\n",
      "Processing row 1015 of 1050.\n",
      "Processing row 1016 of 1050.\n",
      "Processing row 1017 of 1050.\n",
      "Processing row 1018 of 1050.\n",
      "Processing row 1019 of 1050.\n",
      "Processing row 1020 of 1050.\n",
      "Processing row 1021 of 1050.\n",
      "Processing row 1022 of 1050.\n",
      "Processing row 1023 of 1050.\n",
      "Processing row 1024 of 1050.\n",
      "Processing row 1025 of 1050.\n",
      "Processing row 1026 of 1050.\n",
      "Processing row 1027 of 1050.\n",
      "Processing row 1028 of 1050.\n",
      "Processing row 1029 of 1050.\n",
      "Processing row 1030 of 1050.\n",
      "Processing row 1031 of 1050.\n",
      "Processing row 1032 of 1050.\n",
      "Processing row 1033 of 1050.\n",
      "Processing row 1034 of 1050.\n",
      "Processing row 1035 of 1050.\n",
      "Processing row 1036 of 1050.\n",
      "Processing row 1037 of 1050.\n",
      "Processing row 1038 of 1050.\n",
      "Processing row 1039 of 1050.\n",
      "Processing row 1040 of 1050.\n",
      "Processing row 1041 of 1050.\n",
      "Processing row 1042 of 1050.\n",
      "Processing row 1043 of 1050.\n",
      "Processing row 1044 of 1050.\n",
      "Processing row 1045 of 1050.\n",
      "Processing row 1046 of 1050.\n",
      "Processing row 1047 of 1050.\n",
      "Processing row 1048 of 1050.\n",
      "Processing row 1049 of 1050.\n",
      "Processing row 1050 of 1050.\n",
      "30872.925096035004\n"
     ]
    }
   ],
   "source": [
    "# Iterations. Update the similar score values to article_article\n",
    "\n",
    "# ATTENTION: THE ITERATION TAKES time, better to track progress\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Number of articles\n",
    "len_articles = article_article.shape[0]\n",
    "\n",
    "# List of index of articles\n",
    "idx_articles = list(article_article.index)\n",
    "\n",
    "\n",
    "# Loop thru each article: for each row, loop each columns. \n",
    "for i in idx_articles:\n",
    "    print(f\"Processing row {i} of {len_articles-1}.\")\n",
    "    \n",
    "    for j in idx_articles:\n",
    "        article_article.loc[i, j] = compute_article_similarity(i, j)\n",
    "        \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1041</th>\n",
       "      <th>1042</th>\n",
       "      <th>1043</th>\n",
       "      <th>1044</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032471</td>\n",
       "      <td>0.072460</td>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.259333</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>0.046419</td>\n",
       "      <td>0.047824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063201</td>\n",
       "      <td>0.051671</td>\n",
       "      <td>0.143709</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.032471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186967</td>\n",
       "      <td>0.028671</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>0.098205</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>0.155317</td>\n",
       "      <td>0.206362</td>\n",
       "      <td>0.103742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104562</td>\n",
       "      <td>0.152361</td>\n",
       "      <td>0.093032</td>\n",
       "      <td>0.030101</td>\n",
       "      <td>0.082490</td>\n",
       "      <td>0.014128</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>0.090637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072460</td>\n",
       "      <td>0.186967</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053018</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>0.041646</td>\n",
       "      <td>0.204603</td>\n",
       "      <td>0.339824</td>\n",
       "      <td>0.153497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149979</td>\n",
       "      <td>0.208527</td>\n",
       "      <td>0.139453</td>\n",
       "      <td>0.036274</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.104403</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.160396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.028671</td>\n",
       "      <td>0.053018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>0.037732</td>\n",
       "      <td>0.084109</td>\n",
       "      <td>0.048761</td>\n",
       "      <td>0.031276</td>\n",
       "      <td>0.049991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.033499</td>\n",
       "      <td>0.010938</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.036531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.259333</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095990</td>\n",
       "      <td>0.026990</td>\n",
       "      <td>0.154261</td>\n",
       "      <td>0.135846</td>\n",
       "      <td>0.152328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122135</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.137278</td>\n",
       "      <td>0.024670</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.072108</td>\n",
       "      <td>0.138237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.014128</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>0.009555</td>\n",
       "      <td>0.018015</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.010529</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.006396</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.017164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.104403</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.026182</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>0.069507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074214</td>\n",
       "      <td>0.024484</td>\n",
       "      <td>0.034961</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>0.015387</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.072108</td>\n",
       "      <td>0.010935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012977</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>0.066733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>0.069338</td>\n",
       "      <td>0.068079</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.078722</td>\n",
       "      <td>0.090637</td>\n",
       "      <td>0.160396</td>\n",
       "      <td>0.036531</td>\n",
       "      <td>0.138237</td>\n",
       "      <td>0.030091</td>\n",
       "      <td>0.038946</td>\n",
       "      <td>0.130746</td>\n",
       "      <td>0.084648</td>\n",
       "      <td>0.095793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188039</td>\n",
       "      <td>0.113751</td>\n",
       "      <td>0.140985</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>0.023651</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.114996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows × 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id      0         1         2         3         4         5     \\\n",
       "article_id                                                               \n",
       "0           1.000000  0.032471  0.072460  0.020439  0.259333  0.033410   \n",
       "1           0.032471  1.000000  0.186967  0.028671  0.093232  0.098205   \n",
       "2           0.072460  0.186967  1.000000  0.053018  0.172458  0.106455   \n",
       "3           0.020439  0.028671  0.053018  1.000000  0.027609  0.037732   \n",
       "4           0.259333  0.093232  0.172458  0.027609  1.000000  0.095990   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "1046        0.003492  0.014128  0.009416  0.012851  0.006955  0.019682   \n",
       "1047        0.004095  0.019350  0.013930  0.011922  0.003037  0.010529   \n",
       "1048        0.028436  0.056505  0.104403  0.010652  0.038724  0.026182   \n",
       "1049        0.000000  0.010927  0.018914  0.009425  0.072108  0.010935   \n",
       "1050        0.078722  0.090637  0.160396  0.036531  0.138237  0.030091   \n",
       "\n",
       "article_id      6         7         8         9     ...      1041      1042  \\\n",
       "article_id                                          ...                       \n",
       "0           0.016156  0.051355  0.046419  0.047824  ...  0.063201  0.051671   \n",
       "1           0.035012  0.155317  0.206362  0.103742  ...  0.104562  0.152361   \n",
       "2           0.041646  0.204603  0.339824  0.153497  ...  0.149979  0.208527   \n",
       "3           0.084109  0.048761  0.031276  0.049991  ...  0.027299  0.026927   \n",
       "4           0.026990  0.154261  0.135846  0.152328  ...  0.122135  0.141956   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "1046        0.009555  0.018015  0.004764  0.006668  ...  0.013567  0.003230   \n",
       "1047        0.002925  0.006396  0.007393  0.017164  ...  0.007295  0.002512   \n",
       "1048        0.017823  0.050228  0.041883  0.069507  ...  0.074214  0.024484   \n",
       "1049        0.000000  0.012977  0.014043  0.066733  ...  0.005548  0.069338   \n",
       "1050        0.038946  0.130746  0.084648  0.095793  ...  0.188039  0.113751   \n",
       "\n",
       "article_id      1043      1044      1045      1046      1047      1048  \\\n",
       "article_id                                                               \n",
       "0           0.143709  0.020163  0.010646  0.003492  0.004095  0.028436   \n",
       "1           0.093032  0.030101  0.082490  0.014128  0.019350  0.056505   \n",
       "2           0.139453  0.036274  0.009098  0.009416  0.013930  0.104403   \n",
       "3           0.033499  0.010938  0.036107  0.012851  0.011922  0.010652   \n",
       "4           0.137278  0.024670  0.007071  0.006955  0.003037  0.038724   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "1046        0.003622  0.014652  0.005776  1.000000  0.000000  0.001758   \n",
       "1047        0.005704  0.001240  0.006689  0.000000  1.000000  0.058308   \n",
       "1048        0.034961  0.020447  0.015387  0.001758  0.058308  1.000000   \n",
       "1049        0.068079  0.005126  0.000000  0.000000  0.000000  0.000000   \n",
       "1050        0.140985  0.025935  0.023651  0.005407  0.006413  0.114996   \n",
       "\n",
       "article_id      1049      1050  \n",
       "article_id                      \n",
       "0           0.000000  0.078722  \n",
       "1           0.010927  0.090637  \n",
       "2           0.018914  0.160396  \n",
       "3           0.009425  0.036531  \n",
       "4           0.072108  0.138237  \n",
       "...              ...       ...  \n",
       "1046        0.000000  0.005407  \n",
       "1047        0.000000  0.006413  \n",
       "1048        0.000000  0.114996  \n",
       "1049        1.000000  0.000000  \n",
       "1050        0.000000  1.000000  \n",
       "\n",
       "[1051 rows x 1051 columns]"
      ]
     },
     "execution_count": 1694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check updated article_article with calculated results. \n",
    "\n",
    "article_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to pickle\n",
    "\n",
    "article_article.to_pickle('article_article_similarity_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1777,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of finding in real time. Use a query method from pre-calculated \n",
    "# article_article df. Save time.\n",
    "\n",
    "def loopup_similar_articles(article_id, data=article_article, n=20):\n",
    "    \n",
    "    # input: n - number of top similar to return\n",
    "    \n",
    "    ids = list(data.loc[article_id][data.loc[article_id].index != article_id].sort_values(ascending=False).head(n).index)\n",
    "    names = list(df_nlp.loc[ids].doc_full_name.values)\n",
    "    \n",
    "    \n",
    "    return ids, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([800,\n",
       "  1035,\n",
       "  313,\n",
       "  805,\n",
       "  444,\n",
       "  721,\n",
       "  260,\n",
       "  967,\n",
       "  122,\n",
       "  96,\n",
       "  384,\n",
       "  809,\n",
       "  124,\n",
       "  723,\n",
       "  234,\n",
       "  892,\n",
       "  54,\n",
       "  253,\n",
       "  812,\n",
       "  861,\n",
       "  412,\n",
       "  732,\n",
       "  871,\n",
       "  74,\n",
       "  221,\n",
       "  89,\n",
       "  479,\n",
       "  616,\n",
       "  500,\n",
       "  567],\n",
       " ['Machine Learning for the Enterprise',\n",
       "  'Machine Learning for the Enterprise.',\n",
       "  'What is machine learning?',\n",
       "  'Machine Learning for everyone',\n",
       "  'Declarative Machine Learning',\n",
       "  'The power of machine learning in Spark',\n",
       "  'The Machine Learning Database',\n",
       "  'ML Algorithm != Learning Machine',\n",
       "  'Watson Machine Learning for Developers',\n",
       "  'Improving quality of life with Spark-empowered machine learning',\n",
       "  'Continuous Learning on Watson',\n",
       "  'Use the Machine Learning Library',\n",
       "  'Python Machine Learning: Scikit-Learn Tutorial',\n",
       "  '10 Essential Algorithms For Machine Learning Engineers',\n",
       "  '3 Scenarios for Machine Learning on Multicloud',\n",
       "  'Breaking the 80/20 rule: How data catalogs transform data scientists’ productivity',\n",
       "  '8 ways to turn data into value with Apache Spark machine learning',\n",
       "  'Lifelong (machine) learning: how automation can help your models get smarter over time',\n",
       "  'Machine Learning Exercises In Python, Part 1',\n",
       "  'Cleaning the swamp: Turn your data lake into a source of crystal-clear insight',\n",
       "  'Adoption of machine learning to software failure prediction',\n",
       "  'Rapidly build Machine Learning flows with DSX',\n",
       "  'Overfitting in Machine Learning: What It Is and How to Prevent It',\n",
       "  'The 3 Kinds of Context: Machine Learning and the Art of the Frame',\n",
       "  'How smart catalogs can turn the big data flood into an ocean of opportunity',\n",
       "  'Top 20 R Machine Learning and Data Science packages',\n",
       "  'Drowning in data sources: How data cataloging could fix your findability problems',\n",
       "  'Three reasons machine learning models go out of sync',\n",
       "  'The Difference Between AI, Machine Learning, and Deep Learning?',\n",
       "  'You could be looking at it all wrong'])"
      ]
     },
     "execution_count": 1785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test result: look up most relavant articles for 455th article\n",
    "\n",
    "relavant_for_455th_article = loopup_similar_articles(455, n=30)\n",
    "relavant_for_455th_article\n",
    "\n",
    "# Revealing that they are about on 'machine learning'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on datasets inconsistency: \n",
    "\n",
    "Since that orginal df_content / df_nlp (cleaned version) only contain article from \\[0 - 1050\\].\n",
    "\n",
    "However, df (the interaction) dataset showing more article ids that beyone 1050. e,g 1429.\n",
    "\n",
    "So these two dataset's article are not up to dated.  \n",
    "The given df_content is 'late' (not catch up), which means: **articles that are in df (interaction) dataset might \n",
    "not be found in df_content dataset**. \n",
    "\n",
    "Therefore, due to this inconsistency, we will not be able to find content details for some articles mentioned in df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ideas to explore: \n",
    "- Based on the relavant article for a given article, can you get common words on them. Extract topic perhaps\n",
    "- What are the top contents? (that also in df_content/df_nlp)\n",
    "- What are the common words / terms of those top content? word bag counter\n",
    "- Can these words/terms represent trending topic?\n",
    "- Make a list of most trending topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics from relavant content.\n",
    "# Try extract common terms represented in top relavant articles. \n",
    "# e.g the relavant results showing they all contain 'machine learning'. \n",
    "# Approach with intersection of tokens for those relavant documents.\n",
    "# bag of words, 1-gram, 2-gram, 3-gram? counter? then intersections \n",
    "\n",
    "def extract_topics_from_relavant_articles(ids, data=df_nlp):\n",
    "    \n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " 'enterprise',\n",
       " 'last',\n",
       " 'blog',\n",
       " 'business',\n",
       " 'differentiation',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'introduced',\n",
       " 'described',\n",
       " 'concept',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'traced',\n",
       " 'origin',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'project',\n",
       " 'watson',\n",
       " 'show',\n",
       " 'skip',\n",
       " 'contentdinesh',\n",
       " 'nirmal',\n",
       " 'blog',\n",
       " 'blog',\n",
       " 'big',\n",
       " 'data',\n",
       " 'analytics',\n",
       " 'digital',\n",
       " 'technology',\n",
       " 'impacting',\n",
       " 'business',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'economy',\n",
       " 'menu',\n",
       " 'welcome',\n",
       " 'twitter',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'enterprise',\n",
       " 'last',\n",
       " 'blog',\n",
       " 'business',\n",
       " 'differentiation',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'introduced',\n",
       " 'described',\n",
       " 'concept',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'traced',\n",
       " 'origin',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'project',\n",
       " 'watson',\n",
       " 'showcasing',\n",
       " 'winning',\n",
       " 'jeopardy',\n",
       " 'tv',\n",
       " 'quiz',\n",
       " 'show',\n",
       " 'real',\n",
       " 'world',\n",
       " 'use',\n",
       " 'across',\n",
       " 'numerous',\n",
       " 'industry',\n",
       " 'including',\n",
       " 'health',\n",
       " 'care',\n",
       " 'concluded',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'potential',\n",
       " 'help',\n",
       " 'make',\n",
       " 'world',\n",
       " 'better',\n",
       " 'safer',\n",
       " 'place',\n",
       " 'however',\n",
       " 'yep',\n",
       " 'always',\n",
       " 'however',\n",
       " 'order',\n",
       " 'become',\n",
       " 'reality',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'form',\n",
       " 'enterprise',\n",
       " 'ready',\n",
       " 'enterprise',\n",
       " 'requirement',\n",
       " 'use',\n",
       " 'word',\n",
       " 'enterprise',\n",
       " 'mean',\n",
       " 'organization',\n",
       " 'business',\n",
       " 'critical',\n",
       " 'requirement',\n",
       " 'organization',\n",
       " 'size',\n",
       " 'volume',\n",
       " 'transaction',\n",
       " 'data',\n",
       " 'velocity',\n",
       " 'interaction',\n",
       " 'variety',\n",
       " 'data',\n",
       " 'yes',\n",
       " 'v',\n",
       " 'big',\n",
       " 'data',\n",
       " 'key',\n",
       " 'factor',\n",
       " 'might',\n",
       " 'impact',\n",
       " 'organization',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'requirement',\n",
       " 'also',\n",
       " 'collaboration',\n",
       " 'across',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'engineer',\n",
       " 'developer',\n",
       " 'creating',\n",
       " 'testing',\n",
       " 'training',\n",
       " 'deploying',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'level',\n",
       " 'different',\n",
       " 'audience',\n",
       " 'want',\n",
       " 'exposed',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'let',\n",
       " 'look',\n",
       " 'factor',\n",
       " 'make',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ibm',\n",
       " 'truly',\n",
       " 'enterprise',\n",
       " 'ready',\n",
       " 'collaboration',\n",
       " 'large',\n",
       " 'enterprise',\n",
       " 'tend',\n",
       " 'significant',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'team',\n",
       " 'often',\n",
       " 'multiple',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'engaged',\n",
       " 'single',\n",
       " 'project',\n",
       " 'collaboration',\n",
       " 'across',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'maybe',\n",
       " 'persona',\n",
       " 'required',\n",
       " 'maximize',\n",
       " 'productivity',\n",
       " 'agility',\n",
       " 'effectiveness',\n",
       " 'today',\n",
       " 'part',\n",
       " 'ibm',\n",
       " 'data',\n",
       " 'science',\n",
       " 'experience',\n",
       " 'bring',\n",
       " 'concept',\n",
       " 'project',\n",
       " 'wherein',\n",
       " 'various',\n",
       " 'persona',\n",
       " 'user',\n",
       " 'safely',\n",
       " 'collaborate',\n",
       " 'project',\n",
       " 'build',\n",
       " 'test',\n",
       " 'use',\n",
       " 'deploy',\n",
       " 'many',\n",
       " 'artefact',\n",
       " 'group',\n",
       " 'people',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'technology',\n",
       " 'adopt',\n",
       " 'concept',\n",
       " 'able',\n",
       " 'share',\n",
       " 'analytic',\n",
       " 'artefact',\n",
       " 'notebook',\n",
       " 'pipeline',\n",
       " 'model',\n",
       " 'etc',\n",
       " 'example',\n",
       " 'group',\n",
       " 'people',\n",
       " 'collaborate',\n",
       " 'single',\n",
       " 'notebook',\n",
       " 'wherein',\n",
       " 'one',\n",
       " 'person',\n",
       " 'curation',\n",
       " 'transformation',\n",
       " 'hand',\n",
       " 'another',\n",
       " 'person',\n",
       " 'creating',\n",
       " 'algorithm',\n",
       " 'testing',\n",
       " 'training',\n",
       " 'model',\n",
       " 'team',\n",
       " 'member',\n",
       " 'evaluate',\n",
       " 'model',\n",
       " 'deploy',\n",
       " 'individual',\n",
       " 'user',\n",
       " 'authenticated',\n",
       " 'separately',\n",
       " 'authorized',\n",
       " 'part',\n",
       " 'role',\n",
       " 'defined',\n",
       " 'project',\n",
       " 'limiting',\n",
       " 'granting',\n",
       " 'access',\n",
       " 'part',\n",
       " 'overall',\n",
       " 'process',\n",
       " 'experience',\n",
       " 'accordingly',\n",
       " 'consumption',\n",
       " 'everyone',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'want',\n",
       " 'need',\n",
       " 'know',\n",
       " 'model',\n",
       " 'design',\n",
       " 'statistical',\n",
       " 'theory',\n",
       " 'training',\n",
       " 'model',\n",
       " 'developer',\n",
       " 'example',\n",
       " 'may',\n",
       " 'varying',\n",
       " 'level',\n",
       " 'need',\n",
       " 'may',\n",
       " 'want',\n",
       " 'able',\n",
       " 'use',\n",
       " 'known',\n",
       " 'model',\n",
       " 'work',\n",
       " 'well',\n",
       " 'deploy',\n",
       " 'app',\n",
       " 'figure',\n",
       " '1',\n",
       " 'show',\n",
       " 'ibm',\n",
       " 'designed',\n",
       " 'work',\n",
       " 'space',\n",
       " 'allows',\n",
       " 'application',\n",
       " 'developer',\n",
       " 'choose',\n",
       " 'deploy',\n",
       " 'model',\n",
       " 'actually',\n",
       " 'create',\n",
       " 'pipeline',\n",
       " 'via',\n",
       " 'step',\n",
       " 'step',\n",
       " 'process',\n",
       " 'take',\n",
       " 'one',\n",
       " 'step',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'developer',\n",
       " 'may',\n",
       " 'want',\n",
       " 'choose',\n",
       " 'collection',\n",
       " 'pre',\n",
       " 'packaged',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'fraud',\n",
       " 'detection',\n",
       " 'weather',\n",
       " 'prediction',\n",
       " 'manufacturing',\n",
       " 'model',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'emotional',\n",
       " 'analysis',\n",
       " 'ibm',\n",
       " 'provides',\n",
       " 'today',\n",
       " 'bluemix',\n",
       " 'service',\n",
       " 'integrated',\n",
       " 'part',\n",
       " 'data',\n",
       " 'science',\n",
       " 'experience',\n",
       " 'figure',\n",
       " '1',\n",
       " 'integrated',\n",
       " 'workspace',\n",
       " 'creating',\n",
       " 'pipeline',\n",
       " 'commoditizing',\n",
       " 'automating',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'enterprise',\n",
       " 'environment',\n",
       " 'challenging',\n",
       " 'start',\n",
       " 'assumption',\n",
       " 'model',\n",
       " 'becomes',\n",
       " 'stale',\n",
       " 'minute',\n",
       " 'stop',\n",
       " 'training',\n",
       " 'time',\n",
       " 'accuracy',\n",
       " 'model',\n",
       " 'worsen',\n",
       " 'take',\n",
       " 'significant',\n",
       " 'time',\n",
       " 'understand',\n",
       " 'happening',\n",
       " 'retrain',\n",
       " 'existing',\n",
       " 'model',\n",
       " 'deploy',\n",
       " 'new',\n",
       " 'version',\n",
       " 'come',\n",
       " 'revenue',\n",
       " 'enterprise',\n",
       " 'may',\n",
       " 'hard',\n",
       " 'time',\n",
       " 'adopting',\n",
       " 'necessary',\n",
       " 'discipline',\n",
       " 'gauge',\n",
       " 'impact',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'lot',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'use',\n",
       " 'case',\n",
       " 'might',\n",
       " 'intuitive',\n",
       " 'sense',\n",
       " 'set',\n",
       " 'clear',\n",
       " 'control',\n",
       " 'point',\n",
       " 'flow',\n",
       " 'people',\n",
       " 'logically',\n",
       " 'relate',\n",
       " 'term',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'may',\n",
       " 'send',\n",
       " 'le',\n",
       " 'scientific',\n",
       " 'people',\n",
       " 'community',\n",
       " 'running',\n",
       " 'opposite',\n",
       " 'direction',\n",
       " 'often',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'perform',\n",
       " 'number',\n",
       " 'tedious',\n",
       " 'time',\n",
       " 'consuming',\n",
       " 'step',\n",
       " 'derive',\n",
       " 'insight',\n",
       " 'raw',\n",
       " 'data',\n",
       " 'set',\n",
       " 'process',\n",
       " 'involve',\n",
       " 'data',\n",
       " 'ingestion',\n",
       " 'cleaning',\n",
       " 'transformation',\n",
       " 'e',\n",
       " 'g',\n",
       " 'outlier',\n",
       " 'removal',\n",
       " 'missing',\n",
       " 'value',\n",
       " 'imputation',\n",
       " 'proceed',\n",
       " 'model',\n",
       " 'building',\n",
       " 'finally',\n",
       " 'presentation',\n",
       " 'prediction',\n",
       " 'align',\n",
       " 'end',\n",
       " 'user',\n",
       " 'objective',\n",
       " 'preference',\n",
       " 'long',\n",
       " 'complex',\n",
       " 'sometimes',\n",
       " 'artful',\n",
       " 'process',\n",
       " 'requiring',\n",
       " 'substantial',\n",
       " 'time',\n",
       " 'effort',\n",
       " 'especially',\n",
       " 'combinatorial',\n",
       " 'explosion',\n",
       " 'choice',\n",
       " 'algorithm',\n",
       " 'platform',\n",
       " 'parameter',\n",
       " 'composition',\n",
       " 'tool',\n",
       " 'help',\n",
       " 'automate',\n",
       " 'step',\n",
       " 'process',\n",
       " 'potential',\n",
       " 'accelerate',\n",
       " 'time',\n",
       " 'delivery',\n",
       " 'useful',\n",
       " 'result',\n",
       " 'expand',\n",
       " 'reach',\n",
       " 'data',\n",
       " 'science',\n",
       " 'non',\n",
       " 'expert',\n",
       " 'offer',\n",
       " 'systematic',\n",
       " 'exploration',\n",
       " 'available',\n",
       " 'option',\n",
       " 'cognitive',\n",
       " 'automation',\n",
       " 'data',\n",
       " 'science',\n",
       " 'cad',\n",
       " 'help',\n",
       " 'integrate',\n",
       " 'learning',\n",
       " 'planning',\n",
       " 'composition',\n",
       " 'orchestration',\n",
       " 'technique',\n",
       " 'automatically',\n",
       " 'efficiently',\n",
       " 'build',\n",
       " 'model',\n",
       " 'done',\n",
       " 'deploying',\n",
       " 'analytic',\n",
       " 'flow',\n",
       " 'interactively',\n",
       " 'support',\n",
       " 'would',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'task',\n",
       " 'cad',\n",
       " 'also',\n",
       " 'provides',\n",
       " 'capability',\n",
       " 'run',\n",
       " 'multiple',\n",
       " 'predefined',\n",
       " 'algorithm',\n",
       " 'parallel',\n",
       " 'identify',\n",
       " 'best',\n",
       " 'suitable',\n",
       " 'algorithm',\n",
       " 'particular',\n",
       " 'use',\n",
       " 'case',\n",
       " 'short',\n",
       " 'cad',\n",
       " 'selects',\n",
       " 'best',\n",
       " 'algorithm',\n",
       " 'given',\n",
       " 'use',\n",
       " 'case',\n",
       " 'click',\n",
       " 'read',\n",
       " 'ibm',\n",
       " 'paper',\n",
       " 'cad',\n",
       " 'training',\n",
       " 'tuning',\n",
       " 'model',\n",
       " 'optimization',\n",
       " 'time',\n",
       " 'model',\n",
       " 'algorithm',\n",
       " 'used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'becomes',\n",
       " 'good',\n",
       " 'true',\n",
       " 'training',\n",
       " 'data',\n",
       " 'model',\n",
       " 'predicts',\n",
       " 'training',\n",
       " 'data',\n",
       " 'well',\n",
       " 'performs',\n",
       " 'poorly',\n",
       " 'new',\n",
       " 'data',\n",
       " 'known',\n",
       " 'overfitting',\n",
       " 'extreme',\n",
       " 'case',\n",
       " 'occur',\n",
       " 'rote',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'achieves',\n",
       " '100',\n",
       " 'performance',\n",
       " 'data',\n",
       " 'already',\n",
       " 'seen',\n",
       " 'probably',\n",
       " 'better',\n",
       " 'random',\n",
       " 'guess',\n",
       " 'new',\n",
       " 'data',\n",
       " 'imagine',\n",
       " 'would',\n",
       " 'business',\n",
       " 'could',\n",
       " 'ruin',\n",
       " 'upset',\n",
       " 'customer',\n",
       " 'set',\n",
       " 'wrong',\n",
       " 'price',\n",
       " 'point',\n",
       " 'miss',\n",
       " 'many',\n",
       " 'business',\n",
       " 'opportunity',\n",
       " 'ibm',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'help',\n",
       " 'counter',\n",
       " 'clean',\n",
       " 'separation',\n",
       " 'training',\n",
       " 'data',\n",
       " 'holdout',\n",
       " 'data',\n",
       " 'used',\n",
       " 'evaluate',\n",
       " 'model',\n",
       " 'performance',\n",
       " 'well',\n",
       " 'careful',\n",
       " 'use',\n",
       " 'cross',\n",
       " 'validation',\n",
       " 'technique',\n",
       " 'data',\n",
       " 'sovereignty',\n",
       " 'isolation',\n",
       " 'enterprise',\n",
       " 'organization',\n",
       " 'fear',\n",
       " 'psychological',\n",
       " 'otherwise',\n",
       " 'come',\n",
       " 'putting',\n",
       " 'data',\n",
       " 'application',\n",
       " 'hardware',\n",
       " 'storage',\n",
       " 'network',\n",
       " 'infrastructure',\n",
       " 'shared',\n",
       " 'organization',\n",
       " 'ibm',\n",
       " 'cloud',\n",
       " 'first',\n",
       " 'strategy',\n",
       " 'provides',\n",
       " 'necessary',\n",
       " 'sovereignty',\n",
       " 'multi',\n",
       " 'tenancy',\n",
       " 'isolation',\n",
       " 'help',\n",
       " 'ensure',\n",
       " 'data',\n",
       " 'application',\n",
       " 'managed',\n",
       " 'privately',\n",
       " 'across',\n",
       " 'ibm',\n",
       " 'world',\n",
       " 'wide',\n",
       " 'data',\n",
       " 'center',\n",
       " 'variety',\n",
       " 'type',\n",
       " 'data',\n",
       " 'used',\n",
       " 'time',\n",
       " 'data',\n",
       " 'simpler',\n",
       " 'structured',\n",
       " 'relational',\n",
       " 'hierarchical',\n",
       " 'data',\n",
       " 'stored',\n",
       " 'database',\n",
       " 'big',\n",
       " 'data',\n",
       " 'simply',\n",
       " 'mean',\n",
       " 'data',\n",
       " 'includes',\n",
       " 'volume',\n",
       " 'raw',\n",
       " 'content',\n",
       " 'structured',\n",
       " 'form',\n",
       " 'part',\n",
       " 'unstructured',\n",
       " 'add',\n",
       " 'internet',\n",
       " 'thing',\n",
       " 'sending',\n",
       " 'massive',\n",
       " 'amount',\n",
       " 'sensor',\n",
       " 'data',\n",
       " 'world',\n",
       " 'simple',\n",
       " 'ibm',\n",
       " 'data',\n",
       " 'strategy',\n",
       " 'make',\n",
       " 'data',\n",
       " 'simple',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'capability',\n",
       " 'leverage',\n",
       " 'strategy',\n",
       " 'able',\n",
       " 'process',\n",
       " 'structured',\n",
       " 'semi',\n",
       " 'structured',\n",
       " 'unstructured',\n",
       " 'data',\n",
       " 'set',\n",
       " 'using',\n",
       " 'many',\n",
       " 'connector',\n",
       " 'abstracting',\n",
       " 'complexity',\n",
       " 'exploiting',\n",
       " 'spark',\n",
       " 'r',\n",
       " 'python',\n",
       " 'runtimes',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ibm',\n",
       " 'provides',\n",
       " '20',\n",
       " 'different',\n",
       " 'data',\n",
       " 'source',\n",
       " 'connector',\n",
       " 'organization',\n",
       " 'ingest',\n",
       " 'data',\n",
       " 'large',\n",
       " 'compute',\n",
       " 'power',\n",
       " 'enterprise',\n",
       " 'need',\n",
       " 'high',\n",
       " 'compute',\n",
       " 'power',\n",
       " 'since',\n",
       " 'process',\n",
       " 'ever',\n",
       " 'increasing',\n",
       " 'work',\n",
       " 'load',\n",
       " 'data',\n",
       " 'transaction',\n",
       " 'process',\n",
       " 'since',\n",
       " 'spark',\n",
       " 'service',\n",
       " 'single',\n",
       " 'multi',\n",
       " 'tenant',\n",
       " 'cluster',\n",
       " 'resource',\n",
       " 'utilization',\n",
       " 'wasted',\n",
       " 'repurpose',\n",
       " 'computer',\n",
       " 'power',\n",
       " 'scaling',\n",
       " 'scaling',\n",
       " 'capability',\n",
       " 'built',\n",
       " 'part',\n",
       " 'service',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'able',\n",
       " 'transparently',\n",
       " 'enable',\n",
       " 'scale',\n",
       " 'scale',\n",
       " 'capability',\n",
       " 'information',\n",
       " 'governance',\n",
       " 'ibm',\n",
       " 'strong',\n",
       " 'heritage',\n",
       " 'information',\n",
       " 'governance',\n",
       " 'managing',\n",
       " 'data',\n",
       " 'lifecycle',\n",
       " 'cleansing',\n",
       " 'quality',\n",
       " 'data',\n",
       " 'data',\n",
       " 'wrangling',\n",
       " 'shaping',\n",
       " 'security',\n",
       " 'privacy',\n",
       " 'data',\n",
       " 'information',\n",
       " 'governance',\n",
       " 'catalogue',\n",
       " 'us',\n",
       " 'policy',\n",
       " 'help',\n",
       " 'ensure',\n",
       " 'right',\n",
       " 'people',\n",
       " 'see',\n",
       " 'access',\n",
       " 'execute',\n",
       " 'data',\n",
       " 'service',\n",
       " 'ibm',\n",
       " 'also',\n",
       " 'help',\n",
       " 'provide',\n",
       " 'real',\n",
       " 'time',\n",
       " 'monitoring',\n",
       " 'threat',\n",
       " 'detection',\n",
       " 'prevention',\n",
       " 'intervention',\n",
       " 'well',\n",
       " 'forensics',\n",
       " 'compliance',\n",
       " 'detailed',\n",
       " 'audit',\n",
       " 'obfuscation',\n",
       " 'masking',\n",
       " 'data',\n",
       " 'encryption',\n",
       " 'applied',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'training',\n",
       " 'data',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'one',\n",
       " 'trick',\n",
       " 'pony',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'mlaas',\n",
       " 'many',\n",
       " 'form',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'system',\n",
       " 'ml',\n",
       " 'ibm',\n",
       " 'donated',\n",
       " 'apache',\n",
       " 'foundation',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'vision',\n",
       " 'personality',\n",
       " 'emotional',\n",
       " 'insight',\n",
       " 'customer',\n",
       " 'sentiment',\n",
       " 'retrieve',\n",
       " 'rank',\n",
       " 'ibm',\n",
       " 'makeit',\n",
       " 'simple',\n",
       " 'enterprise',\n",
       " 'size',\n",
       " 'pick',\n",
       " 'choose',\n",
       " 'number',\n",
       " 'predefined',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'bluemix',\n",
       " 'tile',\n",
       " 'figure',\n",
       " '2',\n",
       " 'ran',\n",
       " 'earlier',\n",
       " 'blog',\n",
       " 'personality',\n",
       " 'analyser',\n",
       " 'knew',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'guy',\n",
       " 'reassuring',\n",
       " 'hear',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'service',\n",
       " 'believe',\n",
       " 'try',\n",
       " 'figure',\n",
       " '2',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'bluemix',\n",
       " 'service',\n",
       " 'using',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'reduce',\n",
       " 'cost',\n",
       " 'risk',\n",
       " 'many',\n",
       " 'customer',\n",
       " 'across',\n",
       " 'different',\n",
       " 'industry',\n",
       " 'used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'capability',\n",
       " 'help',\n",
       " 'reduce',\n",
       " 'cost',\n",
       " 'improve',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'reduce',\n",
       " 'risk',\n",
       " 'vermont',\n",
       " 'electrical',\n",
       " 'power',\n",
       " 'company',\n",
       " 'velco',\n",
       " 'worked',\n",
       " 'ibm',\n",
       " 'research',\n",
       " 'develop',\n",
       " 'integrated',\n",
       " 'weather',\n",
       " 'forecasting',\n",
       " 'system',\n",
       " 'help',\n",
       " 'deliver',\n",
       " 'reliable',\n",
       " 'clean',\n",
       " 'affordable',\n",
       " 'power',\n",
       " 'consumer',\n",
       " 'integrating',\n",
       " 'renewable',\n",
       " 'energy',\n",
       " 'grid',\n",
       " 'solution',\n",
       " 'combine',\n",
       " 'high',\n",
       " 'resolution',\n",
       " 'weather',\n",
       " 'multiple',\n",
       " 'forecasting',\n",
       " 'tool',\n",
       " 'based',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'trained',\n",
       " 'hindcasts',\n",
       " 'weather',\n",
       " 'correlated',\n",
       " 'historical',\n",
       " 'energy',\n",
       " 'production',\n",
       " 'historical',\n",
       " 'net',\n",
       " 'demand',\n",
       " 'result',\n",
       " 'precise',\n",
       " 'accurate',\n",
       " 'wind',\n",
       " 'solar',\n",
       " ...]"
      ]
     },
     "execution_count": 1808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_id = 455\n",
    "relevant_articles = loopup_similar_articles(target_id, n=30)\n",
    "\n",
    "r_ids = relevant_articles[0]\n",
    "r_ids\n",
    "\n",
    "# concat tokens: for an article, combine token of title, desc, body all together to represent its tokened text\n",
    "test_id = 800\n",
    "\n",
    "title = df_nlp.loc[test_id].doc_full_name\n",
    "desc = df_nlp.loc[test_id].doc_description\n",
    "body = df_nlp.loc[test_id].doc_body\n",
    "\n",
    "tokened_text = tokenize(title) + tokenize(desc) + tokenize(body)\n",
    "tokened_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning',\n",
       " 'learning enterprise',\n",
       " 'enterprise last',\n",
       " 'last blog',\n",
       " 'blog business',\n",
       " 'business differentiation',\n",
       " 'differentiation machine',\n",
       " 'machine learning',\n",
       " 'learning introduced',\n",
       " 'introduced described',\n",
       " 'described concept',\n",
       " 'concept machine',\n",
       " 'machine learning',\n",
       " 'learning traced',\n",
       " 'traced origin',\n",
       " 'origin computer',\n",
       " 'computer science',\n",
       " 'science project',\n",
       " 'project watson',\n",
       " 'watson show',\n",
       " 'show skip',\n",
       " 'skip contentdinesh',\n",
       " 'contentdinesh nirmal',\n",
       " 'nirmal blog',\n",
       " 'blog blog',\n",
       " 'blog big',\n",
       " 'big data',\n",
       " 'data analytics',\n",
       " 'analytics digital',\n",
       " 'digital technology',\n",
       " 'technology impacting',\n",
       " 'impacting business',\n",
       " 'business data',\n",
       " 'data driven',\n",
       " 'driven economy',\n",
       " 'economy menu',\n",
       " 'menu welcome',\n",
       " 'welcome twitter',\n",
       " 'twitter machine',\n",
       " 'machine learning',\n",
       " 'learning enterprise',\n",
       " 'enterprise last',\n",
       " 'last blog',\n",
       " 'blog business',\n",
       " 'business differentiation',\n",
       " 'differentiation machine',\n",
       " 'machine learning',\n",
       " 'learning introduced',\n",
       " 'introduced described',\n",
       " 'described concept',\n",
       " 'concept machine',\n",
       " 'machine learning',\n",
       " 'learning traced',\n",
       " 'traced origin',\n",
       " 'origin computer',\n",
       " 'computer science',\n",
       " 'science project',\n",
       " 'project watson',\n",
       " 'watson showcasing',\n",
       " 'showcasing winning',\n",
       " 'winning jeopardy',\n",
       " 'jeopardy tv',\n",
       " 'tv quiz',\n",
       " 'quiz show',\n",
       " 'show real',\n",
       " 'real world',\n",
       " 'world use',\n",
       " 'use across',\n",
       " 'across numerous',\n",
       " 'numerous industry',\n",
       " 'industry including',\n",
       " 'including health',\n",
       " 'health care',\n",
       " 'care concluded',\n",
       " 'concluded machine',\n",
       " 'machine learning',\n",
       " 'learning potential',\n",
       " 'potential help',\n",
       " 'help make',\n",
       " 'make world',\n",
       " 'world better',\n",
       " 'better safer',\n",
       " 'safer place',\n",
       " 'place however',\n",
       " 'however yep',\n",
       " 'yep always',\n",
       " 'always however',\n",
       " 'however order',\n",
       " 'order become',\n",
       " 'become reality',\n",
       " 'reality machine',\n",
       " 'machine learning',\n",
       " 'learning form',\n",
       " 'form enterprise',\n",
       " 'enterprise ready',\n",
       " 'ready enterprise',\n",
       " 'enterprise requirement',\n",
       " 'requirement use',\n",
       " 'use word',\n",
       " 'word enterprise',\n",
       " 'enterprise mean',\n",
       " 'mean organization',\n",
       " 'organization business',\n",
       " 'business critical',\n",
       " 'critical requirement',\n",
       " 'requirement organization',\n",
       " 'organization size',\n",
       " 'size volume',\n",
       " 'volume transaction',\n",
       " 'transaction data',\n",
       " 'data velocity',\n",
       " 'velocity interaction',\n",
       " 'interaction variety',\n",
       " 'variety data',\n",
       " 'data yes',\n",
       " 'yes v',\n",
       " 'v big',\n",
       " 'big data',\n",
       " 'data key',\n",
       " 'key factor',\n",
       " 'factor might',\n",
       " 'might impact',\n",
       " 'impact organization',\n",
       " 'organization machine',\n",
       " 'machine learning',\n",
       " 'learning requirement',\n",
       " 'requirement also',\n",
       " 'also collaboration',\n",
       " 'collaboration across',\n",
       " 'across data',\n",
       " 'data scientist',\n",
       " 'scientist engineer',\n",
       " 'engineer developer',\n",
       " 'developer creating',\n",
       " 'creating testing',\n",
       " 'testing training',\n",
       " 'training deploying',\n",
       " 'deploying machine',\n",
       " 'machine learning',\n",
       " 'learning model',\n",
       " 'model level',\n",
       " 'level different',\n",
       " 'different audience',\n",
       " 'audience want',\n",
       " 'want exposed',\n",
       " 'exposed machine',\n",
       " 'machine learning',\n",
       " 'learning let',\n",
       " 'let look',\n",
       " 'look factor',\n",
       " 'factor make',\n",
       " 'make machine',\n",
       " 'machine learning',\n",
       " 'learning ibm',\n",
       " 'ibm truly',\n",
       " 'truly enterprise',\n",
       " 'enterprise ready',\n",
       " 'ready collaboration',\n",
       " 'collaboration large',\n",
       " 'large enterprise',\n",
       " 'enterprise tend',\n",
       " 'tend significant',\n",
       " 'significant data',\n",
       " 'data scientist',\n",
       " 'scientist team',\n",
       " 'team often',\n",
       " 'often multiple',\n",
       " 'multiple data',\n",
       " 'data scientist',\n",
       " 'scientist engaged',\n",
       " 'engaged single',\n",
       " 'single project',\n",
       " 'project collaboration',\n",
       " 'collaboration across',\n",
       " 'across data',\n",
       " 'data scientist',\n",
       " 'scientist maybe',\n",
       " 'maybe persona',\n",
       " 'persona required',\n",
       " 'required maximize',\n",
       " 'maximize productivity',\n",
       " 'productivity agility',\n",
       " 'agility effectiveness',\n",
       " 'effectiveness today',\n",
       " 'today part',\n",
       " 'part ibm',\n",
       " 'ibm data',\n",
       " 'data science',\n",
       " 'science experience',\n",
       " 'experience bring',\n",
       " 'bring concept',\n",
       " 'concept project',\n",
       " 'project wherein',\n",
       " 'wherein various',\n",
       " 'various persona',\n",
       " 'persona user',\n",
       " 'user safely',\n",
       " 'safely collaborate',\n",
       " 'collaborate project',\n",
       " 'project build',\n",
       " 'build test',\n",
       " 'test use',\n",
       " 'use deploy',\n",
       " 'deploy many',\n",
       " 'many artefact',\n",
       " 'artefact group',\n",
       " 'group people',\n",
       " 'people machine',\n",
       " 'machine learning',\n",
       " 'learning technology',\n",
       " 'technology adopt',\n",
       " 'adopt concept',\n",
       " 'concept able',\n",
       " 'able share',\n",
       " 'share analytic',\n",
       " 'analytic artefact',\n",
       " 'artefact notebook',\n",
       " 'notebook pipeline',\n",
       " 'pipeline model',\n",
       " 'model etc',\n",
       " 'etc example',\n",
       " 'example group',\n",
       " 'group people',\n",
       " 'people collaborate',\n",
       " 'collaborate single',\n",
       " 'single notebook',\n",
       " 'notebook wherein',\n",
       " 'wherein one',\n",
       " 'one person',\n",
       " 'person curation',\n",
       " 'curation transformation',\n",
       " 'transformation hand',\n",
       " 'hand another',\n",
       " 'another person',\n",
       " 'person creating',\n",
       " 'creating algorithm',\n",
       " 'algorithm testing',\n",
       " 'testing training',\n",
       " 'training model',\n",
       " 'model team',\n",
       " 'team member',\n",
       " 'member evaluate',\n",
       " 'evaluate model',\n",
       " 'model deploy',\n",
       " 'deploy individual',\n",
       " 'individual user',\n",
       " 'user authenticated',\n",
       " 'authenticated separately',\n",
       " 'separately authorized',\n",
       " 'authorized part',\n",
       " 'part role',\n",
       " 'role defined',\n",
       " 'defined project',\n",
       " 'project limiting',\n",
       " 'limiting granting',\n",
       " 'granting access',\n",
       " 'access part',\n",
       " 'part overall',\n",
       " 'overall process',\n",
       " 'process experience',\n",
       " 'experience accordingly',\n",
       " 'accordingly consumption',\n",
       " 'consumption everyone',\n",
       " 'everyone data',\n",
       " 'data scientist',\n",
       " 'scientist want',\n",
       " 'want need',\n",
       " 'need know',\n",
       " 'know model',\n",
       " 'model design',\n",
       " 'design statistical',\n",
       " 'statistical theory',\n",
       " 'theory training',\n",
       " 'training model',\n",
       " 'model developer',\n",
       " 'developer example',\n",
       " 'example may',\n",
       " 'may varying',\n",
       " 'varying level',\n",
       " 'level need',\n",
       " 'need may',\n",
       " 'may want',\n",
       " 'want able',\n",
       " 'able use',\n",
       " 'use known',\n",
       " 'known model',\n",
       " 'model work',\n",
       " 'work well',\n",
       " 'well deploy',\n",
       " 'deploy app',\n",
       " 'app figure',\n",
       " 'figure 1',\n",
       " '1 show',\n",
       " 'show ibm',\n",
       " 'ibm designed',\n",
       " 'designed work',\n",
       " 'work space',\n",
       " 'space allows',\n",
       " 'allows application',\n",
       " 'application developer',\n",
       " 'developer choose',\n",
       " 'choose deploy',\n",
       " 'deploy model',\n",
       " 'model actually',\n",
       " 'actually create',\n",
       " 'create pipeline',\n",
       " 'pipeline via',\n",
       " 'via step',\n",
       " 'step step',\n",
       " 'step process',\n",
       " 'process take',\n",
       " 'take one',\n",
       " 'one step',\n",
       " 'step higher',\n",
       " 'higher level',\n",
       " 'level developer',\n",
       " 'developer may',\n",
       " 'may want',\n",
       " 'want choose',\n",
       " 'choose collection',\n",
       " 'collection pre',\n",
       " 'pre packaged',\n",
       " 'packaged machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service fraud',\n",
       " 'fraud detection',\n",
       " 'detection weather',\n",
       " 'weather prediction',\n",
       " 'prediction manufacturing',\n",
       " 'manufacturing model',\n",
       " 'model sentiment',\n",
       " 'sentiment analysis',\n",
       " 'analysis emotional',\n",
       " 'emotional analysis',\n",
       " 'analysis ibm',\n",
       " 'ibm provides',\n",
       " 'provides today',\n",
       " 'today bluemix',\n",
       " 'bluemix service',\n",
       " 'service integrated',\n",
       " 'integrated part',\n",
       " 'part data',\n",
       " 'data science',\n",
       " 'science experience',\n",
       " 'experience figure',\n",
       " 'figure 1',\n",
       " '1 integrated',\n",
       " 'integrated workspace',\n",
       " 'workspace creating',\n",
       " 'creating pipeline',\n",
       " 'pipeline commoditizing',\n",
       " 'commoditizing automating',\n",
       " 'automating machine',\n",
       " 'machine learning',\n",
       " 'learning machine',\n",
       " 'machine learning',\n",
       " 'learning enterprise',\n",
       " 'enterprise environment',\n",
       " 'environment challenging',\n",
       " 'challenging start',\n",
       " 'start assumption',\n",
       " 'assumption model',\n",
       " 'model becomes',\n",
       " 'becomes stale',\n",
       " 'stale minute',\n",
       " 'minute stop',\n",
       " 'stop training',\n",
       " 'training time',\n",
       " 'time accuracy',\n",
       " 'accuracy model',\n",
       " 'model worsen',\n",
       " 'worsen take',\n",
       " 'take significant',\n",
       " 'significant time',\n",
       " 'time understand',\n",
       " 'understand happening',\n",
       " 'happening retrain',\n",
       " 'retrain existing',\n",
       " 'existing model',\n",
       " 'model deploy',\n",
       " 'deploy new',\n",
       " 'new version',\n",
       " 'version come',\n",
       " 'come revenue',\n",
       " 'revenue enterprise',\n",
       " 'enterprise may',\n",
       " 'may hard',\n",
       " 'hard time',\n",
       " 'time adopting',\n",
       " 'adopting necessary',\n",
       " 'necessary discipline',\n",
       " 'discipline gauge',\n",
       " 'gauge impact',\n",
       " 'impact bottom',\n",
       " 'bottom line',\n",
       " 'line lot',\n",
       " 'lot machine',\n",
       " 'machine learning',\n",
       " 'learning use',\n",
       " 'use case',\n",
       " 'case might',\n",
       " 'might intuitive',\n",
       " 'intuitive sense',\n",
       " 'sense set',\n",
       " 'set clear',\n",
       " 'clear control',\n",
       " 'control point',\n",
       " 'point flow',\n",
       " 'flow people',\n",
       " 'people logically',\n",
       " 'logically relate',\n",
       " 'relate term',\n",
       " 'term machine',\n",
       " 'machine learning',\n",
       " 'learning may',\n",
       " 'may send',\n",
       " 'send le',\n",
       " 'le scientific',\n",
       " 'scientific people',\n",
       " 'people community',\n",
       " 'community running',\n",
       " 'running opposite',\n",
       " 'opposite direction',\n",
       " 'direction often',\n",
       " 'often data',\n",
       " 'data scientist',\n",
       " 'scientist perform',\n",
       " 'perform number',\n",
       " 'number tedious',\n",
       " 'tedious time',\n",
       " 'time consuming',\n",
       " 'consuming step',\n",
       " 'step derive',\n",
       " 'derive insight',\n",
       " 'insight raw',\n",
       " 'raw data',\n",
       " 'data set',\n",
       " 'set process',\n",
       " 'process involve',\n",
       " 'involve data',\n",
       " 'data ingestion',\n",
       " 'ingestion cleaning',\n",
       " 'cleaning transformation',\n",
       " 'transformation e',\n",
       " 'e g',\n",
       " 'g outlier',\n",
       " 'outlier removal',\n",
       " 'removal missing',\n",
       " 'missing value',\n",
       " 'value imputation',\n",
       " 'imputation proceed',\n",
       " 'proceed model',\n",
       " 'model building',\n",
       " 'building finally',\n",
       " 'finally presentation',\n",
       " 'presentation prediction',\n",
       " 'prediction align',\n",
       " 'align end',\n",
       " 'end user',\n",
       " 'user objective',\n",
       " 'objective preference',\n",
       " 'preference long',\n",
       " 'long complex',\n",
       " 'complex sometimes',\n",
       " 'sometimes artful',\n",
       " 'artful process',\n",
       " 'process requiring',\n",
       " 'requiring substantial',\n",
       " 'substantial time',\n",
       " 'time effort',\n",
       " 'effort especially',\n",
       " 'especially combinatorial',\n",
       " 'combinatorial explosion',\n",
       " 'explosion choice',\n",
       " 'choice algorithm',\n",
       " 'algorithm platform',\n",
       " 'platform parameter',\n",
       " 'parameter composition',\n",
       " 'composition tool',\n",
       " 'tool help',\n",
       " 'help automate',\n",
       " 'automate step',\n",
       " 'step process',\n",
       " 'process potential',\n",
       " 'potential accelerate',\n",
       " 'accelerate time',\n",
       " 'time delivery',\n",
       " 'delivery useful',\n",
       " 'useful result',\n",
       " 'result expand',\n",
       " 'expand reach',\n",
       " 'reach data',\n",
       " 'data science',\n",
       " 'science non',\n",
       " 'non expert',\n",
       " 'expert offer',\n",
       " 'offer systematic',\n",
       " 'systematic exploration',\n",
       " 'exploration available',\n",
       " 'available option',\n",
       " 'option cognitive',\n",
       " 'cognitive automation',\n",
       " 'automation data',\n",
       " 'data science',\n",
       " 'science cad',\n",
       " 'cad help',\n",
       " 'help integrate',\n",
       " 'integrate learning',\n",
       " 'learning planning',\n",
       " 'planning composition',\n",
       " 'composition orchestration',\n",
       " 'orchestration technique',\n",
       " 'technique automatically',\n",
       " 'automatically efficiently',\n",
       " 'efficiently build',\n",
       " 'build model',\n",
       " 'model done',\n",
       " 'done deploying',\n",
       " 'deploying analytic',\n",
       " 'analytic flow',\n",
       " 'flow interactively',\n",
       " 'interactively support',\n",
       " 'support would',\n",
       " 'would data',\n",
       " 'data scientist',\n",
       " 'scientist task',\n",
       " 'task cad',\n",
       " 'cad also',\n",
       " 'also provides',\n",
       " 'provides capability',\n",
       " 'capability run',\n",
       " 'run multiple',\n",
       " 'multiple predefined',\n",
       " 'predefined algorithm',\n",
       " 'algorithm parallel',\n",
       " 'parallel identify',\n",
       " 'identify best',\n",
       " 'best suitable',\n",
       " 'suitable algorithm',\n",
       " 'algorithm particular',\n",
       " 'particular use',\n",
       " 'use case',\n",
       " 'case short',\n",
       " 'short cad',\n",
       " 'cad selects',\n",
       " 'selects best',\n",
       " 'best algorithm',\n",
       " 'algorithm given',\n",
       " 'given use',\n",
       " 'use case',\n",
       " 'case click',\n",
       " 'click read',\n",
       " 'read ibm',\n",
       " 'ibm paper',\n",
       " 'paper cad',\n",
       " 'cad training',\n",
       " 'training tuning',\n",
       " 'tuning model',\n",
       " 'model optimization',\n",
       " 'optimization time',\n",
       " 'time model',\n",
       " 'model algorithm',\n",
       " 'algorithm used',\n",
       " 'used machine',\n",
       " 'machine learning',\n",
       " 'learning becomes',\n",
       " 'becomes good',\n",
       " 'good true',\n",
       " 'true training',\n",
       " 'training data',\n",
       " 'data model',\n",
       " 'model predicts',\n",
       " 'predicts training',\n",
       " 'training data',\n",
       " 'data well',\n",
       " 'well performs',\n",
       " 'performs poorly',\n",
       " 'poorly new',\n",
       " 'new data',\n",
       " 'data known',\n",
       " 'known overfitting',\n",
       " 'overfitting extreme',\n",
       " 'extreme case',\n",
       " 'case occur',\n",
       " 'occur rote',\n",
       " 'rote learning',\n",
       " 'learning model',\n",
       " 'model achieves',\n",
       " 'achieves 100',\n",
       " '100 performance',\n",
       " 'performance data',\n",
       " 'data already',\n",
       " 'already seen',\n",
       " 'seen probably',\n",
       " 'probably better',\n",
       " 'better random',\n",
       " 'random guess',\n",
       " 'guess new',\n",
       " 'new data',\n",
       " 'data imagine',\n",
       " 'imagine would',\n",
       " 'would business',\n",
       " 'business could',\n",
       " 'could ruin',\n",
       " 'ruin upset',\n",
       " 'upset customer',\n",
       " 'customer set',\n",
       " 'set wrong',\n",
       " 'wrong price',\n",
       " 'price point',\n",
       " 'point miss',\n",
       " 'miss many',\n",
       " 'many business',\n",
       " 'business opportunity',\n",
       " 'opportunity ibm',\n",
       " 'ibm machine',\n",
       " 'machine learning',\n",
       " 'learning help',\n",
       " 'help counter',\n",
       " 'counter clean',\n",
       " 'clean separation',\n",
       " 'separation training',\n",
       " 'training data',\n",
       " 'data holdout',\n",
       " 'holdout data',\n",
       " 'data used',\n",
       " 'used evaluate',\n",
       " 'evaluate model',\n",
       " 'model performance',\n",
       " 'performance well',\n",
       " 'well careful',\n",
       " 'careful use',\n",
       " 'use cross',\n",
       " 'cross validation',\n",
       " 'validation technique',\n",
       " 'technique data',\n",
       " 'data sovereignty',\n",
       " 'sovereignty isolation',\n",
       " 'isolation enterprise',\n",
       " 'enterprise organization',\n",
       " 'organization fear',\n",
       " 'fear psychological',\n",
       " 'psychological otherwise',\n",
       " 'otherwise come',\n",
       " 'come putting',\n",
       " 'putting data',\n",
       " 'data application',\n",
       " 'application hardware',\n",
       " 'hardware storage',\n",
       " 'storage network',\n",
       " 'network infrastructure',\n",
       " 'infrastructure shared',\n",
       " 'shared organization',\n",
       " 'organization ibm',\n",
       " 'ibm cloud',\n",
       " 'cloud first',\n",
       " 'first strategy',\n",
       " 'strategy provides',\n",
       " 'provides necessary',\n",
       " 'necessary sovereignty',\n",
       " 'sovereignty multi',\n",
       " 'multi tenancy',\n",
       " 'tenancy isolation',\n",
       " 'isolation help',\n",
       " 'help ensure',\n",
       " 'ensure data',\n",
       " 'data application',\n",
       " 'application managed',\n",
       " 'managed privately',\n",
       " 'privately across',\n",
       " 'across ibm',\n",
       " 'ibm world',\n",
       " 'world wide',\n",
       " 'wide data',\n",
       " 'data center',\n",
       " 'center variety',\n",
       " 'variety type',\n",
       " 'type data',\n",
       " 'data used',\n",
       " 'used time',\n",
       " 'time data',\n",
       " 'data simpler',\n",
       " 'simpler structured',\n",
       " 'structured relational',\n",
       " 'relational hierarchical',\n",
       " 'hierarchical data',\n",
       " 'data stored',\n",
       " 'stored database',\n",
       " 'database big',\n",
       " 'big data',\n",
       " 'data simply',\n",
       " 'simply mean',\n",
       " 'mean data',\n",
       " 'data includes',\n",
       " 'includes volume',\n",
       " 'volume raw',\n",
       " 'raw content',\n",
       " 'content structured',\n",
       " 'structured form',\n",
       " 'form part',\n",
       " 'part unstructured',\n",
       " 'unstructured add',\n",
       " 'add internet',\n",
       " 'internet thing',\n",
       " 'thing sending',\n",
       " 'sending massive',\n",
       " 'massive amount',\n",
       " 'amount sensor',\n",
       " 'sensor data',\n",
       " 'data world',\n",
       " 'world simple',\n",
       " 'simple ibm',\n",
       " 'ibm data',\n",
       " 'data strategy',\n",
       " 'strategy make',\n",
       " 'make data',\n",
       " 'data simple',\n",
       " 'simple machine',\n",
       " 'machine learning',\n",
       " 'learning capability',\n",
       " 'capability leverage',\n",
       " 'leverage strategy',\n",
       " 'strategy able',\n",
       " 'able process',\n",
       " 'process structured',\n",
       " 'structured semi',\n",
       " 'semi structured',\n",
       " 'structured unstructured',\n",
       " 'unstructured data',\n",
       " 'data set',\n",
       " 'set using',\n",
       " 'using many',\n",
       " 'many connector',\n",
       " 'connector abstracting',\n",
       " 'abstracting complexity',\n",
       " 'complexity exploiting',\n",
       " 'exploiting spark',\n",
       " 'spark r',\n",
       " 'r python',\n",
       " 'python runtimes',\n",
       " 'runtimes machine',\n",
       " 'machine learning',\n",
       " 'learning ibm',\n",
       " 'ibm provides',\n",
       " 'provides 20',\n",
       " '20 different',\n",
       " 'different data',\n",
       " 'data source',\n",
       " 'source connector',\n",
       " 'connector organization',\n",
       " 'organization ingest',\n",
       " 'ingest data',\n",
       " 'data large',\n",
       " 'large compute',\n",
       " 'compute power',\n",
       " 'power enterprise',\n",
       " 'enterprise need',\n",
       " 'need high',\n",
       " 'high compute',\n",
       " 'compute power',\n",
       " 'power since',\n",
       " 'since process',\n",
       " 'process ever',\n",
       " 'ever increasing',\n",
       " 'increasing work',\n",
       " 'work load',\n",
       " 'load data',\n",
       " 'data transaction',\n",
       " 'transaction process',\n",
       " 'process since',\n",
       " 'since spark',\n",
       " 'spark service',\n",
       " 'service single',\n",
       " 'single multi',\n",
       " 'multi tenant',\n",
       " 'tenant cluster',\n",
       " 'cluster resource',\n",
       " 'resource utilization',\n",
       " 'utilization wasted',\n",
       " 'wasted repurpose',\n",
       " 'repurpose computer',\n",
       " 'computer power',\n",
       " 'power scaling',\n",
       " 'scaling scaling',\n",
       " 'scaling capability',\n",
       " 'capability built',\n",
       " 'built part',\n",
       " 'part service',\n",
       " 'service machine',\n",
       " 'machine learning',\n",
       " 'learning able',\n",
       " 'able transparently',\n",
       " 'transparently enable',\n",
       " 'enable scale',\n",
       " 'scale scale',\n",
       " 'scale capability',\n",
       " 'capability information',\n",
       " 'information governance',\n",
       " 'governance ibm',\n",
       " 'ibm strong',\n",
       " 'strong heritage',\n",
       " 'heritage information',\n",
       " 'information governance',\n",
       " 'governance managing',\n",
       " 'managing data',\n",
       " 'data lifecycle',\n",
       " 'lifecycle cleansing',\n",
       " 'cleansing quality',\n",
       " 'quality data',\n",
       " 'data data',\n",
       " 'data wrangling',\n",
       " 'wrangling shaping',\n",
       " 'shaping security',\n",
       " 'security privacy',\n",
       " 'privacy data',\n",
       " 'data information',\n",
       " 'information governance',\n",
       " 'governance catalogue',\n",
       " 'catalogue us',\n",
       " 'us policy',\n",
       " 'policy help',\n",
       " 'help ensure',\n",
       " 'ensure right',\n",
       " 'right people',\n",
       " 'people see',\n",
       " 'see access',\n",
       " 'access execute',\n",
       " 'execute data',\n",
       " 'data service',\n",
       " 'service ibm',\n",
       " 'ibm also',\n",
       " 'also help',\n",
       " 'help provide',\n",
       " 'provide real',\n",
       " 'real time',\n",
       " 'time monitoring',\n",
       " 'monitoring threat',\n",
       " 'threat detection',\n",
       " 'detection prevention',\n",
       " 'prevention intervention',\n",
       " 'intervention well',\n",
       " 'well forensics',\n",
       " 'forensics compliance',\n",
       " 'compliance detailed',\n",
       " 'detailed audit',\n",
       " 'audit obfuscation',\n",
       " 'obfuscation masking',\n",
       " 'masking data',\n",
       " 'data encryption',\n",
       " 'encryption applied',\n",
       " 'applied machine',\n",
       " 'machine learning',\n",
       " 'learning training',\n",
       " 'training data',\n",
       " 'data machine',\n",
       " 'machine learning',\n",
       " 'learning one',\n",
       " 'one trick',\n",
       " 'trick pony',\n",
       " 'pony machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service mlaas',\n",
       " 'mlaas many',\n",
       " 'many form',\n",
       " 'form machine',\n",
       " 'machine learning',\n",
       " 'learning system',\n",
       " 'system ml',\n",
       " 'ml ibm',\n",
       " 'ibm donated',\n",
       " 'donated apache',\n",
       " 'apache foundation',\n",
       " 'foundation natural',\n",
       " 'natural language',\n",
       " 'language processing',\n",
       " 'processing vision',\n",
       " 'vision personality',\n",
       " 'personality emotional',\n",
       " 'emotional insight',\n",
       " 'insight customer',\n",
       " 'customer sentiment',\n",
       " 'sentiment retrieve',\n",
       " 'retrieve rank',\n",
       " 'rank ibm',\n",
       " 'ibm makeit',\n",
       " 'makeit simple',\n",
       " 'simple enterprise',\n",
       " 'enterprise size',\n",
       " 'size pick',\n",
       " 'pick choose',\n",
       " 'choose number',\n",
       " 'number predefined',\n",
       " 'predefined machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service bluemix',\n",
       " 'bluemix tile',\n",
       " 'tile figure',\n",
       " 'figure 2',\n",
       " '2 ran',\n",
       " 'ran earlier',\n",
       " 'earlier blog',\n",
       " 'blog personality',\n",
       " 'personality analyser',\n",
       " 'analyser knew',\n",
       " 'knew really',\n",
       " 'really nice',\n",
       " 'nice guy',\n",
       " 'guy reassuring',\n",
       " 'reassuring hear',\n",
       " 'hear machine',\n",
       " 'machine learning',\n",
       " 'learning service',\n",
       " 'service believe',\n",
       " 'believe try',\n",
       " 'try figure',\n",
       " 'figure 2',\n",
       " '2 machine',\n",
       " 'machine learning',\n",
       " 'learning bluemix',\n",
       " 'bluemix service',\n",
       " 'service using',\n",
       " 'using machine',\n",
       " 'machine learning',\n",
       " 'learning reduce',\n",
       " 'reduce cost',\n",
       " 'cost risk',\n",
       " 'risk many',\n",
       " 'many customer',\n",
       " 'customer across',\n",
       " 'across different',\n",
       " 'different industry',\n",
       " 'industry used',\n",
       " 'used machine',\n",
       " 'machine learning',\n",
       " 'learning capability',\n",
       " 'capability help',\n",
       " 'help reduce',\n",
       " 'reduce cost',\n",
       " 'cost improve',\n",
       " 'improve customer',\n",
       " 'customer service',\n",
       " 'service reduce',\n",
       " 'reduce risk',\n",
       " 'risk vermont',\n",
       " 'vermont electrical',\n",
       " 'electrical power',\n",
       " 'power company',\n",
       " 'company velco',\n",
       " 'velco worked',\n",
       " 'worked ibm',\n",
       " 'ibm research',\n",
       " 'research develop',\n",
       " 'develop integrated',\n",
       " 'integrated weather',\n",
       " 'weather forecasting',\n",
       " 'forecasting system',\n",
       " 'system help',\n",
       " 'help deliver',\n",
       " 'deliver reliable',\n",
       " 'reliable clean',\n",
       " 'clean affordable',\n",
       " 'affordable power',\n",
       " 'power consumer',\n",
       " 'consumer integrating',\n",
       " 'integrating renewable',\n",
       " 'renewable energy',\n",
       " 'energy grid',\n",
       " 'grid solution',\n",
       " 'solution combine',\n",
       " 'combine high',\n",
       " 'high resolution',\n",
       " 'resolution weather',\n",
       " 'weather multiple',\n",
       " 'multiple forecasting',\n",
       " 'forecasting tool',\n",
       " 'tool based',\n",
       " 'based machine',\n",
       " 'machine learning',\n",
       " 'learning machine',\n",
       " 'machine learning',\n",
       " 'learning model',\n",
       " 'model trained',\n",
       " 'trained hindcasts',\n",
       " 'hindcasts weather',\n",
       " 'weather correlated',\n",
       " 'correlated historical',\n",
       " 'historical energy',\n",
       " 'energy production',\n",
       " 'production historical',\n",
       " 'historical net',\n",
       " 'net demand',\n",
       " 'demand result',\n",
       " 'result precise',\n",
       " 'precise accurate',\n",
       " 'accurate wind',\n",
       " 'wind solar',\n",
       " 'solar generation',\n",
       " ...]"
      ]
     },
     "execution_count": 1823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two grams\n",
    "\n",
    "[f\"{x[0]} {x[1]}\" for x in list(bigrams(tokened_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning enterprise',\n",
       " 'learning enterprise last',\n",
       " 'enterprise last blog',\n",
       " 'last blog business',\n",
       " 'blog business differentiation',\n",
       " 'business differentiation machine',\n",
       " 'differentiation machine learning',\n",
       " 'machine learning introduced',\n",
       " 'learning introduced described',\n",
       " 'introduced described concept',\n",
       " 'described concept machine',\n",
       " 'concept machine learning',\n",
       " 'machine learning traced',\n",
       " 'learning traced origin',\n",
       " 'traced origin computer',\n",
       " 'origin computer science',\n",
       " 'computer science project',\n",
       " 'science project watson',\n",
       " 'project watson show',\n",
       " 'watson show skip',\n",
       " 'show skip contentdinesh',\n",
       " 'skip contentdinesh nirmal',\n",
       " 'contentdinesh nirmal blog',\n",
       " 'nirmal blog blog',\n",
       " 'blog blog big',\n",
       " 'blog big data',\n",
       " 'big data analytics',\n",
       " 'data analytics digital',\n",
       " 'analytics digital technology',\n",
       " 'digital technology impacting',\n",
       " 'technology impacting business',\n",
       " 'impacting business data',\n",
       " 'business data driven',\n",
       " 'data driven economy',\n",
       " 'driven economy menu',\n",
       " 'economy menu welcome',\n",
       " 'menu welcome twitter',\n",
       " 'welcome twitter machine',\n",
       " 'twitter machine learning',\n",
       " 'machine learning enterprise',\n",
       " 'learning enterprise last',\n",
       " 'enterprise last blog',\n",
       " 'last blog business',\n",
       " 'blog business differentiation',\n",
       " 'business differentiation machine',\n",
       " 'differentiation machine learning',\n",
       " 'machine learning introduced',\n",
       " 'learning introduced described',\n",
       " 'introduced described concept',\n",
       " 'described concept machine',\n",
       " 'concept machine learning',\n",
       " 'machine learning traced',\n",
       " 'learning traced origin',\n",
       " 'traced origin computer',\n",
       " 'origin computer science',\n",
       " 'computer science project',\n",
       " 'science project watson',\n",
       " 'project watson showcasing',\n",
       " 'watson showcasing winning',\n",
       " 'showcasing winning jeopardy',\n",
       " 'winning jeopardy tv',\n",
       " 'jeopardy tv quiz',\n",
       " 'tv quiz show',\n",
       " 'quiz show real',\n",
       " 'show real world',\n",
       " 'real world use',\n",
       " 'world use across',\n",
       " 'use across numerous',\n",
       " 'across numerous industry',\n",
       " 'numerous industry including',\n",
       " 'industry including health',\n",
       " 'including health care',\n",
       " 'health care concluded',\n",
       " 'care concluded machine',\n",
       " 'concluded machine learning',\n",
       " 'machine learning potential',\n",
       " 'learning potential help',\n",
       " 'potential help make',\n",
       " 'help make world',\n",
       " 'make world better',\n",
       " 'world better safer',\n",
       " 'better safer place',\n",
       " 'safer place however',\n",
       " 'place however yep',\n",
       " 'however yep always',\n",
       " 'yep always however',\n",
       " 'always however order',\n",
       " 'however order become',\n",
       " 'order become reality',\n",
       " 'become reality machine',\n",
       " 'reality machine learning',\n",
       " 'machine learning form',\n",
       " 'learning form enterprise',\n",
       " 'form enterprise ready',\n",
       " 'enterprise ready enterprise',\n",
       " 'ready enterprise requirement',\n",
       " 'enterprise requirement use',\n",
       " 'requirement use word',\n",
       " 'use word enterprise',\n",
       " 'word enterprise mean',\n",
       " 'enterprise mean organization',\n",
       " 'mean organization business',\n",
       " 'organization business critical',\n",
       " 'business critical requirement',\n",
       " 'critical requirement organization',\n",
       " 'requirement organization size',\n",
       " 'organization size volume',\n",
       " 'size volume transaction',\n",
       " 'volume transaction data',\n",
       " 'transaction data velocity',\n",
       " 'data velocity interaction',\n",
       " 'velocity interaction variety',\n",
       " 'interaction variety data',\n",
       " 'variety data yes',\n",
       " 'data yes v',\n",
       " 'yes v big',\n",
       " 'v big data',\n",
       " 'big data key',\n",
       " 'data key factor',\n",
       " 'key factor might',\n",
       " 'factor might impact',\n",
       " 'might impact organization',\n",
       " 'impact organization machine',\n",
       " 'organization machine learning',\n",
       " 'machine learning requirement',\n",
       " 'learning requirement also',\n",
       " 'requirement also collaboration',\n",
       " 'also collaboration across',\n",
       " 'collaboration across data',\n",
       " 'across data scientist',\n",
       " 'data scientist engineer',\n",
       " 'scientist engineer developer',\n",
       " 'engineer developer creating',\n",
       " 'developer creating testing',\n",
       " 'creating testing training',\n",
       " 'testing training deploying',\n",
       " 'training deploying machine',\n",
       " 'deploying machine learning',\n",
       " 'machine learning model',\n",
       " 'learning model level',\n",
       " 'model level different',\n",
       " 'level different audience',\n",
       " 'different audience want',\n",
       " 'audience want exposed',\n",
       " 'want exposed machine',\n",
       " 'exposed machine learning',\n",
       " 'machine learning let',\n",
       " 'learning let look',\n",
       " 'let look factor',\n",
       " 'look factor make',\n",
       " 'factor make machine',\n",
       " 'make machine learning',\n",
       " 'machine learning ibm',\n",
       " 'learning ibm truly',\n",
       " 'ibm truly enterprise',\n",
       " 'truly enterprise ready',\n",
       " 'enterprise ready collaboration',\n",
       " 'ready collaboration large',\n",
       " 'collaboration large enterprise',\n",
       " 'large enterprise tend',\n",
       " 'enterprise tend significant',\n",
       " 'tend significant data',\n",
       " 'significant data scientist',\n",
       " 'data scientist team',\n",
       " 'scientist team often',\n",
       " 'team often multiple',\n",
       " 'often multiple data',\n",
       " 'multiple data scientist',\n",
       " 'data scientist engaged',\n",
       " 'scientist engaged single',\n",
       " 'engaged single project',\n",
       " 'single project collaboration',\n",
       " 'project collaboration across',\n",
       " 'collaboration across data',\n",
       " 'across data scientist',\n",
       " 'data scientist maybe',\n",
       " 'scientist maybe persona',\n",
       " 'maybe persona required',\n",
       " 'persona required maximize',\n",
       " 'required maximize productivity',\n",
       " 'maximize productivity agility',\n",
       " 'productivity agility effectiveness',\n",
       " 'agility effectiveness today',\n",
       " 'effectiveness today part',\n",
       " 'today part ibm',\n",
       " 'part ibm data',\n",
       " 'ibm data science',\n",
       " 'data science experience',\n",
       " 'science experience bring',\n",
       " 'experience bring concept',\n",
       " 'bring concept project',\n",
       " 'concept project wherein',\n",
       " 'project wherein various',\n",
       " 'wherein various persona',\n",
       " 'various persona user',\n",
       " 'persona user safely',\n",
       " 'user safely collaborate',\n",
       " 'safely collaborate project',\n",
       " 'collaborate project build',\n",
       " 'project build test',\n",
       " 'build test use',\n",
       " 'test use deploy',\n",
       " 'use deploy many',\n",
       " 'deploy many artefact',\n",
       " 'many artefact group',\n",
       " 'artefact group people',\n",
       " 'group people machine',\n",
       " 'people machine learning',\n",
       " 'machine learning technology',\n",
       " 'learning technology adopt',\n",
       " 'technology adopt concept',\n",
       " 'adopt concept able',\n",
       " 'concept able share',\n",
       " 'able share analytic',\n",
       " 'share analytic artefact',\n",
       " 'analytic artefact notebook',\n",
       " 'artefact notebook pipeline',\n",
       " 'notebook pipeline model',\n",
       " 'pipeline model etc',\n",
       " 'model etc example',\n",
       " 'etc example group',\n",
       " 'example group people',\n",
       " 'group people collaborate',\n",
       " 'people collaborate single',\n",
       " 'collaborate single notebook',\n",
       " 'single notebook wherein',\n",
       " 'notebook wherein one',\n",
       " 'wherein one person',\n",
       " 'one person curation',\n",
       " 'person curation transformation',\n",
       " 'curation transformation hand',\n",
       " 'transformation hand another',\n",
       " 'hand another person',\n",
       " 'another person creating',\n",
       " 'person creating algorithm',\n",
       " 'creating algorithm testing',\n",
       " 'algorithm testing training',\n",
       " 'testing training model',\n",
       " 'training model team',\n",
       " 'model team member',\n",
       " 'team member evaluate',\n",
       " 'member evaluate model',\n",
       " 'evaluate model deploy',\n",
       " 'model deploy individual',\n",
       " 'deploy individual user',\n",
       " 'individual user authenticated',\n",
       " 'user authenticated separately',\n",
       " 'authenticated separately authorized',\n",
       " 'separately authorized part',\n",
       " 'authorized part role',\n",
       " 'part role defined',\n",
       " 'role defined project',\n",
       " 'defined project limiting',\n",
       " 'project limiting granting',\n",
       " 'limiting granting access',\n",
       " 'granting access part',\n",
       " 'access part overall',\n",
       " 'part overall process',\n",
       " 'overall process experience',\n",
       " 'process experience accordingly',\n",
       " 'experience accordingly consumption',\n",
       " 'accordingly consumption everyone',\n",
       " 'consumption everyone data',\n",
       " 'everyone data scientist',\n",
       " 'data scientist want',\n",
       " 'scientist want need',\n",
       " 'want need know',\n",
       " 'need know model',\n",
       " 'know model design',\n",
       " 'model design statistical',\n",
       " 'design statistical theory',\n",
       " 'statistical theory training',\n",
       " 'theory training model',\n",
       " 'training model developer',\n",
       " 'model developer example',\n",
       " 'developer example may',\n",
       " 'example may varying',\n",
       " 'may varying level',\n",
       " 'varying level need',\n",
       " 'level need may',\n",
       " 'need may want',\n",
       " 'may want able',\n",
       " 'want able use',\n",
       " 'able use known',\n",
       " 'use known model',\n",
       " 'known model work',\n",
       " 'model work well',\n",
       " 'work well deploy',\n",
       " 'well deploy app',\n",
       " 'deploy app figure',\n",
       " 'app figure 1',\n",
       " 'figure 1 show',\n",
       " '1 show ibm',\n",
       " 'show ibm designed',\n",
       " 'ibm designed work',\n",
       " 'designed work space',\n",
       " 'work space allows',\n",
       " 'space allows application',\n",
       " 'allows application developer',\n",
       " 'application developer choose',\n",
       " 'developer choose deploy',\n",
       " 'choose deploy model',\n",
       " 'deploy model actually',\n",
       " 'model actually create',\n",
       " 'actually create pipeline',\n",
       " 'create pipeline via',\n",
       " 'pipeline via step',\n",
       " 'via step step',\n",
       " 'step step process',\n",
       " 'step process take',\n",
       " 'process take one',\n",
       " 'take one step',\n",
       " 'one step higher',\n",
       " 'step higher level',\n",
       " 'higher level developer',\n",
       " 'level developer may',\n",
       " 'developer may want',\n",
       " 'may want choose',\n",
       " 'want choose collection',\n",
       " 'choose collection pre',\n",
       " 'collection pre packaged',\n",
       " 'pre packaged machine',\n",
       " 'packaged machine learning',\n",
       " 'machine learning service',\n",
       " 'learning service fraud',\n",
       " 'service fraud detection',\n",
       " 'fraud detection weather',\n",
       " 'detection weather prediction',\n",
       " 'weather prediction manufacturing',\n",
       " 'prediction manufacturing model',\n",
       " 'manufacturing model sentiment',\n",
       " 'model sentiment analysis',\n",
       " 'sentiment analysis emotional',\n",
       " 'analysis emotional analysis',\n",
       " 'emotional analysis ibm',\n",
       " 'analysis ibm provides',\n",
       " 'ibm provides today',\n",
       " 'provides today bluemix',\n",
       " 'today bluemix service',\n",
       " 'bluemix service integrated',\n",
       " 'service integrated part',\n",
       " 'integrated part data',\n",
       " 'part data science',\n",
       " 'data science experience',\n",
       " 'science experience figure',\n",
       " 'experience figure 1',\n",
       " 'figure 1 integrated',\n",
       " '1 integrated workspace',\n",
       " 'integrated workspace creating',\n",
       " 'workspace creating pipeline',\n",
       " 'creating pipeline commoditizing',\n",
       " 'pipeline commoditizing automating',\n",
       " 'commoditizing automating machine',\n",
       " 'automating machine learning',\n",
       " 'machine learning machine',\n",
       " 'learning machine learning',\n",
       " 'machine learning enterprise',\n",
       " 'learning enterprise environment',\n",
       " 'enterprise environment challenging',\n",
       " 'environment challenging start',\n",
       " 'challenging start assumption',\n",
       " 'start assumption model',\n",
       " 'assumption model becomes',\n",
       " 'model becomes stale',\n",
       " 'becomes stale minute',\n",
       " 'stale minute stop',\n",
       " 'minute stop training',\n",
       " 'stop training time',\n",
       " 'training time accuracy',\n",
       " 'time accuracy model',\n",
       " 'accuracy model worsen',\n",
       " 'model worsen take',\n",
       " 'worsen take significant',\n",
       " 'take significant time',\n",
       " 'significant time understand',\n",
       " 'time understand happening',\n",
       " 'understand happening retrain',\n",
       " 'happening retrain existing',\n",
       " 'retrain existing model',\n",
       " 'existing model deploy',\n",
       " 'model deploy new',\n",
       " 'deploy new version',\n",
       " 'new version come',\n",
       " 'version come revenue',\n",
       " 'come revenue enterprise',\n",
       " 'revenue enterprise may',\n",
       " 'enterprise may hard',\n",
       " 'may hard time',\n",
       " 'hard time adopting',\n",
       " 'time adopting necessary',\n",
       " 'adopting necessary discipline',\n",
       " 'necessary discipline gauge',\n",
       " 'discipline gauge impact',\n",
       " 'gauge impact bottom',\n",
       " 'impact bottom line',\n",
       " 'bottom line lot',\n",
       " 'line lot machine',\n",
       " 'lot machine learning',\n",
       " 'machine learning use',\n",
       " 'learning use case',\n",
       " 'use case might',\n",
       " 'case might intuitive',\n",
       " 'might intuitive sense',\n",
       " 'intuitive sense set',\n",
       " 'sense set clear',\n",
       " 'set clear control',\n",
       " 'clear control point',\n",
       " 'control point flow',\n",
       " 'point flow people',\n",
       " 'flow people logically',\n",
       " 'people logically relate',\n",
       " 'logically relate term',\n",
       " 'relate term machine',\n",
       " 'term machine learning',\n",
       " 'machine learning may',\n",
       " 'learning may send',\n",
       " 'may send le',\n",
       " 'send le scientific',\n",
       " 'le scientific people',\n",
       " 'scientific people community',\n",
       " 'people community running',\n",
       " 'community running opposite',\n",
       " 'running opposite direction',\n",
       " 'opposite direction often',\n",
       " 'direction often data',\n",
       " 'often data scientist',\n",
       " 'data scientist perform',\n",
       " 'scientist perform number',\n",
       " 'perform number tedious',\n",
       " 'number tedious time',\n",
       " 'tedious time consuming',\n",
       " 'time consuming step',\n",
       " 'consuming step derive',\n",
       " 'step derive insight',\n",
       " 'derive insight raw',\n",
       " 'insight raw data',\n",
       " 'raw data set',\n",
       " 'data set process',\n",
       " 'set process involve',\n",
       " 'process involve data',\n",
       " 'involve data ingestion',\n",
       " 'data ingestion cleaning',\n",
       " 'ingestion cleaning transformation',\n",
       " 'cleaning transformation e',\n",
       " 'transformation e g',\n",
       " 'e g outlier',\n",
       " 'g outlier removal',\n",
       " 'outlier removal missing',\n",
       " 'removal missing value',\n",
       " 'missing value imputation',\n",
       " 'value imputation proceed',\n",
       " 'imputation proceed model',\n",
       " 'proceed model building',\n",
       " 'model building finally',\n",
       " 'building finally presentation',\n",
       " 'finally presentation prediction',\n",
       " 'presentation prediction align',\n",
       " 'prediction align end',\n",
       " 'align end user',\n",
       " 'end user objective',\n",
       " 'user objective preference',\n",
       " 'objective preference long',\n",
       " 'preference long complex',\n",
       " 'long complex sometimes',\n",
       " 'complex sometimes artful',\n",
       " 'sometimes artful process',\n",
       " 'artful process requiring',\n",
       " 'process requiring substantial',\n",
       " 'requiring substantial time',\n",
       " 'substantial time effort',\n",
       " 'time effort especially',\n",
       " 'effort especially combinatorial',\n",
       " 'especially combinatorial explosion',\n",
       " 'combinatorial explosion choice',\n",
       " 'explosion choice algorithm',\n",
       " 'choice algorithm platform',\n",
       " 'algorithm platform parameter',\n",
       " 'platform parameter composition',\n",
       " 'parameter composition tool',\n",
       " 'composition tool help',\n",
       " 'tool help automate',\n",
       " 'help automate step',\n",
       " 'automate step process',\n",
       " 'step process potential',\n",
       " 'process potential accelerate',\n",
       " 'potential accelerate time',\n",
       " 'accelerate time delivery',\n",
       " 'time delivery useful',\n",
       " 'delivery useful result',\n",
       " 'useful result expand',\n",
       " 'result expand reach',\n",
       " 'expand reach data',\n",
       " 'reach data science',\n",
       " 'data science non',\n",
       " 'science non expert',\n",
       " 'non expert offer',\n",
       " 'expert offer systematic',\n",
       " 'offer systematic exploration',\n",
       " 'systematic exploration available',\n",
       " 'exploration available option',\n",
       " 'available option cognitive',\n",
       " 'option cognitive automation',\n",
       " 'cognitive automation data',\n",
       " 'automation data science',\n",
       " 'data science cad',\n",
       " 'science cad help',\n",
       " 'cad help integrate',\n",
       " 'help integrate learning',\n",
       " 'integrate learning planning',\n",
       " 'learning planning composition',\n",
       " 'planning composition orchestration',\n",
       " 'composition orchestration technique',\n",
       " 'orchestration technique automatically',\n",
       " 'technique automatically efficiently',\n",
       " 'automatically efficiently build',\n",
       " 'efficiently build model',\n",
       " 'build model done',\n",
       " 'model done deploying',\n",
       " 'done deploying analytic',\n",
       " 'deploying analytic flow',\n",
       " 'analytic flow interactively',\n",
       " 'flow interactively support',\n",
       " 'interactively support would',\n",
       " 'support would data',\n",
       " 'would data scientist',\n",
       " 'data scientist task',\n",
       " 'scientist task cad',\n",
       " 'task cad also',\n",
       " 'cad also provides',\n",
       " 'also provides capability',\n",
       " 'provides capability run',\n",
       " 'capability run multiple',\n",
       " 'run multiple predefined',\n",
       " 'multiple predefined algorithm',\n",
       " 'predefined algorithm parallel',\n",
       " 'algorithm parallel identify',\n",
       " 'parallel identify best',\n",
       " 'identify best suitable',\n",
       " 'best suitable algorithm',\n",
       " 'suitable algorithm particular',\n",
       " 'algorithm particular use',\n",
       " 'particular use case',\n",
       " 'use case short',\n",
       " 'case short cad',\n",
       " 'short cad selects',\n",
       " 'cad selects best',\n",
       " 'selects best algorithm',\n",
       " 'best algorithm given',\n",
       " 'algorithm given use',\n",
       " 'given use case',\n",
       " 'use case click',\n",
       " 'case click read',\n",
       " 'click read ibm',\n",
       " 'read ibm paper',\n",
       " 'ibm paper cad',\n",
       " 'paper cad training',\n",
       " 'cad training tuning',\n",
       " 'training tuning model',\n",
       " 'tuning model optimization',\n",
       " 'model optimization time',\n",
       " 'optimization time model',\n",
       " 'time model algorithm',\n",
       " 'model algorithm used',\n",
       " 'algorithm used machine',\n",
       " 'used machine learning',\n",
       " 'machine learning becomes',\n",
       " 'learning becomes good',\n",
       " 'becomes good true',\n",
       " 'good true training',\n",
       " 'true training data',\n",
       " 'training data model',\n",
       " 'data model predicts',\n",
       " 'model predicts training',\n",
       " 'predicts training data',\n",
       " 'training data well',\n",
       " 'data well performs',\n",
       " 'well performs poorly',\n",
       " 'performs poorly new',\n",
       " 'poorly new data',\n",
       " 'new data known',\n",
       " 'data known overfitting',\n",
       " 'known overfitting extreme',\n",
       " 'overfitting extreme case',\n",
       " 'extreme case occur',\n",
       " 'case occur rote',\n",
       " 'occur rote learning',\n",
       " 'rote learning model',\n",
       " 'learning model achieves',\n",
       " 'model achieves 100',\n",
       " 'achieves 100 performance',\n",
       " '100 performance data',\n",
       " 'performance data already',\n",
       " 'data already seen',\n",
       " 'already seen probably',\n",
       " 'seen probably better',\n",
       " 'probably better random',\n",
       " 'better random guess',\n",
       " 'random guess new',\n",
       " 'guess new data',\n",
       " 'new data imagine',\n",
       " 'data imagine would',\n",
       " 'imagine would business',\n",
       " 'would business could',\n",
       " 'business could ruin',\n",
       " 'could ruin upset',\n",
       " 'ruin upset customer',\n",
       " 'upset customer set',\n",
       " 'customer set wrong',\n",
       " 'set wrong price',\n",
       " 'wrong price point',\n",
       " 'price point miss',\n",
       " 'point miss many',\n",
       " 'miss many business',\n",
       " 'many business opportunity',\n",
       " 'business opportunity ibm',\n",
       " 'opportunity ibm machine',\n",
       " 'ibm machine learning',\n",
       " 'machine learning help',\n",
       " 'learning help counter',\n",
       " 'help counter clean',\n",
       " 'counter clean separation',\n",
       " 'clean separation training',\n",
       " 'separation training data',\n",
       " 'training data holdout',\n",
       " 'data holdout data',\n",
       " 'holdout data used',\n",
       " 'data used evaluate',\n",
       " 'used evaluate model',\n",
       " 'evaluate model performance',\n",
       " 'model performance well',\n",
       " 'performance well careful',\n",
       " 'well careful use',\n",
       " 'careful use cross',\n",
       " 'use cross validation',\n",
       " 'cross validation technique',\n",
       " 'validation technique data',\n",
       " 'technique data sovereignty',\n",
       " 'data sovereignty isolation',\n",
       " 'sovereignty isolation enterprise',\n",
       " 'isolation enterprise organization',\n",
       " 'enterprise organization fear',\n",
       " 'organization fear psychological',\n",
       " 'fear psychological otherwise',\n",
       " 'psychological otherwise come',\n",
       " 'otherwise come putting',\n",
       " 'come putting data',\n",
       " 'putting data application',\n",
       " 'data application hardware',\n",
       " 'application hardware storage',\n",
       " 'hardware storage network',\n",
       " 'storage network infrastructure',\n",
       " 'network infrastructure shared',\n",
       " 'infrastructure shared organization',\n",
       " 'shared organization ibm',\n",
       " 'organization ibm cloud',\n",
       " 'ibm cloud first',\n",
       " 'cloud first strategy',\n",
       " 'first strategy provides',\n",
       " 'strategy provides necessary',\n",
       " 'provides necessary sovereignty',\n",
       " 'necessary sovereignty multi',\n",
       " 'sovereignty multi tenancy',\n",
       " 'multi tenancy isolation',\n",
       " 'tenancy isolation help',\n",
       " 'isolation help ensure',\n",
       " 'help ensure data',\n",
       " 'ensure data application',\n",
       " 'data application managed',\n",
       " 'application managed privately',\n",
       " 'managed privately across',\n",
       " 'privately across ibm',\n",
       " 'across ibm world',\n",
       " 'ibm world wide',\n",
       " 'world wide data',\n",
       " 'wide data center',\n",
       " 'data center variety',\n",
       " 'center variety type',\n",
       " 'variety type data',\n",
       " 'type data used',\n",
       " 'data used time',\n",
       " 'used time data',\n",
       " 'time data simpler',\n",
       " 'data simpler structured',\n",
       " 'simpler structured relational',\n",
       " 'structured relational hierarchical',\n",
       " 'relational hierarchical data',\n",
       " 'hierarchical data stored',\n",
       " 'data stored database',\n",
       " 'stored database big',\n",
       " 'database big data',\n",
       " 'big data simply',\n",
       " 'data simply mean',\n",
       " 'simply mean data',\n",
       " 'mean data includes',\n",
       " 'data includes volume',\n",
       " 'includes volume raw',\n",
       " 'volume raw content',\n",
       " 'raw content structured',\n",
       " 'content structured form',\n",
       " 'structured form part',\n",
       " 'form part unstructured',\n",
       " 'part unstructured add',\n",
       " 'unstructured add internet',\n",
       " 'add internet thing',\n",
       " 'internet thing sending',\n",
       " 'thing sending massive',\n",
       " 'sending massive amount',\n",
       " 'massive amount sensor',\n",
       " 'amount sensor data',\n",
       " 'sensor data world',\n",
       " 'data world simple',\n",
       " 'world simple ibm',\n",
       " 'simple ibm data',\n",
       " 'ibm data strategy',\n",
       " 'data strategy make',\n",
       " 'strategy make data',\n",
       " 'make data simple',\n",
       " 'data simple machine',\n",
       " 'simple machine learning',\n",
       " 'machine learning capability',\n",
       " 'learning capability leverage',\n",
       " 'capability leverage strategy',\n",
       " 'leverage strategy able',\n",
       " 'strategy able process',\n",
       " 'able process structured',\n",
       " 'process structured semi',\n",
       " 'structured semi structured',\n",
       " 'semi structured unstructured',\n",
       " 'structured unstructured data',\n",
       " 'unstructured data set',\n",
       " 'data set using',\n",
       " 'set using many',\n",
       " 'using many connector',\n",
       " 'many connector abstracting',\n",
       " 'connector abstracting complexity',\n",
       " 'abstracting complexity exploiting',\n",
       " 'complexity exploiting spark',\n",
       " 'exploiting spark r',\n",
       " 'spark r python',\n",
       " 'r python runtimes',\n",
       " 'python runtimes machine',\n",
       " 'runtimes machine learning',\n",
       " 'machine learning ibm',\n",
       " 'learning ibm provides',\n",
       " 'ibm provides 20',\n",
       " 'provides 20 different',\n",
       " '20 different data',\n",
       " 'different data source',\n",
       " 'data source connector',\n",
       " 'source connector organization',\n",
       " 'connector organization ingest',\n",
       " 'organization ingest data',\n",
       " 'ingest data large',\n",
       " 'data large compute',\n",
       " 'large compute power',\n",
       " 'compute power enterprise',\n",
       " 'power enterprise need',\n",
       " 'enterprise need high',\n",
       " 'need high compute',\n",
       " 'high compute power',\n",
       " 'compute power since',\n",
       " 'power since process',\n",
       " 'since process ever',\n",
       " 'process ever increasing',\n",
       " 'ever increasing work',\n",
       " 'increasing work load',\n",
       " 'work load data',\n",
       " 'load data transaction',\n",
       " 'data transaction process',\n",
       " 'transaction process since',\n",
       " 'process since spark',\n",
       " 'since spark service',\n",
       " 'spark service single',\n",
       " 'service single multi',\n",
       " 'single multi tenant',\n",
       " 'multi tenant cluster',\n",
       " 'tenant cluster resource',\n",
       " 'cluster resource utilization',\n",
       " 'resource utilization wasted',\n",
       " 'utilization wasted repurpose',\n",
       " 'wasted repurpose computer',\n",
       " 'repurpose computer power',\n",
       " 'computer power scaling',\n",
       " 'power scaling scaling',\n",
       " 'scaling scaling capability',\n",
       " 'scaling capability built',\n",
       " 'capability built part',\n",
       " 'built part service',\n",
       " 'part service machine',\n",
       " 'service machine learning',\n",
       " 'machine learning able',\n",
       " 'learning able transparently',\n",
       " 'able transparently enable',\n",
       " 'transparently enable scale',\n",
       " 'enable scale scale',\n",
       " 'scale scale capability',\n",
       " 'scale capability information',\n",
       " 'capability information governance',\n",
       " 'information governance ibm',\n",
       " 'governance ibm strong',\n",
       " 'ibm strong heritage',\n",
       " 'strong heritage information',\n",
       " 'heritage information governance',\n",
       " 'information governance managing',\n",
       " 'governance managing data',\n",
       " 'managing data lifecycle',\n",
       " 'data lifecycle cleansing',\n",
       " 'lifecycle cleansing quality',\n",
       " 'cleansing quality data',\n",
       " 'quality data data',\n",
       " 'data data wrangling',\n",
       " 'data wrangling shaping',\n",
       " 'wrangling shaping security',\n",
       " 'shaping security privacy',\n",
       " 'security privacy data',\n",
       " 'privacy data information',\n",
       " 'data information governance',\n",
       " 'information governance catalogue',\n",
       " 'governance catalogue us',\n",
       " 'catalogue us policy',\n",
       " 'us policy help',\n",
       " 'policy help ensure',\n",
       " 'help ensure right',\n",
       " 'ensure right people',\n",
       " 'right people see',\n",
       " 'people see access',\n",
       " 'see access execute',\n",
       " 'access execute data',\n",
       " 'execute data service',\n",
       " 'data service ibm',\n",
       " 'service ibm also',\n",
       " 'ibm also help',\n",
       " 'also help provide',\n",
       " 'help provide real',\n",
       " 'provide real time',\n",
       " 'real time monitoring',\n",
       " 'time monitoring threat',\n",
       " 'monitoring threat detection',\n",
       " 'threat detection prevention',\n",
       " 'detection prevention intervention',\n",
       " 'prevention intervention well',\n",
       " 'intervention well forensics',\n",
       " 'well forensics compliance',\n",
       " 'forensics compliance detailed',\n",
       " 'compliance detailed audit',\n",
       " 'detailed audit obfuscation',\n",
       " 'audit obfuscation masking',\n",
       " 'obfuscation masking data',\n",
       " 'masking data encryption',\n",
       " 'data encryption applied',\n",
       " 'encryption applied machine',\n",
       " 'applied machine learning',\n",
       " 'machine learning training',\n",
       " 'learning training data',\n",
       " 'training data machine',\n",
       " 'data machine learning',\n",
       " 'machine learning one',\n",
       " 'learning one trick',\n",
       " 'one trick pony',\n",
       " 'trick pony machine',\n",
       " 'pony machine learning',\n",
       " 'machine learning service',\n",
       " 'learning service mlaas',\n",
       " 'service mlaas many',\n",
       " 'mlaas many form',\n",
       " 'many form machine',\n",
       " 'form machine learning',\n",
       " 'machine learning system',\n",
       " 'learning system ml',\n",
       " 'system ml ibm',\n",
       " 'ml ibm donated',\n",
       " 'ibm donated apache',\n",
       " 'donated apache foundation',\n",
       " 'apache foundation natural',\n",
       " 'foundation natural language',\n",
       " 'natural language processing',\n",
       " 'language processing vision',\n",
       " 'processing vision personality',\n",
       " 'vision personality emotional',\n",
       " 'personality emotional insight',\n",
       " 'emotional insight customer',\n",
       " 'insight customer sentiment',\n",
       " 'customer sentiment retrieve',\n",
       " 'sentiment retrieve rank',\n",
       " 'retrieve rank ibm',\n",
       " 'rank ibm makeit',\n",
       " 'ibm makeit simple',\n",
       " 'makeit simple enterprise',\n",
       " 'simple enterprise size',\n",
       " 'enterprise size pick',\n",
       " 'size pick choose',\n",
       " 'pick choose number',\n",
       " 'choose number predefined',\n",
       " 'number predefined machine',\n",
       " 'predefined machine learning',\n",
       " 'machine learning service',\n",
       " 'learning service bluemix',\n",
       " 'service bluemix tile',\n",
       " 'bluemix tile figure',\n",
       " 'tile figure 2',\n",
       " 'figure 2 ran',\n",
       " '2 ran earlier',\n",
       " 'ran earlier blog',\n",
       " 'earlier blog personality',\n",
       " 'blog personality analyser',\n",
       " 'personality analyser knew',\n",
       " 'analyser knew really',\n",
       " 'knew really nice',\n",
       " 'really nice guy',\n",
       " 'nice guy reassuring',\n",
       " 'guy reassuring hear',\n",
       " 'reassuring hear machine',\n",
       " 'hear machine learning',\n",
       " 'machine learning service',\n",
       " 'learning service believe',\n",
       " 'service believe try',\n",
       " 'believe try figure',\n",
       " 'try figure 2',\n",
       " 'figure 2 machine',\n",
       " '2 machine learning',\n",
       " 'machine learning bluemix',\n",
       " 'learning bluemix service',\n",
       " 'bluemix service using',\n",
       " 'service using machine',\n",
       " 'using machine learning',\n",
       " 'machine learning reduce',\n",
       " 'learning reduce cost',\n",
       " 'reduce cost risk',\n",
       " 'cost risk many',\n",
       " 'risk many customer',\n",
       " 'many customer across',\n",
       " 'customer across different',\n",
       " 'across different industry',\n",
       " 'different industry used',\n",
       " 'industry used machine',\n",
       " 'used machine learning',\n",
       " 'machine learning capability',\n",
       " 'learning capability help',\n",
       " 'capability help reduce',\n",
       " 'help reduce cost',\n",
       " 'reduce cost improve',\n",
       " 'cost improve customer',\n",
       " 'improve customer service',\n",
       " 'customer service reduce',\n",
       " 'service reduce risk',\n",
       " 'reduce risk vermont',\n",
       " 'risk vermont electrical',\n",
       " 'vermont electrical power',\n",
       " 'electrical power company',\n",
       " 'power company velco',\n",
       " 'company velco worked',\n",
       " 'velco worked ibm',\n",
       " 'worked ibm research',\n",
       " 'ibm research develop',\n",
       " 'research develop integrated',\n",
       " 'develop integrated weather',\n",
       " 'integrated weather forecasting',\n",
       " 'weather forecasting system',\n",
       " 'forecasting system help',\n",
       " 'system help deliver',\n",
       " 'help deliver reliable',\n",
       " 'deliver reliable clean',\n",
       " 'reliable clean affordable',\n",
       " 'clean affordable power',\n",
       " 'affordable power consumer',\n",
       " 'power consumer integrating',\n",
       " 'consumer integrating renewable',\n",
       " 'integrating renewable energy',\n",
       " 'renewable energy grid',\n",
       " 'energy grid solution',\n",
       " 'grid solution combine',\n",
       " 'solution combine high',\n",
       " 'combine high resolution',\n",
       " 'high resolution weather',\n",
       " 'resolution weather multiple',\n",
       " 'weather multiple forecasting',\n",
       " 'multiple forecasting tool',\n",
       " 'forecasting tool based',\n",
       " 'tool based machine',\n",
       " 'based machine learning',\n",
       " 'machine learning machine',\n",
       " 'learning machine learning',\n",
       " 'machine learning model',\n",
       " 'learning model trained',\n",
       " 'model trained hindcasts',\n",
       " 'trained hindcasts weather',\n",
       " 'hindcasts weather correlated',\n",
       " 'weather correlated historical',\n",
       " 'correlated historical energy',\n",
       " 'historical energy production',\n",
       " 'energy production historical',\n",
       " 'production historical net',\n",
       " 'historical net demand',\n",
       " 'net demand result',\n",
       " 'demand result precise',\n",
       " 'result precise accurate',\n",
       " 'precise accurate wind',\n",
       " 'accurate wind solar',\n",
       " 'wind solar generation',\n",
       " 'solar generation forecast',\n",
       " ...]"
      ]
     },
     "execution_count": 1826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-gram\n",
    "\n",
    "list(ngrams(tokened_text, n=3))\n",
    "\n",
    "[f\"{x[0]} {x[1]} {x[2]}\" for x in list(ngrams(tokened_text, n=3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: get two grams\n",
    "\n",
    "def get_ngrams_2_by_article_id(id):\n",
    "    title = df_nlp.loc[id].doc_full_name\n",
    "    desc = df_nlp.loc[id].doc_description\n",
    "    body = df_nlp.loc[id].doc_body\n",
    "    \n",
    "    # a list of tokens\n",
    "    tokened_text = tokenize(title) + tokenize(desc) + tokenize(body)\n",
    "    \n",
    "    # ngrams 2\n",
    "    ngrams = [f\"{x[0]} {x[1]}\" for x in list(bigrams(tokened_text))]\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apache spark',\n",
       " 'spark sql',\n",
       " 'sql analyzer',\n",
       " 'analyzer resolve',\n",
       " 'resolve order',\n",
       " 'order column',\n",
       " 'column apache',\n",
       " 'apache spark',\n",
       " 'spark sql',\n",
       " 'sql component',\n",
       " 'component several',\n",
       " 'several sub',\n",
       " 'sub component',\n",
       " 'component including',\n",
       " 'including analyzer',\n",
       " 'analyzer play',\n",
       " 'play important',\n",
       " 'important role',\n",
       " 'role making',\n",
       " 'making sure',\n",
       " 'sure logical',\n",
       " 'logical plan',\n",
       " 'plan fully',\n",
       " 'fully resolved',\n",
       " 'resolved end',\n",
       " 'end analysis',\n",
       " 'analysis phase',\n",
       " 'phase analyzer',\n",
       " 'analyzer take',\n",
       " 'take parsed',\n",
       " 'parsed logical',\n",
       " 'logical plan',\n",
       " 'plan input',\n",
       " 'input make',\n",
       " 'make sure',\n",
       " 'sure table',\n",
       " 'table reference',\n",
       " 'reference attribute',\n",
       " 'attribute column',\n",
       " 'column reference',\n",
       " 'reference function',\n",
       " 'function reference',\n",
       " 'reference resolved',\n",
       " 'resolved looking',\n",
       " 'looking metadata',\n",
       " 'metadata catalog',\n",
       " 'catalog work',\n",
       " 'work applying',\n",
       " 'applying set',\n",
       " 'set rule',\n",
       " 'rule logical',\n",
       " 'logical plan',\n",
       " 'plan transforming',\n",
       " 'transforming stage',\n",
       " 'stage order',\n",
       " 'order resolve',\n",
       " 'resolve specific',\n",
       " 'specific portion',\n",
       " 'portion plan',\n",
       " 'plan home',\n",
       " 'home community',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'blog resource',\n",
       " 'resource code',\n",
       " 'code contribution',\n",
       " 'contribution university',\n",
       " 'university ibm',\n",
       " 'ibm design',\n",
       " 'design apache',\n",
       " 'apache systemml',\n",
       " 'systemml apache',\n",
       " 'apache spark',\n",
       " 'spark spark',\n",
       " 'spark tc',\n",
       " 'tc community',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'blog resource',\n",
       " 'resource code',\n",
       " 'code contribution',\n",
       " 'contribution university',\n",
       " 'university ibm',\n",
       " 'ibm design',\n",
       " 'design apache',\n",
       " 'apache systemml',\n",
       " 'systemml apache',\n",
       " 'apache spark',\n",
       " 'spark spark',\n",
       " 'spark sql',\n",
       " 'sql apache',\n",
       " 'apache spark',\n",
       " 'spark sql',\n",
       " 'sql analyzer',\n",
       " 'analyzer resolve',\n",
       " 'resolve order',\n",
       " 'order column',\n",
       " 'column apache',\n",
       " 'apache spark',\n",
       " 'spark sql',\n",
       " 'sql component',\n",
       " 'component several',\n",
       " 'several sub',\n",
       " 'sub component',\n",
       " 'component including',\n",
       " 'including analyzer',\n",
       " 'analyzer play',\n",
       " 'play important',\n",
       " 'important role',\n",
       " 'role making',\n",
       " 'making sure',\n",
       " 'sure logical',\n",
       " 'logical plan',\n",
       " 'plan fully',\n",
       " 'fully resolved',\n",
       " 'resolved end',\n",
       " 'end analysis',\n",
       " 'analysis phase',\n",
       " 'phase analyzer',\n",
       " 'analyzer take',\n",
       " 'take parsed',\n",
       " 'parsed logical',\n",
       " 'logical plan',\n",
       " 'plan input',\n",
       " 'input make',\n",
       " 'make sure',\n",
       " 'sure table',\n",
       " 'table reference',\n",
       " 'reference attribute',\n",
       " 'attribute column',\n",
       " 'column reference',\n",
       " 'reference function',\n",
       " 'function reference',\n",
       " 'reference resolved',\n",
       " 'resolved looking',\n",
       " 'looking metadata',\n",
       " 'metadata catalog',\n",
       " 'catalog work',\n",
       " 'work applying',\n",
       " 'applying set',\n",
       " 'set rule',\n",
       " 'rule logical',\n",
       " 'logical plan',\n",
       " 'plan transforming',\n",
       " 'transforming stage',\n",
       " 'stage order',\n",
       " 'order resolve',\n",
       " 'resolve specific',\n",
       " 'specific portion',\n",
       " 'portion plan',\n",
       " 'plan examine',\n",
       " 'examine working',\n",
       " 'working analyzer',\n",
       " 'analyzer taking',\n",
       " 'taking example',\n",
       " 'example defect',\n",
       " 'defect describing',\n",
       " 'describing addressed',\n",
       " 'addressed problem',\n",
       " 'problem example',\n",
       " 'example query',\n",
       " 'query select',\n",
       " 'select a1',\n",
       " 'a1 c',\n",
       " 'c a2',\n",
       " 'a2 count',\n",
       " 'count c',\n",
       " 'c a3',\n",
       " 'a3 tab',\n",
       " 'tab group',\n",
       " 'group b',\n",
       " 'b order',\n",
       " 'order a1',\n",
       " 'a1 c',\n",
       " 'c problem',\n",
       " 'problem description',\n",
       " 'description case',\n",
       " 'case analyzer',\n",
       " 'analyzer unable',\n",
       " 'unable resolve',\n",
       " 'resolve attribute',\n",
       " 'attribute referenced',\n",
       " 'referenced order',\n",
       " 'order clause',\n",
       " 'clause see',\n",
       " 'see let',\n",
       " 'let look',\n",
       " 'look underlying',\n",
       " 'underlying parsed',\n",
       " 'parsed logical',\n",
       " 'logical plan',\n",
       " 'plan parsed',\n",
       " 'parsed logical',\n",
       " 'logical plan',\n",
       " 'plan sort',\n",
       " 'sort a1',\n",
       " 'a1 asc',\n",
       " 'asc c',\n",
       " 'c asc',\n",
       " 'asc true',\n",
       " 'true aggregate',\n",
       " 'aggregate c',\n",
       " 'c a1',\n",
       " 'a1 17',\n",
       " '17 c',\n",
       " 'c a2',\n",
       " 'a2 18',\n",
       " '18 count',\n",
       " 'count mode',\n",
       " 'mode complete',\n",
       " 'complete isdistinct',\n",
       " 'isdistinct false',\n",
       " 'false a3',\n",
       " 'a3 19',\n",
       " '19 localrelation',\n",
       " 'localrelation 1',\n",
       " '1 b',\n",
       " 'b 2',\n",
       " '2 c',\n",
       " 'c 3',\n",
       " '3 4',\n",
       " '4 e',\n",
       " 'e 5',\n",
       " '5 case',\n",
       " 'case localrelation',\n",
       " 'localrelation resolved',\n",
       " 'resolved none',\n",
       " 'none plan',\n",
       " 'plan operator',\n",
       " 'operator resolved',\n",
       " 'resolved since',\n",
       " 'since underlying',\n",
       " 'underlying attribute',\n",
       " 'attribute refer',\n",
       " 'refer resolved',\n",
       " 'resolved however',\n",
       " 'however see',\n",
       " 'see sort',\n",
       " 'sort operator',\n",
       " 'operator aggregate',\n",
       " 'aggregate operator',\n",
       " 'operator attribute',\n",
       " 'attribute referenced',\n",
       " 'referenced sort',\n",
       " 'sort operator',\n",
       " 'operator resolved',\n",
       " 'resolved output',\n",
       " 'output child',\n",
       " 'child aggregate',\n",
       " 'aggregate operator',\n",
       " 'operator output',\n",
       " 'output aggregate',\n",
       " 'aggregate operator',\n",
       " 'operator a1',\n",
       " 'a1 17',\n",
       " '17 a2',\n",
       " 'a2 18',\n",
       " '18 a3',\n",
       " 'a3 19',\n",
       " '19 plan',\n",
       " 'plan missing',\n",
       " 'missing attribute',\n",
       " 'attribute c',\n",
       " 'c 3',\n",
       " '3 referenced',\n",
       " 'referenced sort',\n",
       " 'sort operator',\n",
       " 'operator cause',\n",
       " 'cause failure',\n",
       " 'failure analysis',\n",
       " 'analysis process',\n",
       " 'process turn',\n",
       " 'turn result',\n",
       " 'result query',\n",
       " 'query failure',\n",
       " 'failure order',\n",
       " 'order properly',\n",
       " 'properly resolve',\n",
       " 'resolve sort',\n",
       " 'sort operator',\n",
       " 'operator need',\n",
       " 'need make',\n",
       " 'make sure',\n",
       " 'sure a1',\n",
       " 'a1 sort',\n",
       " 'sort resolved',\n",
       " 'resolved immediate',\n",
       " 'immediate child',\n",
       " 'child aggregate',\n",
       " 'aggregate c',\n",
       " 'c sort',\n",
       " 'sort resolved',\n",
       " 'resolved grandchild',\n",
       " 'grandchild local',\n",
       " 'local relation',\n",
       " 'relation spark',\n",
       " 'spark analyzer',\n",
       " 'analyzer resolveaggregatefunctions',\n",
       " 'resolveaggregatefunctions rule',\n",
       " 'rule modified',\n",
       " 'modified order',\n",
       " 'order properly',\n",
       " 'properly resolve',\n",
       " 'resolve sort',\n",
       " 'sort operator',\n",
       " 'operator query',\n",
       " 'query result',\n",
       " 'result following',\n",
       " 'following analyzed',\n",
       " 'analyzed logical',\n",
       " 'logical plan',\n",
       " 'plan fix',\n",
       " 'fix project',\n",
       " 'project a1',\n",
       " 'a1 14',\n",
       " '14 a2',\n",
       " 'a2 15',\n",
       " '15 a3',\n",
       " 'a3 16l',\n",
       " '16l sort',\n",
       " 'sort a1',\n",
       " 'a1 14',\n",
       " '14 asc',\n",
       " 'asc a2',\n",
       " 'a2 15',\n",
       " '15 asc',\n",
       " 'asc true',\n",
       " 'true aggregate',\n",
       " 'aggregate 1',\n",
       " '1 c',\n",
       " 'c 3',\n",
       " '3 1',\n",
       " '1 a1',\n",
       " 'a1 14',\n",
       " '14 c',\n",
       " 'c 3',\n",
       " '3 a2',\n",
       " 'a2 15',\n",
       " '15 count',\n",
       " 'count 1',\n",
       " '1 mode',\n",
       " 'mode complete',\n",
       " 'complete isdistinct',\n",
       " 'isdistinct false',\n",
       " 'false a3',\n",
       " 'a3 16l',\n",
       " '16l localrelation',\n",
       " 'localrelation 1',\n",
       " '1 b',\n",
       " 'b 2',\n",
       " '2 c',\n",
       " 'c 3',\n",
       " '3 4',\n",
       " '4 e',\n",
       " 'e 5',\n",
       " '5 conclusion',\n",
       " 'conclusion hopefully',\n",
       " 'hopefully blog',\n",
       " 'blog give',\n",
       " 'give brief',\n",
       " 'brief insight',\n",
       " 'insight working',\n",
       " 'working analyzer',\n",
       " 'analyzer post',\n",
       " 'post extended',\n",
       " 'extended description',\n",
       " 'description analyzer',\n",
       " 'analyzer future',\n",
       " 'future general',\n",
       " 'general handling',\n",
       " 'handling analyzer',\n",
       " 'analyzer issue',\n",
       " 'issue requires',\n",
       " 'requires deep',\n",
       " 'deep understanding',\n",
       " 'understanding spark',\n",
       " 'spark logical',\n",
       " 'logical plan',\n",
       " 'plan author',\n",
       " 'author dilip',\n",
       " 'dilip biswal',\n",
       " 'biswal senior',\n",
       " 'senior software',\n",
       " 'software engineer',\n",
       " 'engineer spark',\n",
       " 'spark technology',\n",
       " 'technology center',\n",
       " 'center ibm',\n",
       " 'ibm active',\n",
       " 'active apache',\n",
       " 'apache spark',\n",
       " 'spark contributor',\n",
       " 'contributor work',\n",
       " 'work open',\n",
       " 'open source',\n",
       " 'source community',\n",
       " 'community experienced',\n",
       " 'experienced relational',\n",
       " 'relational database',\n",
       " 'database distributed',\n",
       " 'distributed computing',\n",
       " 'computing big',\n",
       " 'big data',\n",
       " 'data analytics',\n",
       " 'analytics extensively',\n",
       " 'extensively worked',\n",
       " 'worked sql',\n",
       " 'sql engine',\n",
       " 'engine like',\n",
       " 'like informix',\n",
       " 'informix derby',\n",
       " 'derby big',\n",
       " 'big sql',\n",
       " 'sql share',\n",
       " 'share share',\n",
       " 'share dilip',\n",
       " 'dilip biswal',\n",
       " 'biswal date',\n",
       " 'date 05',\n",
       " '05 june',\n",
       " 'june 2016tags',\n",
       " '2016tags spark',\n",
       " 'spark sql',\n",
       " 'sql apache',\n",
       " 'apache sparkspark',\n",
       " 'sparkspark technology',\n",
       " 'technology center',\n",
       " 'center community',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'blog apache',\n",
       " 'apache software',\n",
       " 'software foundation',\n",
       " 'foundation affiliation',\n",
       " 'affiliation endorse',\n",
       " 'endorse review',\n",
       " 'review material',\n",
       " 'material provided',\n",
       " 'provided website',\n",
       " 'website managed',\n",
       " 'managed ibm',\n",
       " 'ibm apache',\n",
       " 'apache apache',\n",
       " 'apache spark',\n",
       " 'spark spark',\n",
       " 'spark trademark',\n",
       " 'trademark apache',\n",
       " 'apache software',\n",
       " 'software foundation',\n",
       " 'foundation united',\n",
       " 'united state',\n",
       " 'state country']"
      ]
     },
     "execution_count": 1844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_ngrams_2_by_article_id(949)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apache spark',\n",
       " 'spark analytics',\n",
       " 'analytics combine',\n",
       " 'combine apache',\n",
       " 'apache spark',\n",
       " 'spark cloud',\n",
       " 'cloud service',\n",
       " 'service speed',\n",
       " 'speed analysis',\n",
       " 'analysis reveal',\n",
       " 'reveal insight',\n",
       " 'insight apache',\n",
       " 'apache spark',\n",
       " 'spark analyticscombine',\n",
       " 'analyticscombine apache',\n",
       " 'apache spark',\n",
       " 'spark cloud',\n",
       " 'cloud service',\n",
       " 'service speed',\n",
       " 'speed analysis',\n",
       " 'analysis revealinsights',\n",
       " 'revealinsights whether',\n",
       " 'whether want',\n",
       " 'want track',\n",
       " 'track conversation',\n",
       " 'conversation trending',\n",
       " 'trending twitter',\n",
       " 'twitter predictfuture',\n",
       " 'predictfuture event',\n",
       " 'event based',\n",
       " 'based trove',\n",
       " 'trove existing',\n",
       " 'existing data',\n",
       " 'data lighting',\n",
       " 'lighting fast',\n",
       " 'fast processingpower',\n",
       " 'processingpower apache',\n",
       " 'apache spark',\n",
       " 'spark help',\n",
       " 'help crunch',\n",
       " 'crunch large',\n",
       " 'large data',\n",
       " 'data set',\n",
       " 'set fast',\n",
       " 'fast show',\n",
       " 'show youhow',\n",
       " 'youhow harness',\n",
       " 'harness power',\n",
       " 'power potential',\n",
       " 'potential apache',\n",
       " 'apache spark',\n",
       " 'spark engine',\n",
       " 'engine andprogramming',\n",
       " 'andprogramming model',\n",
       " 'model spark',\n",
       " 'spark apache',\n",
       " 'apache spark',\n",
       " 'spark open',\n",
       " 'open source',\n",
       " 'source cluster',\n",
       " 'cluster computing',\n",
       " 'computing framework',\n",
       " 'framework memoryprocessing',\n",
       " 'memoryprocessing run',\n",
       " 'run 100',\n",
       " '100 time',\n",
       " 'time faster',\n",
       " 'faster technology',\n",
       " 'technology themarket',\n",
       " 'themarket today',\n",
       " 'today one',\n",
       " 'one nicest',\n",
       " 'nicest thing',\n",
       " 'thing technology',\n",
       " 'technology featuresa',\n",
       " 'featuresa simple',\n",
       " 'simple programming',\n",
       " 'programming model',\n",
       " 'model hide',\n",
       " 'hide complexity',\n",
       " 'complexity inherent',\n",
       " 'inherent distributedcomputing',\n",
       " 'distributedcomputing added',\n",
       " 'added bonus',\n",
       " 'bonus apis',\n",
       " 'apis come',\n",
       " 'come multiple',\n",
       " 'multiple flavor',\n",
       " 'flavor scala',\n",
       " 'scala java',\n",
       " 'java python',\n",
       " 'python r',\n",
       " 'r integrated',\n",
       " 'integrated servicesanalytics',\n",
       " 'servicesanalytics apache',\n",
       " 'apache spark',\n",
       " 'spark integrates',\n",
       " 'integrates swift',\n",
       " 'swift object',\n",
       " 'object storage',\n",
       " 'storage cloudant',\n",
       " 'cloudant dashdb',\n",
       " 'dashdb sqldb',\n",
       " 'sqldb watson',\n",
       " 'watson ibm',\n",
       " 'ibm cloud',\n",
       " 'cloud service',\n",
       " 'service service',\n",
       " 'service playwell',\n",
       " 'playwell together',\n",
       " 'together single',\n",
       " 'single cloud',\n",
       " 'cloud platform',\n",
       " 'platform eas',\n",
       " 'eas development',\n",
       " 'development let',\n",
       " 'let youbring',\n",
       " 'youbring creativity',\n",
       " 'creativity breadth',\n",
       " 'breadth analysis',\n",
       " 'analysis solution',\n",
       " 'solution combined',\n",
       " 'combined service',\n",
       " 'service actionthis',\n",
       " 'actionthis video',\n",
       " 'video demo',\n",
       " 'demo show',\n",
       " 'show set',\n",
       " 'set apache',\n",
       " 'apache spark',\n",
       " 'spark work',\n",
       " 'work watson',\n",
       " 'watson toneanalyzer',\n",
       " 'toneanalyzer gauge',\n",
       " 'gauge social',\n",
       " 'social emotional',\n",
       " 'emotional tone',\n",
       " 'tone set',\n",
       " 'set tweet',\n",
       " 'tweet dive',\n",
       " 'dive check',\n",
       " 'check growing',\n",
       " 'growing list',\n",
       " 'list tutorial',\n",
       " 'tutorial top',\n",
       " 'top right',\n",
       " 'right page',\n",
       " 'page choose',\n",
       " 'choose one',\n",
       " 'one interest',\n",
       " 'interest tutorial',\n",
       " 'tutorial start',\n",
       " 'start developing',\n",
       " 'developing spark',\n",
       " 'spark notebook',\n",
       " 'notebook sentiment',\n",
       " 'sentiment analysis',\n",
       " 'analysis twitter',\n",
       " 'twitter hashtags',\n",
       " 'hashtags realtime',\n",
       " 'realtime sentiment',\n",
       " 'sentiment analysis',\n",
       " 'analysis twitter',\n",
       " 'twitter hashtags',\n",
       " 'hashtags sentiment',\n",
       " 'sentiment analysis',\n",
       " 'analysis reddit',\n",
       " 'reddit amas',\n",
       " 'amas speed',\n",
       " 'speed sql',\n",
       " 'sql query',\n",
       " 'query spark',\n",
       " 'spark sql',\n",
       " 'sql ibm',\n",
       " 'ibm technology',\n",
       " 'technology bluemix',\n",
       " 'bluemix analytics',\n",
       " 'analytics apache',\n",
       " 'apache spark',\n",
       " 'spark spark',\n",
       " 'spark cloudant',\n",
       " 'cloudant connector',\n",
       " 'connector watson',\n",
       " 'watson tone',\n",
       " 'tone analyzer',\n",
       " 'analyzer message',\n",
       " 'message hub',\n",
       " 'hub apache',\n",
       " 'apache kafka',\n",
       " 'kafka message',\n",
       " 'message connect',\n",
       " 'connect event',\n",
       " 'event stream',\n",
       " 'stream blog',\n",
       " 'blog n',\n",
       " 'n stuff',\n",
       " 'stuff spark',\n",
       " 'spark learning',\n",
       " 'learning center',\n",
       " 'center spark',\n",
       " 'spark article',\n",
       " 'article journey',\n",
       " 'journey space',\n",
       " 'space seti',\n",
       " 'seti ibm',\n",
       " 'ibm analytics',\n",
       " 'analytics apache',\n",
       " 'apache spark',\n",
       " 'spark status',\n",
       " 'status update',\n",
       " 'update seti',\n",
       " 'seti institute',\n",
       " 'institute apache',\n",
       " 'apache spark',\n",
       " 'spark apache',\n",
       " 'apache spark',\n",
       " 'spark trademark',\n",
       " 'trademark registered',\n",
       " 'registered trademarksof',\n",
       " 'trademarksof apache',\n",
       " 'apache software',\n",
       " 'software foundation',\n",
       " 'foundation brand',\n",
       " 'brand trademark',\n",
       " 'trademark theproperty',\n",
       " 'theproperty respective',\n",
       " 'respective owner',\n",
       " 'owner share',\n",
       " 'share click',\n",
       " 'click email',\n",
       " 'email friend',\n",
       " 'friend open',\n",
       " 'open new',\n",
       " 'new window',\n",
       " 'window click',\n",
       " 'click share',\n",
       " 'share twitter',\n",
       " 'twitter open',\n",
       " 'open new',\n",
       " 'new window',\n",
       " 'window click',\n",
       " 'click share',\n",
       " 'share linkedin',\n",
       " 'linkedin open',\n",
       " 'open new',\n",
       " 'new window',\n",
       " 'window share',\n",
       " 'share facebook',\n",
       " 'facebook open',\n",
       " 'open new',\n",
       " 'new window',\n",
       " 'window click',\n",
       " 'click share',\n",
       " 'share reddit',\n",
       " 'reddit open',\n",
       " 'open new',\n",
       " 'new window',\n",
       " 'window click',\n",
       " 'click share',\n",
       " 'share pocket',\n",
       " 'pocket open',\n",
       " 'open new',\n",
       " 'new window',\n",
       " 'window r',\n",
       " 'r feed',\n",
       " 'feed report',\n",
       " 'report abuse',\n",
       " 'abuse term',\n",
       " 'term use',\n",
       " 'use third',\n",
       " 'third party',\n",
       " 'party notice',\n",
       " 'notice ibm',\n",
       " 'ibm privacyibm',\n",
       " 'privacyibm send',\n",
       " 'send email',\n",
       " 'email address',\n",
       " 'address name',\n",
       " 'name email',\n",
       " 'email address',\n",
       " 'address cancel',\n",
       " 'cancel post',\n",
       " 'post sent',\n",
       " 'sent check',\n",
       " 'check email',\n",
       " 'email address',\n",
       " 'address email',\n",
       " 'email check',\n",
       " 'check failed',\n",
       " 'failed please',\n",
       " 'please try',\n",
       " 'try sorry',\n",
       " 'sorry blog',\n",
       " 'blog share',\n",
       " 'share post',\n",
       " 'post email']"
      ]
     },
     "execution_count": 1845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = get_ngrams_2_by_article_id(592)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apache spark',\n",
       " 'spark 2',\n",
       " '2 0',\n",
       " '0 extend',\n",
       " 'extend structured',\n",
       " 'structured streaming',\n",
       " 'streaming spark',\n",
       " 'spark ml',\n",
       " 'ml early',\n",
       " 'early method',\n",
       " 'method integrate',\n",
       " 'integrate machine',\n",
       " 'machine learning',\n",
       " 'learning using',\n",
       " 'using naive',\n",
       " 'naive bayes',\n",
       " 'bayes custom',\n",
       " 'custom sink',\n",
       " 'sink home',\n",
       " 'home community',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'blog advisory',\n",
       " 'advisory council',\n",
       " 'council resource',\n",
       " 'resource code',\n",
       " 'code contribution',\n",
       " 'contribution university',\n",
       " 'university ibm',\n",
       " 'ibm design',\n",
       " 'design apache',\n",
       " 'apache systemml',\n",
       " 'systemml apache',\n",
       " 'apache spark',\n",
       " 'spark spark',\n",
       " 'spark tc',\n",
       " 'tc community',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'blog advisory',\n",
       " 'advisory council',\n",
       " 'council resource',\n",
       " 'resource code',\n",
       " 'code contribution',\n",
       " 'contribution university',\n",
       " 'university ibm',\n",
       " 'ibm design',\n",
       " 'design apache',\n",
       " 'apache systemml',\n",
       " 'systemml apache',\n",
       " 'apache spark',\n",
       " 'spark streaming',\n",
       " 'streaming extend',\n",
       " 'extend structured',\n",
       " 'structured streaming',\n",
       " 'streaming spark',\n",
       " 'spark ml',\n",
       " 'ml early',\n",
       " 'early method',\n",
       " 'method integrate',\n",
       " 'integrate machine',\n",
       " 'machine learning',\n",
       " 'learning using',\n",
       " 'using naive',\n",
       " 'naive bayes',\n",
       " 'bayes custom',\n",
       " 'custom sink',\n",
       " 'sink learn',\n",
       " 'learn structured',\n",
       " 'structured streaming',\n",
       " 'streaming machine',\n",
       " 'machine learning',\n",
       " 'learning check',\n",
       " 'check holden',\n",
       " 'holden karau',\n",
       " 'karau seth',\n",
       " 'seth hendrickson',\n",
       " 'hendrickson session',\n",
       " 'session spark',\n",
       " 'spark structured',\n",
       " 'structured streaming',\n",
       " 'streaming machine',\n",
       " 'machine learning',\n",
       " 'learning stratum',\n",
       " 'stratum hadoop',\n",
       " 'hadoop world',\n",
       " 'world new',\n",
       " 'new york',\n",
       " 'york 2',\n",
       " '2 05pm',\n",
       " '05pm 2',\n",
       " '2 45pm',\n",
       " '45pm thursday',\n",
       " 'thursday september',\n",
       " 'september 29th',\n",
       " '29th spark',\n",
       " 'spark new',\n",
       " 'new alpha',\n",
       " 'alpha structured',\n",
       " 'structured streaming',\n",
       " 'streaming api',\n",
       " 'api caused',\n",
       " 'caused lot',\n",
       " 'lot excitement',\n",
       " 'excitement brings',\n",
       " 'brings data',\n",
       " 'data set',\n",
       " 'set dataframe',\n",
       " 'dataframe sql',\n",
       " 'sql apis',\n",
       " 'apis streaming',\n",
       " 'streaming context',\n",
       " 'context initial',\n",
       " 'initial version',\n",
       " 'version structured',\n",
       " 'structured streaming',\n",
       " 'streaming machine',\n",
       " 'machine learning',\n",
       " 'learning apis',\n",
       " 'apis yet',\n",
       " 'yet integrated',\n",
       " 'integrated however',\n",
       " 'however stop',\n",
       " 'stop u',\n",
       " 'u fun',\n",
       " 'fun exploring',\n",
       " 'exploring get',\n",
       " 'get machine',\n",
       " 'machine learning',\n",
       " 'learning work',\n",
       " 'work structured',\n",
       " 'structured streaming',\n",
       " 'streaming simply',\n",
       " 'simply keep',\n",
       " 'keep mind',\n",
       " 'mind exploratory',\n",
       " 'exploratory thing',\n",
       " 'thing change',\n",
       " 'change future',\n",
       " 'future version',\n",
       " 'version spark',\n",
       " 'spark structured',\n",
       " 'structured streaming',\n",
       " 'streaming machine',\n",
       " 'machine learning',\n",
       " 'learning talk',\n",
       " 'talk stratum',\n",
       " 'stratum hadoop',\n",
       " 'hadoop world',\n",
       " 'world new',\n",
       " 'new york',\n",
       " 'york 2016',\n",
       " '2016 started',\n",
       " 'started early',\n",
       " 'early proof',\n",
       " 'proof concept',\n",
       " 'concept work',\n",
       " 'work integrate',\n",
       " 'integrate structured',\n",
       " 'structured streaming',\n",
       " 'streaming machine',\n",
       " 'machine learning',\n",
       " 'learning available',\n",
       " 'available spark',\n",
       " 'spark structured',\n",
       " 'structured streaming',\n",
       " 'streaming ml',\n",
       " 'ml repo',\n",
       " 'repo interested',\n",
       " 'interested following',\n",
       " 'following along',\n",
       " 'along progress',\n",
       " 'progress toward',\n",
       " 'toward spark',\n",
       " 'spark ml',\n",
       " 'ml pipeline',\n",
       " 'pipeline supporting',\n",
       " 'supporting structured',\n",
       " 'structured streaming',\n",
       " 'streaming encourage',\n",
       " 'encourage follow',\n",
       " 'follow spark',\n",
       " 'spark 16424',\n",
       " '16424 give',\n",
       " 'give u',\n",
       " 'u feedback',\n",
       " 'feedback early',\n",
       " 'early draft',\n",
       " 'draft design',\n",
       " 'design document',\n",
       " 'document one',\n",
       " 'one simplest',\n",
       " 'simplest streaming',\n",
       " 'streaming machine',\n",
       " 'machine learning',\n",
       " 'learning algorithm',\n",
       " 'algorithm implement',\n",
       " 'implement top',\n",
       " 'top structured',\n",
       " 'structured streaming',\n",
       " 'streaming naive',\n",
       " 'naive bayes',\n",
       " 'bayes since',\n",
       " 'since much',\n",
       " 'much computation',\n",
       " 'computation simplified',\n",
       " 'simplified grouping',\n",
       " 'grouping aggregating',\n",
       " 'aggregating challenge',\n",
       " 'challenge collect',\n",
       " 'collect aggregate',\n",
       " 'aggregate data',\n",
       " 'data way',\n",
       " 'way use',\n",
       " 'use make',\n",
       " 'make prediction',\n",
       " 'prediction approach',\n",
       " 'approach taken',\n",
       " 'taken current',\n",
       " 'current streaming',\n",
       " 'streaming naive',\n",
       " 'naive bayes',\n",
       " 'bayes directly',\n",
       " 'directly work',\n",
       " 'work foreachsink',\n",
       " 'foreachsink available',\n",
       " 'available spark',\n",
       " 'spark structured',\n",
       " 'structured streaming',\n",
       " 'streaming executes',\n",
       " 'executes action',\n",
       " 'action worker',\n",
       " 'worker update',\n",
       " 'update local',\n",
       " 'local data',\n",
       " 'data structure',\n",
       " 'structure latest',\n",
       " 'latest count',\n",
       " 'count instead',\n",
       " 'instead spark',\n",
       " 'spark structured',\n",
       " 'structured streaming',\n",
       " 'streaming memory',\n",
       " 'memory table',\n",
       " 'table output',\n",
       " 'output format',\n",
       " 'format use',\n",
       " 'use store',\n",
       " 'store aggregate',\n",
       " 'aggregate count',\n",
       " 'count compute',\n",
       " 'compute count',\n",
       " 'count using',\n",
       " 'using dataset',\n",
       " 'dataset transformation',\n",
       " 'transformation val',\n",
       " 'val count',\n",
       " 'count d',\n",
       " 'd flatmap',\n",
       " 'flatmap case',\n",
       " 'case labeledpoint',\n",
       " 'labeledpoint label',\n",
       " 'label vec',\n",
       " 'vec vec',\n",
       " 'vec toarray',\n",
       " 'toarray zip',\n",
       " 'zip stream',\n",
       " 'stream 1',\n",
       " '1 map',\n",
       " 'map value',\n",
       " 'value labeledtoken',\n",
       " 'labeledtoken label',\n",
       " 'label value',\n",
       " 'value groupby',\n",
       " 'groupby label',\n",
       " 'label value',\n",
       " 'value agg',\n",
       " 'agg count',\n",
       " 'count value',\n",
       " 'value alias',\n",
       " 'alias count',\n",
       " 'count labeledtokencounts',\n",
       " 'labeledtokencounts create',\n",
       " 'create table',\n",
       " 'table name',\n",
       " 'name store',\n",
       " 'store output',\n",
       " 'output val',\n",
       " 'val tblname',\n",
       " 'tblname qbsnb',\n",
       " 'qbsnb java',\n",
       " 'java util',\n",
       " 'util uuid',\n",
       " 'uuid randomuuid',\n",
       " 'randomuuid tostring',\n",
       " 'tostring filter',\n",
       " 'filter tostring',\n",
       " 'tostring write',\n",
       " 'write aggregate',\n",
       " 'aggregate result',\n",
       " 'result complete',\n",
       " 'complete form',\n",
       " 'form memory',\n",
       " 'memory table',\n",
       " 'table val',\n",
       " 'val query',\n",
       " 'query count',\n",
       " 'count writestream',\n",
       " 'writestream outputmode',\n",
       " 'outputmode outputmode',\n",
       " 'outputmode complete',\n",
       " 'complete format',\n",
       " 'format memory',\n",
       " 'memory queryname',\n",
       " 'queryname tblname',\n",
       " 'tblname start',\n",
       " 'start val',\n",
       " 'val tbl',\n",
       " 'tbl d',\n",
       " 'd sparksession',\n",
       " 'sparksession table',\n",
       " 'table tblname',\n",
       " 'tblname labeledtokencounts',\n",
       " 'labeledtokencounts initial',\n",
       " 'initial approach',\n",
       " 'approach taken',\n",
       " 'taken naive',\n",
       " 'naive bayes',\n",
       " 'bayes easily',\n",
       " 'easily generalizable',\n",
       " 'generalizable algorithm',\n",
       " 'algorithm easily',\n",
       " 'easily represented',\n",
       " 'represented aggregate',\n",
       " 'aggregate operation',\n",
       " 'operation dataset',\n",
       " 'dataset looking',\n",
       " 'looking back',\n",
       " 'back early',\n",
       " 'early dstream',\n",
       " 'dstream based',\n",
       " 'based spark',\n",
       " 'spark streaming',\n",
       " 'streaming api',\n",
       " 'api implemented',\n",
       " 'implemented machine',\n",
       " 'machine learning',\n",
       " 'learning provide',\n",
       " 'provide hint',\n",
       " 'hint one',\n",
       " 'one possible',\n",
       " 'possible solution',\n",
       " 'solution provided',\n",
       " 'provided come',\n",
       " 'come update',\n",
       " 'update mechanism',\n",
       " 'mechanism merge',\n",
       " 'merge new',\n",
       " 'new data',\n",
       " 'data existing',\n",
       " 'existing model',\n",
       " 'model dstream',\n",
       " 'dstream foreachrdd',\n",
       " 'foreachrdd solution',\n",
       " 'solution allows',\n",
       " 'allows access',\n",
       " 'access underlying',\n",
       " 'underlying micro',\n",
       " 'micro batch',\n",
       " 'batch view',\n",
       " 'view data',\n",
       " 'data sadly',\n",
       " 'sadly foreachrdd',\n",
       " 'foreachrdd direct',\n",
       " 'direct equivalent',\n",
       " 'equivalent structured',\n",
       " 'structured streaming',\n",
       " 'streaming using',\n",
       " 'using custom',\n",
       " 'custom sink',\n",
       " 'sink get',\n",
       " 'get similar',\n",
       " 'similar behavior',\n",
       " 'behavior structured',\n",
       " 'structured streaming',\n",
       " 'streaming sink',\n",
       " 'sink api',\n",
       " 'api defined',\n",
       " 'defined streamsinkprovider',\n",
       " 'streamsinkprovider used',\n",
       " 'used create',\n",
       " 'create instance',\n",
       " 'instance sink',\n",
       " 'sink given',\n",
       " 'given sqlcontext',\n",
       " 'sqlcontext setting',\n",
       " 'setting sink',\n",
       " 'sink sink',\n",
       " 'sink trait',\n",
       " 'trait used',\n",
       " 'used process',\n",
       " 'process actual',\n",
       " 'actual data',\n",
       " 'data batch',\n",
       " 'batch basis',\n",
       " 'basis abstract',\n",
       " 'abstract class',\n",
       " 'class foreachdatasetsinkprovider',\n",
       " 'foreachdatasetsinkprovider extends',\n",
       " 'extends streamsinkprovider',\n",
       " 'streamsinkprovider def',\n",
       " 'def func',\n",
       " 'func df',\n",
       " 'df dataframe',\n",
       " 'dataframe unit',\n",
       " 'unit def',\n",
       " 'def createsink',\n",
       " 'createsink sqlcontext',\n",
       " 'sqlcontext sqlcontext',\n",
       " 'sqlcontext parameter',\n",
       " 'parameter map',\n",
       " 'map string',\n",
       " 'string string',\n",
       " 'string partitioncolumns',\n",
       " 'partitioncolumns seq',\n",
       " 'seq string',\n",
       " 'string outputmode',\n",
       " 'outputmode outputmode',\n",
       " 'outputmode foreachdatasetsink',\n",
       " 'foreachdatasetsink new',\n",
       " 'new foreachdatasetsink',\n",
       " 'foreachdatasetsink func',\n",
       " 'func case',\n",
       " 'case class',\n",
       " 'class foreachdatasetsink',\n",
       " 'foreachdatasetsink func',\n",
       " 'func dataframe',\n",
       " 'dataframe unit',\n",
       " 'unit extends',\n",
       " 'extends sink',\n",
       " 'sink override',\n",
       " 'override def',\n",
       " 'def addbatch',\n",
       " 'addbatch batchid',\n",
       " 'batchid long',\n",
       " 'long data',\n",
       " 'data dataframe',\n",
       " 'dataframe unit',\n",
       " 'unit func',\n",
       " 'func data',\n",
       " 'data writing',\n",
       " 'writing dataframes',\n",
       " 'dataframes custom',\n",
       " 'custom format',\n",
       " 'format use',\n",
       " 'use third',\n",
       " 'third party',\n",
       " 'party sink',\n",
       " 'sink specify',\n",
       " 'specify full',\n",
       " 'full class',\n",
       " 'class name',\n",
       " 'name sink',\n",
       " 'sink since',\n",
       " 'since need',\n",
       " 'need specify',\n",
       " 'specify full',\n",
       " 'full class',\n",
       " 'class name',\n",
       " 'name format',\n",
       " 'format need',\n",
       " 'need ensure',\n",
       " 'ensure instance',\n",
       " 'instance sinkprovider',\n",
       " 'sinkprovider update',\n",
       " 'update model',\n",
       " 'model since',\n",
       " 'since get',\n",
       " 'get access',\n",
       " 'access sink',\n",
       " 'sink object',\n",
       " 'object get',\n",
       " 'get constructed',\n",
       " 'constructed need',\n",
       " 'need make',\n",
       " 'make model',\n",
       " 'model outside',\n",
       " 'outside sink',\n",
       " 'sink object',\n",
       " 'object simplestreamingnaivebayes',\n",
       " 'simplestreamingnaivebayes val',\n",
       " 'val model',\n",
       " 'model new',\n",
       " 'new streamingnaivebayes',\n",
       " 'streamingnaivebayes class',\n",
       " 'class streamingnaivebayessinkprovider',\n",
       " 'streamingnaivebayessinkprovider extends',\n",
       " 'extends foreachdatasetsinkprovider',\n",
       " 'foreachdatasetsinkprovider override',\n",
       " 'override def',\n",
       " 'def func',\n",
       " 'func df',\n",
       " 'df dataframe',\n",
       " 'dataframe val',\n",
       " 'val spark',\n",
       " 'spark df',\n",
       " 'df sparksession',\n",
       " 'sparksession simplestreamingnaivebayes',\n",
       " 'simplestreamingnaivebayes model',\n",
       " 'model update',\n",
       " 'update df',\n",
       " 'df use',\n",
       " 'use custom',\n",
       " 'custom sink',\n",
       " 'sink shown',\n",
       " 'shown integrate',\n",
       " 'integrate machine',\n",
       " 'machine learning',\n",
       " 'learning structured',\n",
       " 'structured streaming',\n",
       " 'streaming waiting',\n",
       " 'waiting spark',\n",
       " 'spark ml',\n",
       " 'ml updated',\n",
       " 'updated structured',\n",
       " 'structured streaming',\n",
       " 'streaming train',\n",
       " 'train using',\n",
       " 'using model',\n",
       " 'model inside',\n",
       " 'inside simplestreamingnaivebayes',\n",
       " 'simplestreamingnaivebayes object',\n",
       " 'object called',\n",
       " 'called multiple',\n",
       " 'multiple stream',\n",
       " 'stream stream',\n",
       " 'stream update',\n",
       " 'update model',\n",
       " 'model would',\n",
       " 'would except',\n",
       " 'except hard',\n",
       " 'hard coded',\n",
       " 'coded query',\n",
       " 'query name',\n",
       " 'name preventing',\n",
       " 'preventing multiple',\n",
       " 'multiple running',\n",
       " 'running def',\n",
       " 'def train',\n",
       " 'train d',\n",
       " 'd dataset',\n",
       " 'dataset d',\n",
       " 'd writestream',\n",
       " 'writestream format',\n",
       " 'format com',\n",
       " 'com highperformancespark',\n",
       " 'highperformancespark example',\n",
       " 'example structuredstreaming',\n",
       " 'structuredstreaming streamingnaivebayessinkprovider',\n",
       " 'streamingnaivebayessinkprovider queryname',\n",
       " 'queryname trainingnaivebayes',\n",
       " 'trainingnaivebayes start',\n",
       " 'start willing',\n",
       " 'willing throw',\n",
       " 'throw caution',\n",
       " 'caution wind',\n",
       " 'wind access',\n",
       " 'access spark',\n",
       " 'spark internals',\n",
       " 'internals construct',\n",
       " 'construct sink',\n",
       " 'sink behaves',\n",
       " 'behaves like',\n",
       " 'like original',\n",
       " 'original foreachrdd',\n",
       " 'foreachrdd interested',\n",
       " 'interested custom',\n",
       " 'custom sink',\n",
       " 'sink support',\n",
       " 'support follow',\n",
       " 'follow spark',\n",
       " 'spark 16407',\n",
       " '16407 pr',\n",
       " 'pr cool',\n",
       " 'cool part',\n",
       " 'part regardless',\n",
       " 'regardless whether',\n",
       " 'whether want',\n",
       " 'want access',\n",
       " 'access internal',\n",
       " 'internal spark',\n",
       " 'spark apis',\n",
       " 'apis handle',\n",
       " 'handle batch',\n",
       " 'batch update',\n",
       " 'update way',\n",
       " 'way spark',\n",
       " 'spark earlier',\n",
       " 'earlier streaming',\n",
       " 'streaming machine',\n",
       " 'machine learning',\n",
       " 'learning implemented',\n",
       " 'implemented certainly',\n",
       " 'certainly ready',\n",
       " 'ready production',\n",
       " 'production usage',\n",
       " 'usage see',\n",
       " 'see structured',\n",
       " 'structured streaming',\n",
       " 'streaming api',\n",
       " 'api offer',\n",
       " 'offer number',\n",
       " 'number different',\n",
       " 'different way',\n",
       " 'way extended',\n",
       " 'extended support',\n",
       " 'support machine',\n",
       " 'machine learning',\n",
       " 'learning learn',\n",
       " 'learn high',\n",
       " 'high performance',\n",
       " 'performance spark',\n",
       " 'spark best',\n",
       " 'best practice',\n",
       " 'practice scaling',\n",
       " 'scaling optimizing',\n",
       " 'optimizing apache',\n",
       " 'apache spark',\n",
       " 'spark share',\n",
       " 'share share',\n",
       " 'share holden',\n",
       " 'holden karau',\n",
       " 'karau date',\n",
       " 'date 22',\n",
       " '22 september',\n",
       " 'september 2016tags',\n",
       " '2016tags streaming',\n",
       " 'streaming data',\n",
       " 'data prosnewsletter',\n",
       " 'prosnewsletter subscribe',\n",
       " 'subscribe spark',\n",
       " 'spark technology',\n",
       " 'technology center',\n",
       " 'center newsletter',\n",
       " 'newsletter latest',\n",
       " 'latest thought',\n",
       " 'thought leadership',\n",
       " 'leadership apache',\n",
       " 'apache spark',\n",
       " 'spark machine',\n",
       " 'machine learning',\n",
       " 'learning open',\n",
       " 'open source',\n",
       " 'source subscribenewsletter',\n",
       " 'subscribenewsletter might',\n",
       " 'might also',\n",
       " 'also enjoy',\n",
       " 'enjoy machine',\n",
       " 'machine learning',\n",
       " 'learning extend',\n",
       " 'extend structured',\n",
       " 'structured streaming',\n",
       " 'streaming spark',\n",
       " 'spark ml',\n",
       " 'ml holden',\n",
       " 'holden karau',\n",
       " 'karau apache',\n",
       " 'apache spark',\n",
       " 'spark reveal',\n",
       " 'reveal people',\n",
       " 'people really',\n",
       " 'really use',\n",
       " 'use ibm',\n",
       " 'ibm cloud',\n",
       " 'cloud storage',\n",
       " 'storage shelly',\n",
       " 'shelly garion',\n",
       " 'garion apache',\n",
       " 'apache spark',\n",
       " 'spark 2',\n",
       " '2 0',\n",
       " '0 deep',\n",
       " 'deep dive',\n",
       " 'dive spark',\n",
       " 'spark catalog',\n",
       " 'catalog ddl',\n",
       " 'ddl native',\n",
       " 'native support',\n",
       " 'support xiao',\n",
       " 'xiao li',\n",
       " 'li apache',\n",
       " 'apache spark',\n",
       " 'spark 2',\n",
       " '2 0',\n",
       " '0 apache',\n",
       " 'apache spark',\n",
       " 'spark 2',\n",
       " '2 0',\n",
       " '0 keeping',\n",
       " 'keeping count',\n",
       " 'count christian',\n",
       " 'christian kadnerspark',\n",
       " 'kadnerspark technology',\n",
       " 'technology center',\n",
       " 'center community',\n",
       " 'community project',\n",
       " 'project blog',\n",
       " 'blog advisory',\n",
       " 'advisory council',\n",
       " 'council apache',\n",
       " 'apache software',\n",
       " 'software foundation',\n",
       " 'foundation affiliation',\n",
       " 'affiliation endorse',\n",
       " 'endorse review',\n",
       " 'review material',\n",
       " 'material provided',\n",
       " 'provided website',\n",
       " 'website managed',\n",
       " 'managed ibm',\n",
       " 'ibm apache',\n",
       " 'apache apache',\n",
       " 'apache spark',\n",
       " 'spark spark',\n",
       " 'spark trademark',\n",
       " 'trademark apache',\n",
       " 'apache software',\n",
       " 'software foundation',\n",
       " 'foundation united',\n",
       " 'united state',\n",
       " 'state country']"
      ]
     },
     "execution_count": 1847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = get_ngrams_2_by_article_id(15)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apache spark'], dtype='<U17')"
      ]
     },
     "execution_count": 1852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:10]\n",
    "\n",
    "np.intersect1d(a[0:10], b[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apache software', 'apache spark', 'open source',\n",
       "       'software foundation', 'spark spark', 'spark trademark'],\n",
       "      dtype='<U51')"
      ]
     },
     "execution_count": 1862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = np.intersect1d(a, b)\n",
    "ab\n",
    "\n",
    "abc = np.intersect1d(ab, np.array(c)\n",
    "abc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
